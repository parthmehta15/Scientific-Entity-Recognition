-DOCSTART- -X- O
The -X- _ O
hyperparameters -X- _ O
used -X- _ O
for -X- _ O
ﬁnetuning -X- _ O
XLNet -X- _ B-MethodName
on -X- _ O
various -X- _ O
tasks -X- _ O
are -X- _ O
shown -X- _ O
in -X- _ O
Table -X- _ O
8 -X- _ O
. -X- _ O
“ -X- _ O
Layer -X- _ B-HyperparameterName
- -X- _ I-HyperparameterName
wise -X- _ I-HyperparameterName
decay -X- _ I-HyperparameterName
” -X- _ O
means -X- _ O
exponentially -X- _ O
decaying -X- _ O
the -X- _ O
learning -X- _ O
rates -X- _ O
of -X- _ O
individual -X- _ O
layers -X- _ O
in -X- _ O
a -X- _ O
top -X- _ O
- -X- _ O
down -X- _ O
manner -X- _ O
. -X- _ O
For -X- _ O
example -X- _ O
, -X- _ O
suppose -X- _ O
the -X- _ O
24 -X- _ O
- -X- _ O
th -X- _ O
layer -X- _ O
uses -X- _ O
a -X- _ O
learning -X- _ B-HyperparameterName
rate -X- _ I-HyperparameterName
l -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
Layer -X- _ O
- -X- _ O
wise -X- _ O
decay -X- _ O
rate -X- _ O
is -X- _ O
α -X- _ O
, -X- _ O
then -X- _ O
the -X- _ O
learning -X- _ O
rate -X- _ O
of -X- _ O
layer -X- _ O
m -X- _ O
is -X- _ O
lα24−m -X- _ O
. -X- _ O

The -X- _ O
hyperparameters -X- _ O
used -X- _ O
for -X- _ O
pretraining -X- _ O
XLNet -X- _ B-MethodName
are -X- _ O
shown -X- _ O
in -X- _ O
Table -X- _ O
7 -X- _ O
. -X- _ O

Number -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
layers -X- _ I-HyperparameterName
24 -X- _ B-HyperparameterValue
Hidden -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
1024 -X- _ B-HyperparameterValue
Number -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
attention -X- _ I-HyperparameterName
heads -X- _ I-HyperparameterName
16 -X- _ B-HyperparameterValue
Attention -X- _ B-HyperparameterName
head -X- _ I-HyperparameterName
size -X- _ I-HyperparameterName
64 -X- _ B-HyperparameterValue
FFN -X- _ B-HyperparameterName
inner -X- _ I-HyperparameterName
hidden -X- _ I-HyperparameterName
size -X- _ I-HyperparameterName
4096 -X- _ B-HyperparameterValue
Hidden -X- _ B-HyperparameterName
Dropout -X- _ I-HyperparameterName
0.1 -X- _ B-HyperparameterValue
GeLU -X- _ B-HyperparameterName
Dropout -X- _ I-HyperparameterName
0.0 -X- _ B-HyperparameterValue
Attention -X- _ B-HyperparameterName
dropout -X- _ I-HyperparameterName
0.1 -X- _ B-HyperparameterValue
Partial -X- _ B-HyperparameterName
prediction -X- _ I-HyperparameterName
K -X- _ I-HyperparameterName
6 -X- _ B-HyperparameterValue
Max -X- _ B-HyperparameterName
sequence -X- _ I-HyperparameterName
length -X- _ I-HyperparameterName
512 -X- _ B-HyperparameterValue
Batch -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
8192 -X- _ B-HyperparameterValue
Learning -X- _ B-HyperparameterName
rate -X- _ I-HyperparameterName
4e-4 -X- _ B-HyperparameterValue
Number -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
steps -X- _ I-HyperparameterName
500 -X- _ B-HyperparameterValue
K -X- _ I-HyperparameterValue
Warmup -X- _ B-HyperparameterName
steps -X- _ I-HyperparameterName
40,000 -X- _ B-HyperparameterValue
Learning -X- _ B-HyperparameterName
rate -X- _ I-HyperparameterName
decay -X- _ I-HyperparameterName
linear -X- _ B-HyperparameterValue
Adam -X- _ B-HyperparameterName
epsilon -X- _ I-HyperparameterName
1e-6 -X- _ B-HyperparameterValue
Weight -X- _ B-HyperparameterName
decay -X- _ I-HyperparameterName
0.01 -X- _ B-HyperparameterValue

Following -X- _ O
the -X- _ O
setting -X- _ O
in -X- _ O
previous -X- _ O
work -X- _ O
[ -X- _ O
8 -X- _ O
] -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
the -X- _ O
ClueWeb09 -X- _ B-DatasetName
- -X- _ I-DatasetName
B -X- _ I-DatasetName
dataset -X- _ O
to -X- _ O
evaluate -X- _ O
the -X- _ O
perfor- -X- _ O
mance -X- _ O
on -X- _ O
document -X- _ B-TaskName
ranking -X- _ I-TaskName
. -X- _ O
The -X- _ O
queries -X- _ O
were -X- _ O
created -X- _ O
by -X- _ O
the -X- _ O
TREC -X- _ B-DatasetName
2009 -X- _ I-DatasetName
- -X- _ I-DatasetName
2012 -X- _ I-DatasetName
Web -X- _ O
Tracks -X- _ O
based -X- _ O
on -X- _ O
50 -X- _ O
M -X- _ O
documents -X- _ O
and -X- _ O
the -X- _ O
task -X- _ O
is -X- _ O
to -X- _ O
rerank -X- _ O
the -X- _ O
top -X- _ O
100 -X- _ O
documents -X- _ O
retrieved -X- _ O
using -X- _ O
a -X- _ O
standard -X- _ O
retrieval -X- _ O
method -X- _ O
. -X- _ O
Since -X- _ O
document -X- _ B-TaskName
ranking -X- _ I-TaskName
, -X- _ O
or -X- _ O
ad -X- _ B-TaskName
- -X- _ I-TaskName
hoc -X- _ I-TaskName
retrieval -X- _ I-TaskName
, -X- _ O
mainly -X- _ O
concerns -X- _ O
the -X- _ O
low -X- _ O
- -X- _ O
level -X- _ O
representations -X- _ O
instead -X- _ O
of -X- _ O
high -X- _ O
- -X- _ O
level -X- _ O
semantics -X- _ O
, -X- _ O
this -X- _ O
dataset -X- _ O
serves -X- _ O
as -X- _ O
a -X- _ O
testbed -X- _ O
for -X- _ O
evaluating -X- _ O
the -X- _ O
quality -X- _ O
of -X- _ O
word -X- _ O
embeddings -X- _ O
. -X- _ O
We -X- _ O
use -X- _ O
a -X- _ O
pretrained -X- _ O
XLNet -X- _ B-MethodName
to -X- _ O
extract -X- _ O
word -X- _ O
embeddings -X- _ O
for -X- _ O
the -X- _ O
documents -X- _ O
and -X- _ O
queries -X- _ O
without -X- _ O
ﬁnetuning -X- _ O
, -X- _ O
and -X- _ O
employ -X- _ O
a -X- _ O
kernel -X- _ O
pooling -X- _ O
network -X- _ O
[ -X- _ O
36 -X- _ O
] -X- _ O
to -X- _ O
rank -X- _ O
the -X- _ O
documents -X- _ O
. -X- _ O

The -X- _ O
GLUE -X- _ B-DatasetName
dataset -X- _ O
[ -X- _ O
34 -X- _ O
] -X- _ O
is -X- _ O
a -X- _ O
collection -X- _ O
of -X- _ O
9 -X- _ O
natural -X- _ B-TaskName
language -X- _ I-TaskName
understanding -X- _ I-TaskName
tasks -X- _ O
. -X- _ O
The -X- _ O
test -X- _ O
set -X- _ O
labels -X- _ O
are -X- _ O
removed -X- _ O
from -X- _ O
the -X- _ O
publicly -X- _ O
released -X- _ O
version -X- _ O
, -X- _ O
and -X- _ O
all -X- _ O
the -X- _ O
practitioners -X- _ O
must -X- _ O
submit -X- _ O
their -X- _ O
predictions -X- _ O
on -X- _ O
the -X- _ O
evaluation -X- _ O
server -X- _ O
to -X- _ O
obtain -X- _ O
test -X- _ O
set -X- _ O
results -X- _ O
. -X- _ O
In -X- _ O
Table -X- _ O
5 -X- _ O
, -X- _ O
we -X- _ O
present -X- _ O
results -X- _ O
of -X- _ O
multiple -X- _ O
settings -X- _ O
, -X- _ O
including -X- _ O
single -X- _ O
- -X- _ O
task -X- _ O
and -X- _ O
multi -X- _ O
- -X- _ O
task -X- _ O
, -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
single -X- _ O
models -X- _ O
and -X- _ O
ensembles -X- _ O
. -X- _ O
In -X- _ O
the -X- _ O
multi -X- _ O
- -X- _ O
task -X- _ O
setting -X- _ O
, -X- _ O
we -X- _ O
jointly -X- _ O
train -X- _ O
an -X- _ O
XLNet -X- _ B-MethodName
on -X- _ O
the -X- _ O
four -X- _ O
largest -X- _ O
datasets -X- _ O
— -X- _ O
MNLI -X- _ B-DatasetName
, -X- _ O
SST-2 -X- _ B-DatasetName
, -X- _ O
QNLI -X- _ B-DatasetName
, -X- _ O
and -X- _ O
QQP -X- _ B-DatasetName
— -X- _ O
and -X- _ O
ﬁnetune -X- _ O
the -X- _ O
network -X- _ O
on -X- _ O
the -X- _ O
other -X- _ O
datasets -X- _ O
. -X- _ O
Only -X- _ O
single -X- _ O
- -X- _ O
task -X- _ O
training -X- _ O
is -X- _ O
employed -X- _ O
for -X- _ O
the -X- _ O
four -X- _ O
large -X- _ O
datasets -X- _ O
. -X- _ O
For -X- _ O
QNLI -X- _ B-DatasetName
, -X- _ O
we -X- _ O
employed -X- _ O
a -X- _ O
pairwise -X- _ O
relevance -X- _ O
ranking -X- _ O
scheme -X- _ O
as -X- _ O
in -X- _ O
[ -X- _ O
20 -X- _ O
] -X- _ O
for -X- _ O
our -X- _ O
test -X- _ O
set -X- _ O
submission -X- _ O
. -X- _ O
However -X- _ O
, -X- _ O
for -X- _ O
fair -X- _ O
comparison -X- _ O
with -X- _ O
BERT -X- _ B-MethodName
, -X- _ O
our -X- _ O
result -X- _ O
on -X- _ O
the -X- _ O
QNLI -X- _ B-DatasetName
dev -X- _ O
set -X- _ O
is -X- _ O
based -X- _ O
on -X- _ O
a -X- _ O
standard -X- _ O
classiﬁcation -X- _ O
paradigm -X- _ O
. -X- _ O
For -X- _ O
WNLI -X- _ B-DatasetName
, -X- _ O
we -X- _ O
use -X- _ O
the -X- _ O
loss -X- _ O
described -X- _ O
in -X- _ O
[ -X- _ O
16 -X- _ O
] -X- _ O
. -X- _ O

Following -X- _ O
previous -X- _ O
work -X- _ O
on -X- _ O
text -X- _ B-TaskName
classiﬁcation -X- _ I-TaskName
[ -X- _ O
39 -X- _ O
, -X- _ O
23 -X- _ O
] -X- _ O
, -X- _ O
we -X- _ O
evaluate -X- _ O
XLNet -X- _ B-MethodName
on -X- _ O
the -X- _ O
following -X- _ O
bench- -X- _ O
marks -X- _ O
: -X- _ O
IMDB -X- _ B-DatasetName
, -X- _ O
Yelp-2 -X- _ B-DatasetName
, -X- _ O
Yelp-5 -X- _ B-DatasetName
, -X- _ O
DBpedia -X- _ B-DatasetName
, -X- _ O
AG -X- _ B-DatasetName
, -X- _ O
Amazon-2 -X- _ B-DatasetName
, -X- _ O
and -X- _ O
Amazon-5 -X- _ B-DatasetName
. -X- _ O

SQuAD -X- _ B-DatasetName
is -X- _ O
a -X- _ O
large -X- _ O
- -X- _ O
scale -X- _ O
reading -X- _ O
comprehension -X- _ O
dataset -X- _ O
with -X- _ O
two -X- _ O
tasks -X- _ O
. -X- _ O
SQuAD1.1 -X- _ B-DatasetName
[ -X- _ O
30 -X- _ O
] -X- _ O
contains -X- _ O
questions -X- _ O
that -X- _ O
always -X- _ O
have -X- _ O
a -X- _ O
corresponding -X- _ O
answer -X- _ O
in -X- _ O
the -X- _ O
given -X- _ O
passages -X- _ O
, -X- _ O
while -X- _ O
SQuAD2.0 -X- _ B-DatasetName
[ -X- _ O
29 -X- _ O
] -X- _ O
introduces -X- _ O
unanswerable -X- _ O
questions -X- _ O
. -X- _ O
To -X- _ O
ﬁnetune -X- _ O
an -X- _ O
XLNet -X- _ B-MethodName
on -X- _ O
SQuAD2.0 -X- _ B-DatasetName
, -X- _ O
we -X- _ O
jointly -X- _ O
apply -X- _ O
a -X- _ O
logis- -X- _ B-MetricName
tic -X- _ I-MetricName
regression -X- _ I-MetricName
loss -X- _ I-MetricName
for -X- _ O
answerability -X- _ O
prediction -X- _ O
similar -X- _ O
to -X- _ O
classiﬁcation -X- _ O
tasks -X- _ O
and -X- _ O
a -X- _ O
standard -X- _ O
span -X- _ B-MetricName
extraction -X- _ I-MetricName
loss -X- _ I-MetricName
for -X- _ O
question -X- _ B-TaskName
answering -X- _ I-TaskName
[ -X- _ O
10 -X- _ O
] -X- _ O
. -X- _ O

The -X- _ O
RACE -X- _ B-DatasetName
dataset -X- _ O
[ -X- _ O
18 -X- _ O
] -X- _ O
contains -X- _ O
near -X- _ O
100 -X- _ O
K -X- _ O
questions -X- _ O
taken -X- _ O
from -X- _ O
the -X- _ O
English -X- _ O
exams -X- _ O
for -X- _ O
middle -X- _ O
and -X- _ O
high -X- _ O
school -X- _ O
Chinese -X- _ O
students -X- _ O
in -X- _ O
the -X- _ O
age -X- _ O
range -X- _ O
between -X- _ O
12 -X- _ O
to -X- _ O
18 -X- _ O
, -X- _ O
with -X- _ O
the -X- _ O
answers -X- _ O
generated -X- _ O
by -X- _ O
human -X- _ O
experts -X- _ O
. -X- _ O
This -X- _ O
is -X- _ O
one -X- _ O
of -X- _ O
the -X- _ O
most -X- _ O
difﬁcult -X- _ O
reading -X- _ O
comprehension -X- _ O
datasets -X- _ O
that -X- _ O
involve -X- _ O
challenging -X- _ O
reasoning -X- _ O
questions -X- _ O
. -X- _ O
Moreover -X- _ O
, -X- _ O
the -X- _ O
average -X- _ O
length -X- _ O
of -X- _ O
the -X- _ O
passages -X- _ O
in -X- _ O
RACE -X- _ B-MethodName
are -X- _ O
longer -X- _ O
than -X- _ O
300 -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
signiﬁcantly -X- _ O
longer -X- _ O
than -X- _ O
other -X- _ O
popular -X- _ O
reading -X- _ O
comprehension -X- _ O
datasets -X- _ O
such -X- _ O
as -X- _ O
SQuAD -X- _ B-MethodName
[ -X- _ O
29 -X- _ O
] -X- _ O
. -X- _ O
As -X- _ O
a -X- _ O
result -X- _ O
, -X- _ O
this -X- _ O
dataset -X- _ O
serves -X- _ O
as -X- _ O
a -X- _ O
challenging -X- _ O
benchmark -X- _ O
for -X- _ O
long -X- _ O
text -X- _ O
understanding -X- _ O
. -X- _ O
We -X- _ O
use -X- _ O
a -X- _ O
sequence -X- _ B-HyperparameterName
length -X- _ I-HyperparameterName
of -X- _ O
512 -X- _ B-HyperparameterValue
during -X- _ O
ﬁnetuning -X- _ O
. -X- _ O

For -X- _ O
the -X- _ O
Transformer -X- _ B-MethodName
- -X- _ I-MethodName
XL -X- _ I-MethodName
layer -X- _ O
m -X- _ O
= -X- _ O
1 -X- _ O
, -X- _ O
· -X- _ O
· -X- _ O
· -X- _ O
, -X- _ O
M -X- _ O
, -X- _ O
attention -X- _ O
with -X- _ O
relative -X- _ O
positional -X- _ O
encoding -X- _ O
and -X- _ O
position -X- _ O
- -X- _ O
wise -X- _ O
feed -X- _ O
- -X- _ O
forward -X- _ O
are -X- _ O
consecutively -X- _ O
employed -X- _ O
to -X- _ O
update -X- _ O
the -X- _ O
represetntations -X- _ O
: -X- _ O

Cached -X- _ O
layer -X- _ O
- -X- _ O
m -X- _ O
content -X- _ O
represetation -X- _ O
( -X- _ O
memory -X- _ O
) -X- _ O
from -X- _ O
previous -X- _ O
segment -X- _ O
: -X- _ O
˜h(m -X- _ O
) -X- _ O

Here -X- _ O
, -X- _ O
we -X- _ O
provide -X- _ O
the -X- _ O
implementation -X- _ O
details -X- _ O
of -X- _ O
the -X- _ O
two -X- _ O
- -X- _ O
stream -X- _ O
attention -X- _ O
with -X- _ O
a -X- _ O
Transformer -X- _ B-MethodName
- -X- _ I-MethodName
XL -X- _ I-MethodName
backbone -X- _ O
. -X- _ O
Initial -X- _ O
represetation -X- _ O
: -X- _ O
∀t -X- _ O
= -X- _ O
1 -X- _ O
, -X- _ O
. -X- _ O
. -X- _ O
. -X- _ O
, -X- _ O
T -X- _ O
: -X- _ O
ht -X- _ O
= -X- _ O
e(xt -X- _ O
) -X- _ O
and -X- _ O
gt -X- _ O
= -X- _ O
w -X- _ O

XLNet -X- _ B-MethodName
is -X- _ O
a -X- _ O
generalized -X- _ O
AR -X- _ O
pretraining -X- _ O
method -X- _ O
that -X- _ O
uses -X- _ O
a -X- _ O
permutation -X- _ O
language -X- _ O
modeling -X- _ O
objective -X- _ O
to -X- _ O
combine -X- _ O
the -X- _ O
advantages -X- _ O
of -X- _ O
AR -X- _ O
and -X- _ O
AE -X- _ O
methods -X- _ O
. -X- _ O
The -X- _ O
neural -X- _ O
architecture -X- _ O
of -X- _ O
XLNet -X- _ B-MethodName
is -X- _ O
developed -X- _ O
to -X- _ O
work -X- _ O
seamlessly -X- _ O
with -X- _ O
the -X- _ O
AR -X- _ O
objective -X- _ O
, -X- _ O
including -X- _ O
integrating -X- _ O
Transformer -X- _ B-MethodName
- -X- _ I-MethodName
XL -X- _ I-MethodName
and -X- _ O
the -X- _ O
careful -X- _ O
design -X- _ O
of -X- _ O
the -X- _ O
two -X- _ O
- -X- _ O
stream -X- _ O
attention -X- _ O
mechanism -X- _ O
. -X- _ O
XLNet -X- _ B-MethodName
achieves -X- _ O
substantial -X- _ O
improvement -X- _ O
over -X- _ O
previous -X- _ O
pretraining -X- _ O
objectives -X- _ O
on -X- _ O
various -X- _ O
tasks -X- _ O
. -X- _ O

Finally -X- _ O
, -X- _ O
we -X- _ O
also -X- _ O
perform -X- _ O
a -X- _ O
qualitative -X- _ O
study -X- _ O
of -X- _ O
the -X- _ O
attention -X- _ O
patterns -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
included -X- _ O
in -X- _ O
Appendix -X- _ O
A.6 -X- _ O
due -X- _ O
to -X- _ O
page -X- _ O
limit -X- _ O
. -X- _ O

Examining -X- _ O
rows -X- _ O
1 -X- _ O
- -X- _ O
4 -X- _ O
of -X- _ O
Table -X- _ O
6 -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
see -X- _ O
both -X- _ O
Transformer -X- _ B-MethodName
- -X- _ I-MethodName
XL -X- _ I-MethodName
and -X- _ O
the -X- _ O
permutation -X- _ O
LM -X- _ O
clearly -X- _ O
contribute -X- _ O
the -X- _ O
superior -X- _ O
performance -X- _ O
of -X- _ O
XLNet -X- _ B-MethodName
over -X- _ O
BERT -X- _ B-MethodName
. -X- _ O
Moreover -X- _ O
, -X- _ O
if -X- _ O
we -X- _ O
remove -X- _ O
the -X- _ O
memory -X- _ O
caching -X- _ O
mechanism -X- _ O
( -X- _ O
row -X- _ O
5 -X- _ O
) -X- _ O
, -X- _ O
the -X- _ O
performance -X- _ O
clearly -X- _ O
drops -X- _ O
, -X- _ O
especially -X- _ O
for -X- _ O
RACE -X- _ B-DatasetName
which -X- _ O
involves -X- _ O
the -X- _ O
longest -X- _ O
context -X- _ O
among -X- _ O
the -X- _ O
4 -X- _ O
tasks -X- _ O
. -X- _ O
In -X- _ O
addition -X- _ O
, -X- _ O
rows -X- _ O
6 -X- _ O
- -X- _ O
7 -X- _ O
show -X- _ O
that -X- _ O
both -X- _ O
span -X- _ O
- -X- _ O
based -X- _ O
prediction -X- _ O
and -X- _ O
the -X- _ O
bidirectional -X- _ O
input -X- _ O
pipeline -X- _ O
play -X- _ O
important -X- _ O
roles -X- _ O
in -X- _ O
XLNet -X- _ B-MethodName
. -X- _ O
Finally -X- _ O
, -X- _ O
we -X- _ O
unexpectedly -X- _ O
ﬁnd -X- _ O
the -X- _ O
the -X- _ O
next -X- _ B-DatasetName
- -X- _ I-DatasetName
sentence -X- _ I-DatasetName
prediction -X- _ I-DatasetName
objective -X- _ O
proposed -X- _ O
in -X- _ O
the -X- _ O
original -X- _ O
BERT -X- _ B-MethodName
does -X- _ O
not -X- _ O
necessarily -X- _ O
lead -X- _ O
to -X- _ O
an -X- _ O
improvement -X- _ O
in -X- _ O
our -X- _ O
setting -X- _ O
. -X- _ O
Hence -X- _ O
, -X- _ O
we -X- _ O
exclude -X- _ O
the -X- _ O
next -X- _ B-DatasetName
- -X- _ I-DatasetName
sentence -X- _ I-DatasetName
prediction -X- _ I-DatasetName
objective -X- _ O
from -X- _ O
XLNet -X- _ B-MethodName
. -X- _ O

With -X- _ O
these -X- _ O
purposes -X- _ O
in -X- _ O
mind -X- _ O
, -X- _ O
in -X- _ O
Table -X- _ O
6 -X- _ O
, -X- _ O
we -X- _ O
compare -X- _ O
6 -X- _ O
XLNet -X- _ B-MethodName
- -X- _ O
Base -X- _ O
variants -X- _ O
with -X- _ O
different -X- _ O
implemen- -X- _ O
tation -X- _ O
details -X- _ O
( -X- _ O
rows -X- _ O
3 -X- _ O
- -X- _ O
8) -X- _ O
, -X- _ O
the -X- _ O
original -X- _ O
BERT -X- _ B-MethodName
- -X- _ O
Base -X- _ O
model -X- _ O
( -X- _ O
row -X- _ O
1 -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
an -X- _ O
additional -X- _ O
Transformer -X- _ B-MethodName
- -X- _ I-MethodName
XL -X- _ I-MethodName
baseline -X- _ O
trained -X- _ O
with -X- _ O
the -X- _ O
denoising -X- _ O
auto -X- _ O
- -X- _ O
encoding -X- _ O
( -X- _ O
DAE -X- _ O
) -X- _ O
objective -X- _ O
used -X- _ O
in -X- _ O
BERT -X- _ B-MethodName
but -X- _ O
with -X- _ O
the -X- _ O
bidi- -X- _ O
rectional -X- _ O
input -X- _ O
pipeline -X- _ O
( -X- _ O
row -X- _ O
2 -X- _ O
) -X- _ O
. -X- _ O
For -X- _ O
fair -X- _ O
comparison -X- _ O
, -X- _ O
all -X- _ O
models -X- _ O
are -X- _ O
based -X- _ O
on -X- _ O
a -X- _ O
12 -X- _ B-HyperparameterValue
- -X- _ O
layer -X- _ B-HyperparameterName
architecture -X- _ O
with -X- _ O
the -X- _ O
same -X- _ O
model -X- _ O
hyper -X- _ O
- -X- _ O
parameters -X- _ O
as -X- _ O
BERT -X- _ B-MethodName
- -X- _ O
Base -X- _ O
and -X- _ O
are -X- _ O
trained -X- _ O
on -X- _ O
only -X- _ O
Wikipedia -X- _ B-DatasetName
and -X- _ O
the -X- _ O
BooksCorpus -X- _ B-DatasetName
. -X- _ O
All -X- _ O
results -X- _ O
reported -X- _ O
are -X- _ O
the -X- _ O
median -X- _ O
of -X- _ O
5 -X- _ O
runs -X- _ O
. -X- _ O

• -X- _ O
The -X- _ O
importance -X- _ O
of -X- _ O
using -X- _ O
Transformer -X- _ B-MethodName
- -X- _ I-MethodName
XL -X- _ I-MethodName
as -X- _ O
the -X- _ O
backbone -X- _ O
neural -X- _ O
architecture -X- _ O
. -X- _ O
• -X- _ O
The -X- _ O
necessity -X- _ O
of -X- _ O
some -X- _ O
implementation -X- _ O
details -X- _ O
including -X- _ O
span -X- _ O
- -X- _ O
based -X- _ O
prediction -X- _ O
, -X- _ O
the -X- _ O
bidirectional -X- _ O
input -X- _ O
pipeline -X- _ O
, -X- _ O
and -X- _ O
next -X- _ O
- -X- _ O
sentence -X- _ O
prediction -X- _ O
. -X- _ O

• -X- _ O
The -X- _ O
effectiveness -X- _ O
of -X- _ O
the -X- _ O
permutation -X- _ O
language -X- _ O
modeling -X- _ O
objective -X- _ O
alone -X- _ O
, -X- _ O
especially -X- _ O
compared -X- _ O
to -X- _ O
the -X- _ O
denoising -X- _ O
auto -X- _ O
- -X- _ O
encoding -X- _ O
objective -X- _ O
used -X- _ O
by -X- _ O
BERT -X- _ B-MethodName
. -X- _ O

We -X- _ O
perform -X- _ O
an -X- _ O
ablation -X- _ O
study -X- _ O
to -X- _ O
understand -X- _ O
the -X- _ O
importance -X- _ O
of -X- _ O
each -X- _ O
design -X- _ O
choice -X- _ O
based -X- _ O
on -X- _ O
four -X- _ O
datasets -X- _ O
with -X- _ O
diverse -X- _ O
characteristics -X- _ O
. -X- _ O
Speciﬁcally -X- _ O
, -X- _ O
there -X- _ O
are -X- _ O
three -X- _ O
main -X- _ O
aspects -X- _ O
we -X- _ O
hope -X- _ O
to -X- _ O
study -X- _ O
: -X- _ O

• -X- _ O
For -X- _ O
classiﬁcation -X- _ O
tasks -X- _ O
that -X- _ O
already -X- _ O
have -X- _ O
abundant -X- _ O
supervised -X- _ O
examples -X- _ O
such -X- _ O
as -X- _ O
MNLI -X- _ B-DatasetName
( -X- _ O
> -X- _ O
390 -X- _ O
K -X- _ O
) -X- _ O
, -X- _ O
Yelp -X- _ B-DatasetName
( -X- _ O
> -X- _ O
560 -X- _ O
K -X- _ O
) -X- _ O
and -X- _ O
Amazon -X- _ B-DatasetName
( -X- _ O
> -X- _ O
3 -X- _ O
M -X- _ O
) -X- _ O
, -X- _ O
XLNet -X- _ B-MethodName
still -X- _ O
lead -X- _ O
to -X- _ O
substantial -X- _ O
gains -X- _ O
. -X- _ O

• -X- _ O
For -X- _ O
explicit -X- _ O
reasoning -X- _ O
tasks -X- _ O
like -X- _ O
SQuAD -X- _ B-DatasetName
and -X- _ O
RACE -X- _ B-DatasetName
that -X- _ O
involve -X- _ O
longer -X- _ O
context -X- _ O
, -X- _ O
the -X- _ O
performance -X- _ O
gain -X- _ O
of -X- _ O
XLNet -X- _ B-MethodName
is -X- _ O
usually -X- _ O
larger -X- _ O
. -X- _ O
This -X- _ O
superiority -X- _ O
at -X- _ O
dealing -X- _ O
with -X- _ O
longer -X- _ O
context -X- _ O
could -X- _ O
come -X- _ O
from -X- _ O
the -X- _ O
Transformer -X- _ B-MethodName
- -X- _ I-MethodName
XL -X- _ I-MethodName
backbone -X- _ O
in -X- _ O
XLNet -X- _ B-MethodName
. -X- _ O

The -X- _ O
results -X- _ O
are -X- _ O
presented -X- _ O
in -X- _ O
Tables -X- _ O
2 -X- _ O
( -X- _ O
reading -X- _ B-DatasetName
comprehension -X- _ I-DatasetName
& -X- _ O
document -X- _ B-DatasetName
ranking -X- _ I-DatasetName
) -X- _ O
, -X- _ O
3 -X- _ O
( -X- _ O
question -X- _ B-DatasetName
answering -X- _ I-DatasetName
) -X- _ O
, -X- _ O
4 -X- _ O
( -X- _ O
text -X- _ B-DatasetName
classiﬁcation -X- _ I-DatasetName
) -X- _ O
and -X- _ O
5 -X- _ O
( -X- _ O
natural -X- _ B-DatasetName
language -X- _ I-DatasetName
understanding -X- _ I-DatasetName
) -X- _ O
, -X- _ O
where -X- _ O
XLNet -X- _ B-MethodName
generally -X- _ O
outperforms -X- _ O
BERT -X- _ B-MethodName
and -X- _ O
RoBERTa -X- _ B-MethodName
. -X- _ O
In -X- _ O
addition -X- _ O
, -X- _ O
we -X- _ O
make -X- _ O
two -X- _ O
more -X- _ O
interesting -X- _ O
observations -X- _ O
: -X- _ O

After -X- _ O
the -X- _ O
initial -X- _ O
publication -X- _ O
of -X- _ O
our -X- _ O
manuscript -X- _ O
, -X- _ O
a -X- _ O
few -X- _ O
other -X- _ O
pretrained -X- _ O
models -X- _ O
were -X- _ O
released -X- _ O
such -X- _ O
as -X- _ O
RoBERTa -X- _ B-MethodName
[ -X- _ O
21 -X- _ O
] -X- _ O
and -X- _ O
ALBERT -X- _ B-MethodName
[ -X- _ O
19 -X- _ O
] -X- _ O
. -X- _ O
Since -X- _ O
ALBERT -X- _ B-MethodName
involves -X- _ O
increasing -X- _ O
the -X- _ O
model -X- _ O
hidden -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
from -X- _ O
1024 -X- _ B-HyperparameterValue
to -X- _ O
2048/4096 -X- _ B-HyperparameterValue
and -X- _ O
thus -X- _ O
substantially -X- _ O
increases -X- _ O
the -X- _ O
amount -X- _ O
of -X- _ O
computation -X- _ O
in -X- _ O
terms -X- _ O
of -X- _ O
FLOPs -X- _ O
, -X- _ O
we -X- _ O
exclude -X- _ O
ALBERT -X- _ B-MethodName
from -X- _ O
the -X- _ O
following -X- _ O
results -X- _ O
as -X- _ O
it -X- _ O
is -X- _ O
hard -X- _ O
to -X- _ O
lead -X- _ O
to -X- _ O
scientiﬁc -X- _ O
conclusions -X- _ O
. -X- _ O
To -X- _ O
obtain -X- _ O
relatively -X- _ O
fair -X- _ O
comparison -X- _ O
with -X- _ O
RoBERTa -X- _ B-MethodName
, -X- _ O
the -X- _ O
experiment -X- _ O
in -X- _ O
this -X- _ O
section -X- _ O
is -X- _ O
based -X- _ O
on -X- _ O
full -X- _ O
data -X- _ O
and -X- _ O
reuses -X- _ O
the -X- _ O
hyper -X- _ O
- -X- _ O
parameters -X- _ O
of -X- _ O
RoBERTa -X- _ B-MethodName
, -X- _ O
as -X- _ O
described -X- _ O
in -X- _ O
section -X- _ O
3.1 -X- _ O
. -X- _ O

Here -X- _ O
, -X- _ O
we -X- _ O
ﬁrst -X- _ O
compare -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
BERT -X- _ B-MethodName
and -X- _ O
XLNet -X- _ B-MethodName
in -X- _ O
a -X- _ O
fair -X- _ O
setting -X- _ O
to -X- _ O
decouple -X- _ O
the -X- _ O
effects -X- _ O
of -X- _ O
using -X- _ O
more -X- _ O
data -X- _ O
and -X- _ O
the -X- _ O
improvement -X- _ O
from -X- _ O
BERT -X- _ B-MethodName
to -X- _ O
XLNet -X- _ B-MethodName
. -X- _ O
In -X- _ O
Table -X- _ O
1 -X- _ O
, -X- _ O
we -X- _ O
compare -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
best -X- _ O
performance -X- _ O
of -X- _ O
three -X- _ O
different -X- _ O
variants -X- _ O
of -X- _ O
BERT -X- _ B-MethodName
and -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
XLNet -X- _ B-MethodName
trained -X- _ O
with -X- _ O
the -X- _ O
same -X- _ O
data -X- _ O
and -X- _ O
hyperparameters -X- _ O
. -X- _ O
As -X- _ O
we -X- _ O
can -X- _ O
see -X- _ O
, -X- _ O
trained -X- _ O
on -X- _ O
the -X- _ O
same -X- _ O
data -X- _ O
with -X- _ O
an -X- _ O
almost -X- _ O
identical -X- _ O
training -X- _ O
recipe -X- _ O
, -X- _ O
XLNet -X- _ B-MethodName
outperforms -X- _ O
BERT -X- _ B-MethodName
by -X- _ O
a -X- _ O
sizable -X- _ O
margin -X- _ O
on -X- _ O
all -X- _ O
the -X- _ O
considered -X- _ O
datasets -X- _ O
. -X- _ O

We -X- _ O
use -X- _ O
a -X- _ O
variety -X- _ O
of -X- _ O
natural -X- _ O
language -X- _ O
understanding -X- _ O
datasets -X- _ O
to -X- _ O
evaluate -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
our -X- _ O
method -X- _ O
. -X- _ O
Detailed -X- _ O
descriptions -X- _ O
of -X- _ O
the -X- _ O
settings -X- _ O
for -X- _ O
all -X- _ O
the -X- _ O
datasets -X- _ O
can -X- _ O
be -X- _ O
found -X- _ O
in -X- _ O
Appendix -X- _ O
A.3 -X- _ O
. -X- _ O

Since -X- _ O
the -X- _ O
recurrence -X- _ O
mechanism -X- _ O
is -X- _ O
introduced -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
a -X- _ O
bidirectional -X- _ O
data -X- _ O
input -X- _ O
pipeline -X- _ O
where -X- _ O
each -X- _ O
of -X- _ O
the -X- _ O
forward -X- _ O
and -X- _ O
backward -X- _ O
directions -X- _ O
takes -X- _ O
half -X- _ O
of -X- _ O
the -X- _ O
batch -X- _ O
size -X- _ O
. -X- _ O
For -X- _ O
training -X- _ O
XLNet -X- _ B-MethodName
- -X- _ I-MethodName
Large -X- _ I-MethodName
, -X- _ O
we -X- _ O
set -X- _ O
the -X- _ O
partial -X- _ B-HyperparameterName
prediction -X- _ I-HyperparameterName
constant -X- _ I-HyperparameterName
K -X- _ O
as -X- _ O
6 -X- _ B-HyperparameterValue
( -X- _ O
see -X- _ O
Section -X- _ O
2.3 -X- _ O
) -X- _ O
. -X- _ O
Our -X- _ O
ﬁnetuning -X- _ O
procedure -X- _ O
follows -X- _ O
BERT -X- _ B-MethodName
[ -X- _ O
10 -X- _ O
] -X- _ O
except -X- _ O
otherwise -X- _ O
speciﬁed3 -X- _ O
. -X- _ O
We -X- _ O
employ -X- _ O
an -X- _ O
idea -X- _ O
of -X- _ O
span -X- _ O
- -X- _ O
based -X- _ O
prediction -X- _ O
, -X- _ O
where -X- _ O
we -X- _ O
ﬁrst -X- _ O
sample -X- _ O
a -X- _ O
length -X- _ O
L -X- _ O
∈ -X- _ O
[ -X- _ O
1 -X- _ O
, -X- _ O
· -X- _ O
· -X- _ O
· -X- _ O
, -X- _ O
5 -X- _ O
] -X- _ O
, -X- _ O
and -X- _ O
then -X- _ O
randomly -X- _ O
select -X- _ O
a -X- _ O
consecutive -X- _ O
span -X- _ O
of -X- _ O
L -X- _ O
tokens -X- _ O
as -X- _ O
prediction -X- _ O
targets -X- _ O
within -X- _ O
a -X- _ O
context -X- _ O
of -X- _ O
( -X- _ O
KL -X- _ O
) -X- _ O
tokens -X- _ O
. -X- _ O

observed -X- _ O
that -X- _ O
the -X- _ O
model -X- _ O
still -X- _ O
underﬁts -X- _ O
the -X- _ O
data -X- _ O
at -X- _ O
the -X- _ O
end -X- _ O
of -X- _ O
training -X- _ O
. -X- _ O
Finally -X- _ O
, -X- _ O
we -X- _ O
perform -X- _ O
ablation -X- _ O
study -X- _ O
( -X- _ O
section -X- _ O
3.4 -X- _ O
) -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
XLNet -X- _ B-MethodName
- -X- _ O
Base -X- _ O
- -X- _ O
wikibooks -X- _ O
. -X- _ O

Our -X- _ O
largest -X- _ O
model -X- _ O
XLNet -X- _ B-MethodName
- -X- _ I-MethodName
Large -X- _ I-MethodName
has -X- _ O
the -X- _ O
same -X- _ O
architecture -X- _ O
hyperparameters -X- _ O
as -X- _ O
BERT -X- _ B-MethodName
- -X- _ I-MethodName
Large -X- _ I-MethodName
, -X- _ O
which -X- _ O
results -X- _ O
in -X- _ O
a -X- _ O
similar -X- _ O
model -X- _ O
size -X- _ O
. -X- _ O
During -X- _ O
pretraining -X- _ O
, -X- _ O
we -X- _ O
always -X- _ O
use -X- _ O
a -X- _ O
full -X- _ O
sequence -X- _ O
length -X- _ O
of -X- _ O
512 -X- _ O
. -X- _ O
Firstly -X- _ O
, -X- _ O
to -X- _ O
provide -X- _ O
a -X- _ O
fair -X- _ O
comparison -X- _ O
with -X- _ O
BERT -X- _ B-MethodName
( -X- _ O
section -X- _ O
3.2 -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
also -X- _ O
trained -X- _ O
XLNet -X- _ B-MethodName
- -X- _ I-MethodName
Large -X- _ I-MethodName
- -X- _ O
wikibooks -X- _ O
on -X- _ O
BooksCorpus -X- _ B-DatasetName
and -X- _ O
Wikipedia -X- _ B-DatasetName
only -X- _ O
, -X- _ O
where -X- _ O
we -X- _ O
reuse -X- _ O
all -X- _ O
pretraining -X- _ O
hyper -X- _ O
- -X- _ O
parameters -X- _ O
as -X- _ O
in -X- _ O
the -X- _ O
original -X- _ O
BERT -X- _ B-MethodName
. -X- _ O
Then -X- _ O
, -X- _ O
we -X- _ O
scale -X- _ O
up -X- _ O
the -X- _ O
training -X- _ O
of -X- _ O
XLNet -X- _ B-MethodName
- -X- _ I-MethodName
Large -X- _ I-MethodName
by -X- _ O
using -X- _ O
all -X- _ O
the -X- _ O
datasets -X- _ O
described -X- _ O
above -X- _ O
. -X- _ O
Speciﬁcally -X- _ O
, -X- _ O
we -X- _ O
train -X- _ O
on -X- _ O
512 -X- _ O
TPU -X- _ O
v3 -X- _ O
chips -X- _ O
for -X- _ O
500 -X- _ O
K -X- _ O
steps -X- _ O
with -X- _ O
an -X- _ O
Adam -X- _ B-HyperparameterValue
weight -X- _ O
decay -X- _ O
optimizer -X- _ B-HyperparameterName
, -X- _ O
linear -X- _ O
learning -X- _ O
rate -X- _ O
decay -X- _ O
, -X- _ O
and -X- _ O
a -X- _ O
batch -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
of -X- _ O
8192 -X- _ B-HyperparameterValue
, -X- _ O
which -X- _ O
takes -X- _ O
about -X- _ O
5.5 -X- _ O
days -X- _ O
. -X- _ O
It -X- _ O
was -X- _ O

Following -X- _ O
BERT -X- _ B-MethodName
[ -X- _ O
10 -X- _ O
] -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
the -X- _ O
BooksCorpus -X- _ B-DatasetName
[ -X- _ O
40 -X- _ O
] -X- _ O
and -X- _ O
English -X- _ B-DatasetName
Wikipedia -X- _ I-DatasetName
as -X- _ O
part -X- _ O
of -X- _ O
our -X- _ O
pretraining -X- _ O
data -X- _ O
, -X- _ O
which -X- _ O
have -X- _ O
13 -X- _ O
GB -X- _ O
plain -X- _ O
text -X- _ O
combined -X- _ O
. -X- _ O
In -X- _ O
addition -X- _ O
, -X- _ O
we -X- _ O
include -X- _ O
Giga5 -X- _ B-DatasetName
( -X- _ O
16 -X- _ O
GB -X- _ O
text -X- _ O
) -X- _ O
[ -X- _ O
26 -X- _ O
] -X- _ O
, -X- _ O
ClueWeb -X- _ B-DatasetName
2012 -X- _ I-DatasetName
- -X- _ I-DatasetName
B -X- _ I-DatasetName
( -X- _ O
extended -X- _ O
from -X- _ O
[ -X- _ O
5 -X- _ O
] -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
Common -X- _ B-DatasetName
Crawl -X- _ I-DatasetName
[ -X- _ O
6 -X- _ O
] -X- _ O
for -X- _ O
pretraining -X- _ O
. -X- _ O
We -X- _ O
use -X- _ O
heuristics -X- _ O
to -X- _ O
aggressively -X- _ O
ﬁlter -X- _ O
out -X- _ O
short -X- _ O
or -X- _ O
low -X- _ O
- -X- _ O
quality -X- _ O
articles -X- _ O
for -X- _ O
ClueWeb -X- _ B-DatasetName
2012 -X- _ I-DatasetName
- -X- _ I-DatasetName
B -X- _ I-DatasetName
and -X- _ O
Common -X- _ B-DatasetName
Crawl -X- _ I-DatasetName
, -X- _ O
which -X- _ O
results -X- _ O
in -X- _ O
19 -X- _ O
GB -X- _ O
and -X- _ O
110 -X- _ O
GB -X- _ O
text -X- _ O
respectively -X- _ O
. -X- _ O
After -X- _ O
tokenization -X- _ O
with -X- _ O
SentencePiece -X- _ O
[ -X- _ O
17 -X- _ O
] -X- _ O
, -X- _ O
we -X- _ O
obtain -X- _ O
2.78B -X- _ O
, -X- _ O
1.09B -X- _ O
, -X- _ O
4.75B -X- _ O
, -X- _ O
4.30B -X- _ O
, -X- _ O
and -X- _ O
19.97B -X- _ O
subword -X- _ O
pieces -X- _ O
for -X- _ O
Wikipedia -X- _ O
, -X- _ O
BooksCorpus -X- _ B-DatasetName
, -X- _ O
Giga5 -X- _ B-DatasetName
, -X- _ O
ClueWeb -X- _ B-DatasetName
, -X- _ O
and -X- _ O
Common -X- _ B-DatasetName
Crawl -X- _ I-DatasetName
respectively -X- _ O
, -X- _ O
which -X- _ O
are -X- _ O
32.89B -X- _ O
in -X- _ O
total -X- _ O
. -X- _ O

For -X- _ O
more -X- _ O
formal -X- _ O
analysis -X- _ O
and -X- _ O
further -X- _ O
discussion -X- _ O
, -X- _ O
please -X- _ O
refer -X- _ O
to -X- _ O
Appendix -X- _ O
A.5 -X- _ O
. -X- _ O

Notice -X- _ O
that -X- _ O
XLNet -X- _ B-MethodName
is -X- _ O
able -X- _ O
to -X- _ O
capture -X- _ O
the -X- _ O
dependency -X- _ O
between -X- _ O
the -X- _ O
pair -X- _ O
( -X- _ O
New -X- _ O
, -X- _ O
York -X- _ O
) -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
omitted -X- _ O
by -X- _ O
BERT -X- _ B-MethodName
. -X- _ O
Although -X- _ O
in -X- _ O
this -X- _ O
example -X- _ O
, -X- _ O
BERT -X- _ B-MethodName
learns -X- _ O
some -X- _ O
dependency -X- _ O
pairs -X- _ O
such -X- _ O
as -X- _ O
( -X- _ O
New -X- _ O
, -X- _ O
city -X- _ O
) -X- _ O
and -X- _ O
( -X- _ O
York -X- _ O
, -X- _ O
city -X- _ O
) -X- _ O
, -X- _ O
it -X- _ O
is -X- _ O
obvious -X- _ O
that -X- _ O
XLNet -X- _ B-MethodName
always -X- _ O
learns -X- _ O
more -X- _ O
dependency -X- _ O
pairs -X- _ O
given -X- _ O
the -X- _ O
same -X- _ O
target -X- _ O
and -X- _ O
contains -X- _ O
“ -X- _ O
denser -X- _ O
” -X- _ O
effective -X- _ O
training -X- _ O
signals -X- _ O
. -X- _ O

JXLNet -X- _ O
= -X- _ O
log -X- _ O
p(New -X- _ O
| -X- _ O
is -X- _ O
a -X- _ O
city -X- _ O
) -X- _ O
+ -X- _ O
log -X- _ O
p(York -X- _ O
| -X- _ O
New -X- _ O
, -X- _ O
is -X- _ O
a -X- _ O
city -X- _ O
) -X- _ O
. -X- _ O

To -X- _ O
better -X- _ O
understand -X- _ O
the -X- _ O
difference -X- _ O
, -X- _ O
let -X- _ O
’s -X- _ O
consider -X- _ O
a -X- _ O
concrete -X- _ O
example -X- _ O
[ -X- _ O
New -X- _ O
, -X- _ O
York -X- _ O
, -X- _ O
is -X- _ O
, -X- _ O
a -X- _ O
, -X- _ O
city -X- _ O
] -X- _ O
. -X- _ O
Suppose -X- _ O
both -X- _ O
BERT -X- _ B-MethodName
and -X- _ O
XLNet -X- _ B-MethodName
select -X- _ O
the -X- _ O
two -X- _ O
tokens -X- _ O
[ -X- _ O
New -X- _ O
, -X- _ O
York -X- _ O
] -X- _ O
as -X- _ O
the -X- _ O
prediction -X- _ O
targets -X- _ O
and -X- _ O
maximize -X- _ O
log -X- _ O
p(New -X- _ O
York -X- _ O
| -X- _ O
is -X- _ O
a -X- _ O
city -X- _ O
) -X- _ O
. -X- _ O
Also -X- _ O
suppose -X- _ O
that -X- _ O
XLNet -X- _ B-MethodName
samples -X- _ O
the -X- _ O
factorization -X- _ O
order -X- _ O
[ -X- _ O
is -X- _ O
, -X- _ O
a -X- _ O
, -X- _ O
city -X- _ O
, -X- _ O
New -X- _ O
, -X- _ O
York -X- _ O
] -X- _ O
. -X- _ O
In -X- _ O
this -X- _ O
case -X- _ O
, -X- _ O
BERT -X- _ B-MethodName
and -X- _ O
XLNet -X- _ B-MethodName
respectively -X- _ O
reduce -X- _ O
to -X- _ O
the -X- _ O
following -X- _ O
objectives -X- _ O
: -X- _ O

Comparing -X- _ O
Eq -X- _ O
. -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
and -X- _ O
( -X- _ O
5 -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
observe -X- _ O
that -X- _ O
both -X- _ O
BERT -X- _ B-MethodName
and -X- _ O
XLNet -X- _ B-MethodName
perform -X- _ O
partial -X- _ O
prediction -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
only -X- _ O
predicting -X- _ O
a -X- _ O
subset -X- _ O
of -X- _ O
tokens -X- _ O
in -X- _ O
the -X- _ O
sequence -X- _ O
. -X- _ O
This -X- _ O
is -X- _ O
a -X- _ O
necessary -X- _ O
choice -X- _ O
for -X- _ O
BERT -X- _ B-MethodName
because -X- _ O
if -X- _ O
all -X- _ O
tokens -X- _ O
are -X- _ O
masked -X- _ O
, -X- _ O
it -X- _ O
is -X- _ O
impossible -X- _ O
to -X- _ O
make -X- _ O
any -X- _ O
meaningful -X- _ O
predictions -X- _ O
. -X- _ O
In -X- _ O
addition -X- _ O
, -X- _ O
for -X- _ O
both -X- _ O
BERT -X- _ B-MethodName
and -X- _ O
XLNet -X- _ B-MethodName
, -X- _ O
partial -X- _ O
prediction -X- _ O
plays -X- _ O
a -X- _ O
role -X- _ O
of -X- _ O
reducing -X- _ O
optimization -X- _ O
difﬁculty -X- _ O
by -X- _ O
only -X- _ O
predicting -X- _ O
tokens -X- _ O
with -X- _ O
sufﬁcient -X- _ O
context -X- _ O
. -X- _ O
However -X- _ O
, -X- _ O
the -X- _ O
independence -X- _ O
assumption -X- _ O
discussed -X- _ O
in -X- _ O
Section -X- _ O
2.1 -X- _ O
disables -X- _ O
BERT -X- _ B-MethodName
to -X- _ O
model -X- _ O
dependency -X- _ O
between -X- _ O
targets -X- _ O
. -X- _ O

Relative -X- _ O
Segment -X- _ O
Encodings -X- _ O
Architecturally -X- _ O
, -X- _ O
different -X- _ O
from -X- _ O
BERT -X- _ B-MethodName
that -X- _ O
adds -X- _ O
an -X- _ O
absolute -X- _ O
segment -X- _ O
embedding -X- _ O
to -X- _ O
the -X- _ O
word -X- _ O
embedding -X- _ O
at -X- _ O
each -X- _ O
position -X- _ O
, -X- _ O
we -X- _ O
extend -X- _ O
the -X- _ O
idea -X- _ O
of -X- _ O
relative -X- _ O
encodings -X- _ O
from -X- _ O
Transformer -X- _ B-MethodName
- -X- _ I-MethodName
XL -X- _ I-MethodName
to -X- _ O
also -X- _ O
encode -X- _ O
the -X- _ O
segments -X- _ O
. -X- _ O
Given -X- _ O
a -X- _ O
pair -X- _ O
of -X- _ O
positions -X- _ O
i -X- _ O
and -X- _ O
j -X- _ O
in -X- _ O
the -X- _ O
sequence -X- _ O
, -X- _ O
if -X- _ O
i -X- _ O
and -X- _ O
j -X- _ O
are -X- _ O
from -X- _ O
the -X- _ O
same -X- _ O
segment -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
a -X- _ O
segment -X- _ O
encoding -X- _ O
sij -X- _ O
= -X- _ O
s+ -X- _ O
or -X- _ O
otherwise -X- _ O
sij -X- _ O
= -X- _ O
s− -X- _ O
, -X- _ O
where -X- _ O
s+ -X- _ O
and -X- _ O
s− -X- _ O
are -X- _ O
learnable -X- _ O
model -X- _ O
parameters -X- _ O
for -X- _ O
each -X- _ O
attention -X- _ O
head -X- _ O
. -X- _ O
In -X- _ O
other -X- _ O
words -X- _ O
, -X- _ O
we -X- _ O
only -X- _ O
consider -X- _ O
whether -X- _ O
the -X- _ O
two -X- _ O
positions -X- _ O
are -X- _ O
within -X- _ O
the -X- _ O
same -X- _ O
segment -X- _ O
, -X- _ O
as -X- _ O
opposed -X- _ O
to -X- _ O
considering -X- _ O
which -X- _ O
speciﬁc -X- _ O
segments -X- _ O
they -X- _ O
are -X- _ O
from -X- _ O
. -X- _ O
This -X- _ O
is -X- _ O
consistent -X- _ O
with -X- _ O
the -X- _ O
core -X- _ O
idea -X- _ O
of -X- _ O
relative -X- _ O
encodings -X- _ O
; -X- _ O
i.e. -X- _ O
, -X- _ O
only -X- _ O
modeling -X- _ O
the -X- _ O
relationships -X- _ O
between -X- _ O
positions -X- _ O
. -X- _ O
When -X- _ O
i -X- _ O
attends -X- _ O
to -X- _ O
j -X- _ O
, -X- _ O
the -X- _ O
segment -X- _ O
encoding -X- _ O
sij -X- _ O
is -X- _ O
used -X- _ O
to -X- _ O
compute -X- _ O
an -X- _ O
attention -X- _ O
weight -X- _ O
aij -X- _ O
= -X- _ O
( -X- _ O
qi -X- _ O
+ -X- _ O
b)⊤sij -X- _ O
, -X- _ O
where -X- _ O
qi -X- _ O
is -X- _ O
the -X- _ O
query -X- _ O
vector -X- _ O
as -X- _ O
in -X- _ O
a -X- _ O
standard -X- _ O
attention -X- _ O
operation -X- _ O
and -X- _ O
b -X- _ O
is -X- _ O
a -X- _ O
learnable -X- _ O
head -X- _ O
- -X- _ O
speciﬁc -X- _ O
bias -X- _ O
vector -X- _ O
. -X- _ O
Finally -X- _ O
, -X- _ O
the -X- _ O
value -X- _ O
aij -X- _ O
is -X- _ O
added -X- _ O
to -X- _ O
the -X- _ O
normal -X- _ O
attention -X- _ O
weight -X- _ O
. -X- _ O
There -X- _ O
are -X- _ O
two -X- _ O
beneﬁts -X- _ O
of -X- _ O
using -X- _ O
relative -X- _ O
segment -X- _ O
encodings -X- _ O
. -X- _ O
First -X- _ O
, -X- _ O
the -X- _ O
inductive -X- _ O
bias -X- _ O
of -X- _ O
relative -X- _ O
encodings -X- _ O
improves -X- _ O
generalization -X- _ O
[ -X- _ O
9 -X- _ O
] -X- _ O
. -X- _ O
Second -X- _ O
, -X- _ O
it -X- _ O
opens -X- _ O
the -X- _ O
possibility -X- _ O
of -X- _ O
ﬁnetuning -X- _ O
on -X- _ O
tasks -X- _ O
that -X- _ O
have -X- _ O
more -X- _ O
than -X- _ O
two -X- _ O
input -X- _ O
segments -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
not -X- _ O
possible -X- _ O
using -X- _ O
absolute -X- _ O
segment -X- _ O
encodings -X- _ O
. -X- _ O

we -X- _ O
follow -X- _ O
the -X- _ O
two -X- _ O
- -X- _ O
segment -X- _ O
data -X- _ O
format -X- _ O
, -X- _ O
XLNet -X- _ B-MethodName
- -X- _ I-MethodName
Large -X- _ I-MethodName
does -X- _ O
not -X- _ O
use -X- _ O
the -X- _ O
objective -X- _ O
of -X- _ O
next -X- _ O
sentence -X- _ O
prediction -X- _ O
[ -X- _ O
10 -X- _ O
] -X- _ O
as -X- _ O
it -X- _ O
does -X- _ O
not -X- _ O
show -X- _ O
consistent -X- _ O
improvement -X- _ O
in -X- _ O
our -X- _ O
ablation -X- _ O
study -X- _ O
( -X- _ O
see -X- _ O
Section -X- _ O
3.4 -X- _ O
) -X- _ O
. -X- _ O

Many -X- _ O
downstream -X- _ O
tasks -X- _ O
have -X- _ O
multiple -X- _ O
input -X- _ O
segments -X- _ O
, -X- _ O
e.g. -X- _ O
, -X- _ O
a -X- _ O
question -X- _ O
and -X- _ O
a -X- _ O
context -X- _ O
paragraph -X- _ O
in -X- _ O
question -X- _ B-TaskName
answering -X- _ I-TaskName
. -X- _ O
We -X- _ O
now -X- _ O
discuss -X- _ O
how -X- _ O
we -X- _ O
pretrain -X- _ O
XLNet -X- _ B-MethodName
to -X- _ O
model -X- _ O
multiple -X- _ O
segments -X- _ O
in -X- _ O
the -X- _ O
autoregressive -X- _ O
framework -X- _ O
. -X- _ O
During -X- _ O
the -X- _ O
pretraining -X- _ O
phase -X- _ O
, -X- _ O
following -X- _ O
BERT -X- _ B-MethodName
, -X- _ O
we -X- _ O
randomly -X- _ O
sample -X- _ O
two -X- _ O
segments -X- _ O
( -X- _ O
either -X- _ O
from -X- _ O
the -X- _ O
same -X- _ O
context -X- _ O
or -X- _ O
not -X- _ O
) -X- _ O
and -X- _ O
treat -X- _ O
the -X- _ O
concatenation -X- _ O
of -X- _ O
two -X- _ O
segments -X- _ O
as -X- _ O
one -X- _ O
sequence -X- _ O
to -X- _ O
perform -X- _ O
permutation -X- _ O
language -X- _ O
modeling -X- _ O
. -X- _ O
We -X- _ O
only -X- _ O
reuse -X- _ O
the -X- _ O
memory -X- _ O
that -X- _ O
belongs -X- _ O
to -X- _ O
the -X- _ O
same -X- _ O
context -X- _ O
. -X- _ O
Speciﬁcally -X- _ O
, -X- _ O
the -X- _ O
input -X- _ O
to -X- _ O
our -X- _ O
model -X- _ O
is -X- _ O
the -X- _ O
same -X- _ O
as -X- _ O
BERT -X- _ B-MethodName
: -X- _ O
[ -X- _ O
CLS -X- _ O
, -X- _ O
A -X- _ O
, -X- _ O
SEP -X- _ O
, -X- _ O
B -X- _ O
, -X- _ O
SEP -X- _ O
] -X- _ O
, -X- _ O
where -X- _ O
“ -X- _ O
SEP -X- _ O
” -X- _ O
and -X- _ O
“ -X- _ O
CLS -X- _ O
” -X- _ O
are -X- _ O
two -X- _ O
special -X- _ O
symbols -X- _ O
and -X- _ O
“ -X- _ O
A -X- _ O
” -X- _ O
and -X- _ O
“ -X- _ O
B -X- _ O
” -X- _ O
are -X- _ O
the -X- _ O
two -X- _ O
segments -X- _ O
. -X- _ O
Although -X- _ O

where -X- _ O
[ -X- _ O
. -X- _ O
, -X- _ O
. -X- _ O
] -X- _ O
denotes -X- _ O
concatenation -X- _ O
along -X- _ O
the -X- _ O
sequence -X- _ O
dimension -X- _ O
. -X- _ O
Notice -X- _ O
that -X- _ O
positional -X- _ O
encodings -X- _ O
only -X- _ O
depend -X- _ O
on -X- _ O
the -X- _ O
actual -X- _ O
positions -X- _ O
in -X- _ O
the -X- _ O
original -X- _ O
sequence -X- _ O
. -X- _ O
Thus -X- _ O
, -X- _ O
the -X- _ O
above -X- _ O
attention -X- _ O
update -X- _ O
is -X- _ O
independent -X- _ O
of -X- _ O
˜z -X- _ O
once -X- _ O
the -X- _ O
representations -X- _ O
˜h(m -X- _ O
) -X- _ O
are -X- _ O
obtained -X- _ O
. -X- _ O
This -X- _ O
allows -X- _ O
caching -X- _ O
and -X- _ O
reusing -X- _ O
the -X- _ O
memory -X- _ O
without -X- _ O
knowing -X- _ O
the -X- _ O
factorization -X- _ O
order -X- _ O
of -X- _ O
the -X- _ O
previous -X- _ O
segment -X- _ O
. -X- _ O
In -X- _ O
expectation -X- _ O
, -X- _ O
the -X- _ O
model -X- _ O
learns -X- _ O
to -X- _ O
utilize -X- _ O
the -X- _ O
memory -X- _ O
over -X- _ O
all -X- _ O
factorization -X- _ O
orders -X- _ O
of -X- _ O
the -X- _ O
last -X- _ O
segment -X- _ O
. -X- _ O
The -X- _ O
query -X- _ O
stream -X- _ O
can -X- _ O
be -X- _ O
computed -X- _ O
in -X- _ O
the -X- _ O
same -X- _ O
way -X- _ O
. -X- _ O
Finally -X- _ O
, -X- _ O
Figure -X- _ O
1 -X- _ O
( -X- _ O
c -X- _ O
) -X- _ O
presents -X- _ O
an -X- _ O
overview -X- _ O
of -X- _ O
the -X- _ O
proposed -X- _ O
permutation -X- _ O
language -X- _ O
modeling -X- _ O
with -X- _ O
two -X- _ O
- -X- _ O
stream -X- _ O
attention -X- _ O
( -X- _ O
see -X- _ O
Appendix -X- _ O
A.7 -X- _ O
for -X- _ O
more -X- _ O
detailed -X- _ O
illustration -X- _ O
) -X- _ O
. -X- _ O

h(m -X- _ O
) -X- _ O
zt -X- _ O
← -X- _ O
Attention(Q -X- _ O
= -X- _ O
h(m−1 -X- _ O
) -X- _ O
zt -X- _ O
, -X- _ O
KV -X- _ O
= -X- _ O
� -X- _ O
˜h(m−1 -X- _ O
) -X- _ O
, -X- _ O
h(m−1 -X- _ O
) -X- _ O
z≤t -X- _ O
� -X- _ O
; -X- _ O
θ -X- _ O
) -X- _ O

Since -X- _ O
our -X- _ O
objective -X- _ O
function -X- _ O
ﬁts -X- _ O
in -X- _ O
the -X- _ O
AR -X- _ O
framework -X- _ O
, -X- _ O
we -X- _ O
incorporate -X- _ O
the -X- _ O
state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
art -X- _ O
AR -X- _ O
language -X- _ O
model -X- _ O
, -X- _ O
Transformer -X- _ B-MethodName
- -X- _ I-MethodName
XL -X- _ I-MethodName
[ -X- _ O
9 -X- _ O
] -X- _ O
, -X- _ O
into -X- _ O
our -X- _ O
pretraining -X- _ O
framework -X- _ O
, -X- _ O
and -X- _ O
name -X- _ O
our -X- _ O
method -X- _ O
after -X- _ O
it -X- _ O
. -X- _ O
We -X- _ O
integrate -X- _ O
two -X- _ O
important -X- _ O
techniques -X- _ O
in -X- _ O
Transformer -X- _ B-MethodName
- -X- _ I-MethodName
XL -X- _ I-MethodName
, -X- _ O
namely -X- _ O
the -X- _ O
relative -X- _ O
positional -X- _ O
encoding -X- _ O
scheme -X- _ O
and -X- _ O
the -X- _ O
segment -X- _ O
recurrence -X- _ O
mechanism -X- _ O
. -X- _ O
We -X- _ O
apply -X- _ O
relative -X- _ O
positional -X- _ O
encodings -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
original -X- _ O
sequence -X- _ O
as -X- _ O
discussed -X- _ O
earlier -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
straightforward -X- _ O
. -X- _ O
Now -X- _ O
we -X- _ O
discuss -X- _ O
how -X- _ O
to -X- _ O
integrate -X- _ O
the -X- _ O
recurrence -X- _ O
mechanism -X- _ O
into -X- _ O
the -X- _ O
proposed -X- _ O
permutation -X- _ O
setting -X- _ O
and -X- _ O
enable -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
reuse -X- _ O
hidden -X- _ O
states -X- _ O
from -X- _ O
previous -X- _ O
segments -X- _ O
. -X- _ O
Without -X- _ O
loss -X- _ O
of -X- _ O
generality -X- _ O
, -X- _ O
suppose -X- _ O
we -X- _ O
have -X- _ O
two -X- _ O
segments -X- _ O
taken -X- _ O
from -X- _ O
a -X- _ O
long -X- _ O
sequence -X- _ O
s -X- _ O
; -X- _ O
i.e. -X- _ O
, -X- _ O
˜x -X- _ O
= -X- _ O
s1 -X- _ O
: -X- _ O
T -X- _ O
and -X- _ O
x -X- _ O
= -X- _ O
sT -X- _ O
+1:2 -X- _ O
T -X- _ O
. -X- _ O
Let -X- _ O
˜z -X- _ O
and -X- _ O
z -X- _ O
be -X- _ O
permutations -X- _ O
of -X- _ O
[ -X- _ O
1 -X- _ O
· -X- _ O
· -X- _ O
· -X- _ O
T -X- _ O
] -X- _ O
and -X- _ O
[ -X- _ O
T -X- _ O
+ -X- _ O
1 -X- _ O
· -X- _ O
· -X- _ O
· -X- _ O
2 -X- _ O
T -X- _ O
] -X- _ O
respectively -X- _ O
. -X- _ O
Then -X- _ O
, -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
permutation -X- _ O
˜z -X- _ O
, -X- _ O
we -X- _ O
process -X- _ O
the -X- _ O
ﬁrst -X- _ O
segment -X- _ O
, -X- _ O
and -X- _ O
then -X- _ O
cache -X- _ O
the -X- _ O
obtained -X- _ O
content -X- _ O
representations -X- _ O
˜h(m -X- _ O
) -X- _ O
for -X- _ O
each -X- _ O
layer -X- _ O
m. -X- _ O
Then -X- _ O
, -X- _ O
for -X- _ O
the -X- _ O
next -X- _ O
segment -X- _ O
x -X- _ O
, -X- _ O
the -X- _ O
attention -X- _ O
update -X- _ O
with -X- _ O
memory -X- _ O
can -X- _ O
be -X- _ O
written -X- _ O
as -X- _ O

Note -X- _ O
that -X- _ O
z -X- _ O
> -X- _ O
c -X- _ O
is -X- _ O
chosen -X- _ O
as -X- _ O
the -X- _ O
target -X- _ O
because -X- _ O
it -X- _ O
possesses -X- _ O
the -X- _ O
longest -X- _ O
context -X- _ O
in -X- _ O
the -X- _ O
sequence -X- _ O
given -X- _ O
the -X- _ O
current -X- _ O
factorization -X- _ O
order -X- _ O
z. -X- _ O
A -X- _ O
hyperparameter -X- _ O
K -X- _ B-HyperparameterName
is -X- _ O
used -X- _ O
such -X- _ O
that -X- _ O
about -X- _ O
1 -X- _ O
/ -X- _ O
K -X- _ O
tokens -X- _ O
are -X- _ O
selected -X- _ O
for -X- _ O
predictions -X- _ O
; -X- _ O
i.e. -X- _ O
, -X- _ O
|z| -X- _ O
/(|z| -X- _ O
− -X- _ O
c -X- _ O
) -X- _ O
≈ -X- _ O
K. -X- _ O
For -X- _ O
unselected -X- _ O
tokens -X- _ O
, -X- _ O
their -X- _ O
query -X- _ O
representations -X- _ O
need -X- _ O
not -X- _ O
be -X- _ O
computed -X- _ O
, -X- _ O
which -X- _ O
saves -X- _ O
speed -X- _ O
and -X- _ O
memory -X- _ O
. -X- _ O

Partial -X- _ O
Prediction -X- _ O
While -X- _ O
the -X- _ O
permutation -X- _ O
language -X- _ O
modeling -X- _ O
objective -X- _ O
( -X- _ O
3 -X- _ O
) -X- _ O
has -X- _ O
several -X- _ O
beneﬁts -X- _ O
, -X- _ O
it -X- _ O
is -X- _ O
a -X- _ O
much -X- _ O
more -X- _ O
challenging -X- _ O
optimization -X- _ O
problem -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
permutation -X- _ O
and -X- _ O
causes -X- _ O
slow -X- _ O
convergence -X- _ O
in -X- _ O
preliminary -X- _ O
experiments -X- _ O
. -X- _ O
To -X- _ O
reduce -X- _ O
the -X- _ O
optimization -X- _ O
difﬁculty -X- _ O
, -X- _ O
we -X- _ O
choose -X- _ O
to -X- _ O
only -X- _ O
predict -X- _ O
the -X- _ O
last -X- _ O
tokens -X- _ O
in -X- _ O
a -X- _ O
factorization -X- _ O
order -X- _ O
. -X- _ O
Formally -X- _ O
, -X- _ O
we -X- _ O
split -X- _ O
z -X- _ O
into -X- _ O
a -X- _ O
non -X- _ O
- -X- _ O
target -X- _ O
subsequence -X- _ O
z≤c -X- _ O
and -X- _ O
a -X- _ O
target -X- _ O
subsequence -X- _ O
z -X- _ O
> -X- _ O
c -X- _ O
, -X- _ O
where -X- _ O
c -X- _ O
is -X- _ O
the -X- _ O
cutting -X- _ O
point -X- _ O
. -X- _ O
The -X- _ O
objective -X- _ O
is -X- _ O
to -X- _ O
maximize -X- _ O
the -X- _ O
log -X- _ B-MetricName
- -X- _ I-MetricName
likelihood -X- _ I-MetricName
of -X- _ O
the -X- _ O
target -X- _ O
subsequence -X- _ O
conditioned -X- _ O
on -X- _ O
the -X- _ O
non -X- _ O
- -X- _ O
target -X- _ O
subsequence -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O

where -X- _ O
Q -X- _ O
, -X- _ O
K -X- _ O
, -X- _ O
V -X- _ O
denote -X- _ O
the -X- _ O
query -X- _ O
, -X- _ O
key -X- _ O
, -X- _ O
and -X- _ O
value -X- _ O
in -X- _ O
an -X- _ O
attention -X- _ O
operation -X- _ O
[ -X- _ O
33 -X- _ O
] -X- _ O
. -X- _ O
The -X- _ O
update -X- _ O
rule -X- _ O
of -X- _ O
the -X- _ O
content -X- _ O
representations -X- _ O
is -X- _ O
exactly -X- _ O
the -X- _ O
same -X- _ O
as -X- _ O
the -X- _ O
standard -X- _ O
self -X- _ O
- -X- _ O
attention -X- _ O
, -X- _ O
so -X- _ O
during -X- _ O
ﬁnetuning -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
simply -X- _ O
drop -X- _ O
the -X- _ O
query -X- _ O
stream -X- _ O
and -X- _ O
use -X- _ O
the -X- _ O
content -X- _ O
stream -X- _ O
as -X- _ O
a -X- _ O
normal -X- _ O
Transformer(-XL -X- _ B-MethodName
) -X- _ I-MethodName
. -X- _ I-MethodName
Finally -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
use -X- _ O
the -X- _ O
last -X- _ O
- -X- _ O
layer -X- _ O
query -X- _ O
representation -X- _ O
g(M -X- _ O
) -X- _ O
zt -X- _ O
to -X- _ O
compute -X- _ O
Eq -X- _ O
. -X- _ O
( -X- _ O
4 -X- _ O
) -X- _ O
. -X- _ O

with -X- _ O
a -X- _ O
shared -X- _ O
set -X- _ O
of -X- _ O
parameters -X- _ O
as -X- _ O
follows -X- _ O
( -X- _ O
illustrated -X- _ O
in -X- _ O
Figures -X- _ O
1 -X- _ O
( -X- _ O
a -X- _ O
) -X- _ O
and -X- _ O
( -X- _ O
b -X- _ O
) -X- _ O
): -X- _ O

Computationally -X- _ O
, -X- _ O
the -X- _ O
ﬁrst -X- _ O
layer -X- _ O
query -X- _ O
stream -X- _ O
is -X- _ O
initialized -X- _ O
with -X- _ O
a -X- _ O
trainable -X- _ O
vector -X- _ O
, -X- _ O
i.e. -X- _ O
g(0 -X- _ O
) -X- _ O
i -X- _ O
= -X- _ O
w -X- _ O
, -X- _ O
while -X- _ O
the -X- _ O
content -X- _ O
stream -X- _ O
is -X- _ O
set -X- _ O
to -X- _ O
the -X- _ O
corresponding -X- _ O
word -X- _ O
embedding -X- _ O
, -X- _ O
i.e. -X- _ O
h(0 -X- _ O
) -X- _ O
i -X- _ O
= -X- _ O
e(xi -X- _ O
) -X- _ O
. -X- _ O
For -X- _ O
each -X- _ O
self -X- _ O
- -X- _ O
attention -X- _ O
layer -X- _ O
m -X- _ O
= -X- _ O
1 -X- _ O
, -X- _ O
. -X- _ O
. -X- _ O
. -X- _ O
, -X- _ O
M -X- _ O
, -X- _ O
the -X- _ O
two -X- _ O
streams -X- _ O
of -X- _ O
representations -X- _ O
are -X- _ O
schematically2 -X- _ O
updated -X- _ O

• -X- _ O
The -X- _ O
query -X- _ O
representation -X- _ O
gθ(xz -X- _ O
< -X- _ O
t -X- _ O
, -X- _ O
zt -X- _ O
) -X- _ O
, -X- _ O
or -X- _ O
abbreviated -X- _ O
as -X- _ O
gzt -X- _ O
, -X- _ O
which -X- _ O
only -X- _ O
has -X- _ O
access -X- _ O
to -X- _ O
the -X- _ O
contex- -X- _ O
tual -X- _ O
information -X- _ O
xz -X- _ O
< -X- _ O
t -X- _ O
and -X- _ O
the -X- _ O
position -X- _ O
zt -X- _ O
, -X- _ O
but -X- _ O
not -X- _ O
the -X- _ O
content -X- _ O
xzt -X- _ O
, -X- _ O
as -X- _ O
discussed -X- _ O
above -X- _ O
. -X- _ O

• -X- _ O
The -X- _ O
content -X- _ O
representation -X- _ O
hθ(xz≤t -X- _ O
) -X- _ O
, -X- _ O
or -X- _ O
abbreviated -X- _ O
as -X- _ O
hzt -X- _ O
, -X- _ O
which -X- _ O
serves -X- _ O
a -X- _ O
similar -X- _ O
role -X- _ O
to -X- _ O
the -X- _ O
standard -X- _ O
hidden -X- _ O
states -X- _ O
in -X- _ O
Transformer -X- _ O
. -X- _ O
This -X- _ O
representation -X- _ O
encodes -X- _ O
both -X- _ O
the -X- _ O
context -X- _ O
and -X- _ O
xzt -X- _ O
itself -X- _ O
. -X- _ O

Two -X- _ O
- -X- _ O
Stream -X- _ O
Self -X- _ O
- -X- _ O
Attention -X- _ O
While -X- _ O
the -X- _ O
idea -X- _ O
of -X- _ O
target -X- _ O
- -X- _ O
aware -X- _ O
representations -X- _ O
removes -X- _ O
the -X- _ O
ambiguity -X- _ O
in -X- _ O
target -X- _ O
prediction -X- _ O
, -X- _ O
how -X- _ O
to -X- _ O
formulate -X- _ O
gθ(xz -X- _ O
< -X- _ O
t -X- _ O
, -X- _ O
zt -X- _ O
) -X- _ O
remains -X- _ O
a -X- _ O
non -X- _ O
- -X- _ O
trivial -X- _ O
problem -X- _ O
. -X- _ O
Among -X- _ O
other -X- _ O
possibilities -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
to -X- _ O
“ -X- _ O
stand -X- _ O
” -X- _ O
at -X- _ O
the -X- _ O
target -X- _ O
position -X- _ O
zt -X- _ O
and -X- _ O
rely -X- _ O
on -X- _ O
the -X- _ O
position -X- _ O
zt -X- _ O
to -X- _ O
gather -X- _ O
information -X- _ O
from -X- _ O
the -X- _ O
context -X- _ O
xz -X- _ O
< -X- _ O
t -X- _ O
through -X- _ O
attention -X- _ O
. -X- _ O
For -X- _ O
this -X- _ O
parameterization -X- _ O
to -X- _ O
work -X- _ O
, -X- _ O
there -X- _ O
are -X- _ O
two -X- _ O
requirements -X- _ O
that -X- _ O
are -X- _ O
contradictory -X- _ O
in -X- _ O
a -X- _ O
standard -X- _ O
Transformer -X- _ O
architecture -X- _ O
: -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
to -X- _ O
predict -X- _ O
the -X- _ O
token -X- _ O
xzt -X- _ O
, -X- _ O
gθ(xz -X- _ O
< -X- _ O
t -X- _ O
, -X- _ O
zt -X- _ O
) -X- _ O
should -X- _ O
only -X- _ O
use -X- _ O
the -X- _ O
position -X- _ O
zt -X- _ O
and -X- _ O
not -X- _ O
the -X- _ O
content -X- _ O
xzt -X- _ O
, -X- _ O
otherwise -X- _ O
the -X- _ O
objective -X- _ O
becomes -X- _ O
trivial -X- _ O
; -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
to -X- _ O
predict -X- _ O
the -X- _ O
other -X- _ O
tokens -X- _ O
xzj -X- _ O
with -X- _ O
j -X- _ O
> -X- _ O
t -X- _ O
, -X- _ O
gθ(xz -X- _ O
< -X- _ O
t -X- _ O
, -X- _ O
zt -X- _ O
) -X- _ O
should -X- _ O
also -X- _ O
encode -X- _ O
the -X- _ O
content -X- _ O
xzt -X- _ O
to -X- _ O
provide -X- _ O
full -X- _ O
contextual -X- _ O
information -X- _ O
. -X- _ O
To -X- _ O
resolve -X- _ O
such -X- _ O
a -X- _ O
contradiction -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
to -X- _ O
use -X- _ O
two -X- _ O
sets -X- _ O
of -X- _ O
hidden -X- _ O
representations -X- _ O
instead -X- _ O
of -X- _ O
one -X- _ O
: -X- _ O

where -X- _ O
gθ(xz -X- _ O
< -X- _ O
t -X- _ O
, -X- _ O
zt -X- _ O
) -X- _ O
denotes -X- _ O
a -X- _ O
new -X- _ O
type -X- _ O
of -X- _ O
representations -X- _ O
which -X- _ O
additionally -X- _ O
take -X- _ O
the -X- _ O
target -X- _ O
position -X- _ O
zt -X- _ O
as -X- _ O
input -X- _ O
. -X- _ O

x′ -X- _ O
exp(e(x′)⊤hθ(xz -X- _ O
< -X- _ O
t -X- _ O
) -X- _ O
) -X- _ O
, -X- _ O
where -X- _ O
hθ(xz -X- _ O
< -X- _ O
t -X- _ O
) -X- _ O
denotes -X- _ O
the -X- _ O
hidden -X- _ O
representation -X- _ O
of -X- _ O
xz -X- _ O
< -X- _ O
t -X- _ O
produced -X- _ O
by -X- _ O
the -X- _ O
shared -X- _ O
Transformer -X- _ O
network -X- _ O
after -X- _ O
proper -X- _ O
masking -X- _ O
. -X- _ O
Now -X- _ O
notice -X- _ O
that -X- _ O
the -X- _ O
representation -X- _ O
hθ(xz -X- _ O
< -X- _ O
t -X- _ O
) -X- _ O
does -X- _ O
not -X- _ O
depend -X- _ O
on -X- _ O
which -X- _ O
position -X- _ O
it -X- _ O
will -X- _ O
predict -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
the -X- _ O
value -X- _ O
of -X- _ O
zt -X- _ O
. -X- _ O
Consequently -X- _ O
, -X- _ O
the -X- _ O
same -X- _ O
distribution -X- _ O
is -X- _ O
predicted -X- _ O
regardless -X- _ O
of -X- _ O
the -X- _ O
target -X- _ O
position -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
not -X- _ O
able -X- _ O
to -X- _ O
learn -X- _ O
useful -X- _ O
representations -X- _ O
( -X- _ O
see -X- _ O
Appendix -X- _ O
A.1 -X- _ O
for -X- _ O
a -X- _ O
concrete -X- _ O
example -X- _ O
) -X- _ O
. -X- _ O
To -X- _ O
avoid -X- _ O
this -X- _ O
problem -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
to -X- _ O
re -X- _ O
- -X- _ O
parameterize -X- _ O
the -X- _ O
next -X- _ O
- -X- _ O
token -X- _ O
distribution -X- _ O
to -X- _ O
be -X- _ O
target -X- _ O
position -X- _ O
aware -X- _ O
: -X- _ O

2.3 -X- _ O
Architecture -X- _ O
: -X- _ O
Two -X- _ O
- -X- _ O
Stream -X- _ O
Self -X- _ O
- -X- _ O
Attention -X- _ O
for -X- _ O
Target -X- _ O
- -X- _ O
Aware -X- _ O
Representations -X- _ O

To -X- _ O
provide -X- _ O
an -X- _ O
overall -X- _ O
picture -X- _ O
, -X- _ O
we -X- _ O
show -X- _ O
an -X- _ O
example -X- _ O
of -X- _ O
predicting -X- _ O
the -X- _ O
token -X- _ O
x3 -X- _ O
given -X- _ O
the -X- _ O
same -X- _ O
input -X- _ O
sequence -X- _ O
x -X- _ O
but -X- _ O
under -X- _ O
different -X- _ O
factorization -X- _ O
orders -X- _ O
in -X- _ O
the -X- _ O
Appendix -X- _ O
A.7 -X- _ O
with -X- _ O
Figure -X- _ O
4 -X- _ O
. -X- _ O

Remark -X- _ O
on -X- _ O
Permutation -X- _ O
The -X- _ O
proposed -X- _ O
objective -X- _ O
only -X- _ O
permutes -X- _ O
the -X- _ O
factorization -X- _ O
order -X- _ O
, -X- _ O
not -X- _ O
the -X- _ O
sequence -X- _ O
order -X- _ O
. -X- _ O
In -X- _ O
other -X- _ O
words -X- _ O
, -X- _ O
we -X- _ O
keep -X- _ O
the -X- _ O
original -X- _ O
sequence -X- _ O
order -X- _ O
, -X- _ O
use -X- _ O
the -X- _ O
positional -X- _ O
encodings -X- _ O
corresponding -X- _ O
to -X- _ O
the -X- _ O
original -X- _ O
sequence -X- _ O
, -X- _ O
and -X- _ O
rely -X- _ O
on -X- _ O
a -X- _ O
proper -X- _ O
attention -X- _ O
mask -X- _ O
in -X- _ O
Transformers -X- _ O
to -X- _ O
achieve -X- _ O
permutation -X- _ O
of -X- _ O
the -X- _ O
factorization -X- _ O
order -X- _ O
. -X- _ O
Note -X- _ O
that -X- _ O
this -X- _ O
choice -X- _ O
is -X- _ O
necessary -X- _ O
, -X- _ O
since -X- _ O
the -X- _ O
model -X- _ O
will -X- _ O
only -X- _ O
encounter -X- _ O
text -X- _ O
sequences -X- _ O
with -X- _ O
the -X- _ O
natural -X- _ O
order -X- _ O
during -X- _ O
ﬁnetuning -X- _ O
. -X- _ O

Essentially -X- _ O
, -X- _ O
for -X- _ O
a -X- _ O
text -X- _ O
sequence -X- _ O
x -X- _ O
, -X- _ O
we -X- _ O
sample -X- _ O
a -X- _ O
factorization -X- _ O
order -X- _ O
z -X- _ O
at -X- _ O
a -X- _ O
time -X- _ O
and -X- _ O
decompose -X- _ O
the -X- _ O
likelihood -X- _ B-MetricName
pθ(x -X- _ O
) -X- _ O
according -X- _ O
to -X- _ O
factorization -X- _ O
order -X- _ O
. -X- _ O
Since -X- _ O
the -X- _ O
same -X- _ O
model -X- _ O
parameter -X- _ O
θ -X- _ O
is -X- _ O
shared -X- _ O
across -X- _ O
all -X- _ O
factorization -X- _ O
orders -X- _ O
during -X- _ O
training -X- _ O
, -X- _ O
in -X- _ O
expectation -X- _ O
, -X- _ O
xt -X- _ O
has -X- _ O
seen -X- _ O
every -X- _ O
possible -X- _ O
element -X- _ O
xi -X- _ O
̸= -X- _ O
xt -X- _ O
in -X- _ O
the -X- _ O
sequence -X- _ O
, -X- _ O
hence -X- _ O
being -X- _ O
able -X- _ O
to -X- _ O
capture -X- _ O
the -X- _ O
bidirectional -X- _ O
context -X- _ O
. -X- _ O
Moreover -X- _ O
, -X- _ O
as -X- _ O
this -X- _ O
objective -X- _ O
ﬁts -X- _ O
into -X- _ O
the -X- _ O
AR -X- _ O
framework -X- _ O
, -X- _ O
it -X- _ O
naturally -X- _ O
avoids -X- _ O
the -X- _ O
independence -X- _ O
assumption -X- _ O
and -X- _ O
the -X- _ O
pretrain-ﬁnetune -X- _ O
discrepancy -X- _ O
discussed -X- _ O
in -X- _ O
Section -X- _ O
2.1 -X- _ O
. -X- _ O

• -X- _ O
Independence -X- _ O
Assumption -X- _ O
: -X- _ O
As -X- _ O
emphasized -X- _ O
by -X- _ O
the -X- _ O
≈ -X- _ O
sign -X- _ O
in -X- _ O
Eq -X- _ O
. -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
, -X- _ O
BERT -X- _ B-MethodName
factorizes -X- _ O
the -X- _ O
joint -X- _ O
conditional -X- _ O
probability -X- _ O
p(¯x -X- _ O
| -X- _ O
ˆx -X- _ O
) -X- _ O
based -X- _ O
on -X- _ O
an -X- _ O
independence -X- _ O
assumption -X- _ O
that -X- _ O
all -X- _ O
masked -X- _ O
tokens -X- _ O
¯x -X- _ O
are -X- _ O
separately -X- _ O
reconstructed -X- _ O
. -X- _ O
In -X- _ O
comparison -X- _ O
, -X- _ O
the -X- _ O
AR -X- _ B-MethodName
language -X- _ I-MethodName
modeling -X- _ I-MethodName
objective -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
factorizes -X- _ O
pθ(x -X- _ B-MetricName
) -X- _ O
using -X- _ O
the -X- _ O
product -X- _ O
rule -X- _ O
that -X- _ O
holds -X- _ O
universally -X- _ O
without -X- _ O
such -X- _ O
an -X- _ O
independence -X- _ O
assumption -X- _ O
. -X- _ O

To -X- _ O
formalize -X- _ O
the -X- _ O
idea -X- _ O
, -X- _ O
let -X- _ O
ZT -X- _ O
be -X- _ O
the -X- _ O
set -X- _ O
of -X- _ O
all -X- _ O
possible -X- _ O
permutations -X- _ O
of -X- _ O
the -X- _ O
length -X- _ O
- -X- _ O
T -X- _ O
index -X- _ O
sequence -X- _ O
[ -X- _ O
1 -X- _ O
, -X- _ O
2 -X- _ O
, -X- _ O
. -X- _ O
. -X- _ O
. -X- _ O
, -X- _ O
T -X- _ O
] -X- _ O
. -X- _ O
We -X- _ O
use -X- _ O
zt -X- _ O
and -X- _ O
z -X- _ O
< -X- _ O
t -X- _ O
to -X- _ O
denote -X- _ O
the -X- _ O
t -X- _ O
- -X- _ O
th -X- _ O
element -X- _ O
and -X- _ O
the -X- _ O
ﬁrst -X- _ O
t−1 -X- _ O
elements -X- _ O
of -X- _ O
a -X- _ O
permutation -X- _ O
z -X- _ O
∈ -X- _ O
ZT -X- _ O
. -X- _ O
Then -X- _ O
, -X- _ O
our -X- _ O
proposed -X- _ O
permutation -X- _ O
language -X- _ O
modeling -X- _ O
objective -X- _ O
can -X- _ O
be -X- _ O
expressed -X- _ O
as -X- _ O
follows -X- _ O
: -X- _ O

Borrowing -X- _ O
ideas -X- _ O
from -X- _ O
orderless -X- _ O
NADE -X- _ B-MethodName
[ -X- _ O
32 -X- _ O
] -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
the -X- _ O
permutation -X- _ O
language -X- _ O
modeling -X- _ O
objective -X- _ O
that -X- _ O
not -X- _ O
only -X- _ O
retains -X- _ O
the -X- _ O
beneﬁts -X- _ O
of -X- _ O
AR -X- _ O
models -X- _ O
but -X- _ O
also -X- _ O
allows -X- _ O
models -X- _ O
to -X- _ O
capture -X- _ O
bidirectional -X- _ O
contexts -X- _ O
. -X- _ O
Speciﬁcally -X- _ O
, -X- _ O
for -X- _ O
a -X- _ O
sequence -X- _ O
x -X- _ O
of -X- _ O
length -X- _ O
T -X- _ O
, -X- _ O
there -X- _ O
are -X- _ O
T -X- _ O
! -X- _ O
different -X- _ O
orders -X- _ O
to -X- _ O
perform -X- _ O
a -X- _ O
valid -X- _ O
autoregressive -X- _ O
factorization -X- _ O
. -X- _ O
Intuitively -X- _ O
, -X- _ O
if -X- _ O
model -X- _ O
parameters -X- _ O
are -X- _ O
shared -X- _ O
across -X- _ O
all -X- _ O
factorization -X- _ O
orders -X- _ O
, -X- _ O
in -X- _ O
expectation -X- _ O
, -X- _ O
the -X- _ O
model -X- _ O
will -X- _ O
learn -X- _ O
to -X- _ O
gather -X- _ O
information -X- _ O
from -X- _ O
all -X- _ O
positions -X- _ O
on -X- _ O
both -X- _ O
sides -X- _ O
. -X- _ O

According -X- _ O
to -X- _ O
the -X- _ O
comparison -X- _ O
above -X- _ O
, -X- _ O
AR -X- _ B-MethodName
language -X- _ I-MethodName
modeling -X- _ I-MethodName
and -X- _ O
BERT -X- _ B-MethodName
possess -X- _ O
their -X- _ O
unique -X- _ O
advan- -X- _ O
tages -X- _ O
over -X- _ O
the -X- _ O
other -X- _ O
. -X- _ O
A -X- _ O
natural -X- _ O
question -X- _ O
to -X- _ O
ask -X- _ O
is -X- _ O
whether -X- _ O
there -X- _ O
exists -X- _ O
a -X- _ O
pretraining -X- _ O
objective -X- _ O
that -X- _ O
brings -X- _ O
the -X- _ O
advantages -X- _ O
of -X- _ O
both -X- _ O
while -X- _ O
avoiding -X- _ O
their -X- _ O
weaknesses -X- _ O
. -X- _ O

• -X- _ O
Context -X- _ O
dependency -X- _ O
: -X- _ O
The -X- _ O
AR -X- _ O
representation -X- _ O
hθ(x1 -X- _ O
: -X- _ O
t−1 -X- _ O
) -X- _ O
is -X- _ O
only -X- _ O
conditioned -X- _ O
on -X- _ O
the -X- _ O
tokens -X- _ O
up -X- _ O
to -X- _ O
position -X- _ O
t -X- _ O
( -X- _ O
i.e. -X- _ O
tokens -X- _ O
to -X- _ O
the -X- _ O
left -X- _ O
) -X- _ O
, -X- _ O
while -X- _ O
the -X- _ O
BERT -X- _ B-MethodName
representation -X- _ O
Hθ(x)t -X- _ O
has -X- _ O
access -X- _ O
to -X- _ O
the -X- _ O
contextual -X- _ O
information -X- _ O
on -X- _ O
both -X- _ O
sides -X- _ O
. -X- _ O
As -X- _ O
a -X- _ O
result -X- _ O
, -X- _ O
the -X- _ O
BERT -X- _ B-MethodName
objective -X- _ O
allows -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
be -X- _ O
pretrained -X- _ O
to -X- _ O
better -X- _ O
capture -X- _ O
bidirectional -X- _ O
context -X- _ O
. -X- _ O

• -X- _ O
Input -X- _ O
noise -X- _ O
: -X- _ O
The -X- _ O
input -X- _ O
to -X- _ O
BERT -X- _ B-MethodName
contains -X- _ O
artiﬁcial -X- _ O
symbols -X- _ O
like -X- _ O
[ -X- _ O
MASK -X- _ O
] -X- _ O
that -X- _ O
never -X- _ O
occur -X- _ O
in -X- _ O
downstream -X- _ O
tasks -X- _ O
, -X- _ O
which -X- _ O
creates -X- _ O
a -X- _ O
pretrain-ﬁnetune -X- _ O
discrepancy -X- _ O
. -X- _ O
Replacing -X- _ O
[ -X- _ O
MASK -X- _ O
] -X- _ O
with -X- _ O
original -X- _ O
tokens -X- _ O
as -X- _ O
in -X- _ O
[ -X- _ O
10 -X- _ O
] -X- _ O
does -X- _ O
not -X- _ O
solve -X- _ O
the -X- _ O
problem -X- _ O
because -X- _ O
original -X- _ O
tokens -X- _ O
can -X- _ O
be -X- _ O
only -X- _ O
used -X- _ O
with -X- _ O
a -X- _ O
small -X- _ O
probability -X- _ O
— -X- _ O
otherwise -X- _ O
Eq -X- _ O
. -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
will -X- _ O
be -X- _ O
trivial -X- _ O
to -X- _ O
optimize -X- _ O
. -X- _ O
In -X- _ O
comparison -X- _ O
, -X- _ O
AR -X- _ B-MethodName
language -X- _ I-MethodName
modeling -X- _ I-MethodName
does -X- _ O
not -X- _ O
rely -X- _ O
on -X- _ O
any -X- _ O
input -X- _ O
corruption -X- _ O
and -X- _ O
does -X- _ O
not -X- _ O
suffer -X- _ O
from -X- _ O
this -X- _ O
issue -X- _ O
. -X- _ O

where -X- _ O
mt -X- _ O
= -X- _ O
1 -X- _ O
indicates -X- _ O
xt -X- _ O
is -X- _ O
masked -X- _ O
, -X- _ O
and -X- _ O
Hθ -X- _ O
is -X- _ O
a -X- _ O
Transformer -X- _ O
that -X- _ O
maps -X- _ O
a -X- _ O
length -X- _ O
- -X- _ O
T -X- _ O
text -X- _ O
sequence -X- _ O
x -X- _ O
into -X- _ O
a -X- _ O
sequence -X- _ O
of -X- _ O
hidden -X- _ O
vectors -X- _ O
Hθ(x -X- _ O
) -X- _ O
= -X- _ O
[ -X- _ O
Hθ(x)1 -X- _ O
, -X- _ O
Hθ(x)2 -X- _ O
, -X- _ O
· -X- _ O
· -X- _ O
· -X- _ O
, -X- _ O
Hθ(x)T -X- _ O
] -X- _ O
. -X- _ O
The -X- _ O
pros -X- _ O
and -X- _ O
cons -X- _ O
of -X- _ O
the -X- _ O
two -X- _ O
pretraining -X- _ O
objectives -X- _ O
are -X- _ O
compared -X- _ O
in -X- _ O
the -X- _ O
following -X- _ O
aspects -X- _ O
: -X- _ O

where -X- _ O
hθ(x1 -X- _ O
: -X- _ O
t−1 -X- _ O
) -X- _ O
is -X- _ O
a -X- _ O
context -X- _ O
representation -X- _ O
produced -X- _ O
by -X- _ O
neural -X- _ O
models -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
RNNs -X- _ O
or -X- _ O
Transform- -X- _ O
ers -X- _ O
, -X- _ O
and -X- _ O
e(x -X- _ O
) -X- _ O
denotes -X- _ O
the -X- _ O
embedding -X- _ O
of -X- _ O
x. -X- _ O
In -X- _ O
comparison -X- _ O
, -X- _ O
BERT -X- _ B-MethodName
is -X- _ O
based -X- _ O
on -X- _ O
denoising -X- _ O
auto -X- _ O
- -X- _ O
encoding -X- _ O
. -X- _ O
Speciﬁcally -X- _ O
, -X- _ O
for -X- _ O
a -X- _ O
text -X- _ O
sequence -X- _ O
x -X- _ O
, -X- _ O
BERT -X- _ B-MethodName
ﬁrst -X- _ O
constructs -X- _ O
a -X- _ O
corrupted -X- _ O
version -X- _ O
ˆx -X- _ O
by -X- _ O
randomly -X- _ O
setting -X- _ O
a -X- _ O
portion -X- _ O
( -X- _ O
e.g. -X- _ O
15 -X- _ O
% -X- _ O
) -X- _ O
of -X- _ O
tokens -X- _ O
in -X- _ O
x -X- _ O
to -X- _ O
a -X- _ O
special -X- _ O
symbol -X- _ O
[ -X- _ O
MASK -X- _ O
] -X- _ O
. -X- _ O
Let -X- _ O
the -X- _ O
masked -X- _ O
tokens -X- _ O
be -X- _ O
¯x -X- _ O
. -X- _ O
The -X- _ O
training -X- _ O
objective -X- _ O
is -X- _ O
to -X- _ O
reconstruct -X- _ O
¯x -X- _ O
from -X- _ O
ˆx -X- _ O
: -X- _ O

In -X- _ O
this -X- _ O
section -X- _ O
, -X- _ O
we -X- _ O
ﬁrst -X- _ O
review -X- _ O
and -X- _ O
compare -X- _ O
the -X- _ O
conventional -X- _ O
AR -X- _ B-MethodName
language -X- _ I-MethodName
modeling -X- _ I-MethodName
and -X- _ O
BERT -X- _ B-MethodName
for -X- _ O
language -X- _ O
pretraining -X- _ O
. -X- _ O
Given -X- _ O
a -X- _ O
text -X- _ O
sequence -X- _ O
x -X- _ O
= -X- _ O
[ -X- _ O
x1 -X- _ O
, -X- _ O
· -X- _ O
· -X- _ O
· -X- _ O
, -X- _ O
xT -X- _ O
] -X- _ O
, -X- _ O
AR -X- _ B-MethodName
language -X- _ I-MethodName
modeling -X- _ I-MethodName
performs -X- _ O
pretraining -X- _ O
by -X- _ O
maximizing -X- _ O
the -X- _ O
likelihood -X- _ B-MetricName
under -X- _ O
the -X- _ O
forward -X- _ O
autoregressive -X- _ O
factorization -X- _ O
: -X- _ O

Another -X- _ O
related -X- _ O
idea -X- _ O
is -X- _ O
to -X- _ O
perform -X- _ O
autoregressive -X- _ O
denoising -X- _ O
in -X- _ O
the -X- _ O
context -X- _ O
of -X- _ O
text -X- _ B-TaskName
generation -X- _ I-TaskName
[ -X- _ O
11 -X- _ O
] -X- _ O
, -X- _ O
which -X- _ O
only -X- _ O
considers -X- _ O
a -X- _ O
ﬁxed -X- _ O
order -X- _ O
though -X- _ O
. -X- _ O

Related -X- _ O
Work -X- _ O
The -X- _ O
idea -X- _ O
of -X- _ O
permutation -X- _ O
- -X- _ O
based -X- _ O
AR -X- _ O
modeling -X- _ O
has -X- _ O
been -X- _ O
explored -X- _ O
in -X- _ O
[ -X- _ O
32 -X- _ O
, -X- _ O
12 -X- _ O
] -X- _ O
, -X- _ O
but -X- _ O
there -X- _ O
are -X- _ O
several -X- _ O
key -X- _ O
differences -X- _ O
. -X- _ O
Firstly -X- _ O
, -X- _ O
previous -X- _ O
models -X- _ O
aim -X- _ O
to -X- _ O
improve -X- _ O
density -X- _ O
estimation -X- _ O
by -X- _ O
baking -X- _ O
an -X- _ O
“ -X- _ O
orderless -X- _ O
” -X- _ O
inductive -X- _ O
bias -X- _ O
into -X- _ O
the -X- _ O
model -X- _ O
while -X- _ O
XLNet -X- _ B-MethodName
is -X- _ O
motivated -X- _ O
by -X- _ O
enabling -X- _ O
AR -X- _ O
language -X- _ O
models -X- _ O
to -X- _ O
learn -X- _ O
bidirectional -X- _ O
contexts -X- _ O
. -X- _ O
Technically -X- _ O
, -X- _ O
to -X- _ O
construct -X- _ O
a -X- _ O
valid -X- _ O
target -X- _ O
- -X- _ O
aware -X- _ O
prediction -X- _ O
distribution -X- _ O
, -X- _ O
XLNet -X- _ B-MethodName
incorporates -X- _ O
the -X- _ O
target -X- _ O
position -X- _ O
into -X- _ O
the -X- _ O
hidden -X- _ O
state -X- _ O
via -X- _ O
two -X- _ O
- -X- _ O
stream -X- _ O
attention -X- _ O
while -X- _ O
previous -X- _ O
permutation -X- _ O
- -X- _ O
based -X- _ O
AR -X- _ O
models -X- _ O
relied -X- _ O
on -X- _ O
implicit -X- _ O
position -X- _ O
awareness -X- _ O
inherent -X- _ O
to -X- _ O
their -X- _ O
MLP -X- _ O
architectures -X- _ O
. -X- _ O
Finally -X- _ O
, -X- _ O
for -X- _ O
both -X- _ O
orderless -X- _ O
NADE -X- _ B-MethodName
and -X- _ O
XLNet -X- _ B-MethodName
, -X- _ O
we -X- _ O
would -X- _ O
like -X- _ O
to -X- _ O
emphasize -X- _ O
that -X- _ O
“ -X- _ O
orderless -X- _ O
” -X- _ O
does -X- _ O
not -X- _ O
mean -X- _ O
that -X- _ O
the -X- _ O
input -X- _ O
sequence -X- _ O
can -X- _ O
be -X- _ O
randomly -X- _ O
permuted -X- _ O
but -X- _ O
that -X- _ O
the -X- _ O
model -X- _ O
allows -X- _ O
for -X- _ O
different -X- _ O
factorization -X- _ O
orders -X- _ O
of -X- _ O
the -X- _ O
distribution -X- _ O
. -X- _ O

Empirically -X- _ O
, -X- _ O
under -X- _ O
comparable -X- _ O
experiment -X- _ O
setting -X- _ O
, -X- _ O
XLNet -X- _ B-MethodName
consistently -X- _ O
outperforms -X- _ O
BERT -X- _ B-MethodName
[ -X- _ O
10 -X- _ O
] -X- _ O
on -X- _ O
a -X- _ O
wide -X- _ O
spectrum -X- _ O
of -X- _ O
problems -X- _ O
including -X- _ O
GLUE -X- _ B-DatasetName
language -X- _ B-TaskName
understanding -X- _ I-TaskName
tasks -X- _ O
, -X- _ O
reading -X- _ B-TaskName
comprehension -X- _ I-TaskName
tasks -X- _ O
like -X- _ O
SQuAD -X- _ B-DatasetName
and -X- _ O
RACE -X- _ B-DatasetName
, -X- _ O
text -X- _ B-TaskName
classiﬁcation -X- _ I-TaskName
tasks -X- _ O
such -X- _ O
as -X- _ O
Yelp -X- _ B-DatasetName
and -X- _ O
IMDB -X- _ B-DatasetName
, -X- _ O
and -X- _ O
the -X- _ O
ClueWeb09 -X- _ B-DatasetName
- -X- _ I-DatasetName
B -X- _ I-DatasetName
document -X- _ B-TaskName
ranking -X- _ I-TaskName
task -X- _ O
. -X- _ O

• -X- _ O
Naively -X- _ O
applying -X- _ O
a -X- _ O
Transformer(-XL -X- _ B-MethodName
) -X- _ I-MethodName
architecture -X- _ O
to -X- _ O
permutation -X- _ O
- -X- _ O
based -X- _ O
language -X- _ O
modeling -X- _ O
does -X- _ O
not -X- _ O
work -X- _ O
because -X- _ O
the -X- _ O
factorization -X- _ O
order -X- _ O
is -X- _ O
arbitrary -X- _ O
and -X- _ O
the -X- _ O
target -X- _ O
is -X- _ O
ambiguous -X- _ O
. -X- _ O
As -X- _ O
a -X- _ O
solution -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
to -X- _ O
reparameterize -X- _ O
the -X- _ O
Transformer(-XL -X- _ B-MethodName
) -X- _ I-MethodName
network -X- _ O
to -X- _ O
remove -X- _ O
the -X- _ O
ambiguity -X- _ O
. -X- _ O

• -X- _ O
Inspired -X- _ O
by -X- _ O
the -X- _ O
latest -X- _ O
advancements -X- _ O
in -X- _ O
AR -X- _ O
language -X- _ O
modeling -X- _ O
, -X- _ O
XLNet -X- _ B-MethodName
integrates -X- _ O
the -X- _ O
segment -X- _ O
recurrence -X- _ O
mechanism -X- _ O
and -X- _ O
relative -X- _ O
encoding -X- _ O
scheme -X- _ O
of -X- _ O
Transformer -X- _ B-MethodName
- -X- _ I-MethodName
XL -X- _ I-MethodName
[ -X- _ O
9 -X- _ O
] -X- _ O
into -X- _ O
pretraining -X- _ O
, -X- _ O
which -X- _ O
empirically -X- _ O
improves -X- _ O
the -X- _ O
performance -X- _ O
especially -X- _ O
for -X- _ O
tasks -X- _ O
involving -X- _ O
a -X- _ O
longer -X- _ O
text -X- _ O
sequence -X- _ O
. -X- _ O

In -X- _ O
addition -X- _ O
to -X- _ O
a -X- _ O
novel -X- _ O
pretraining -X- _ O
objective -X- _ O
, -X- _ O
XLNet -X- _ B-MethodName
improves -X- _ O
architectural -X- _ O
designs -X- _ O
for -X- _ O
pretraining -X- _ O
. -X- _ O

• -X- _ O
Secondly -X- _ O
, -X- _ O
as -X- _ O
a -X- _ O
generalized -X- _ O
AR -X- _ O
language -X- _ O
model -X- _ O
, -X- _ O
XLNet -X- _ B-MethodName
does -X- _ O
not -X- _ O
rely -X- _ O
on -X- _ O
data -X- _ O
corruption -X- _ O
. -X- _ O
Hence -X- _ O
, -X- _ O
XLNet -X- _ B-MethodName
does -X- _ O
not -X- _ O
suffer -X- _ O
from -X- _ O
the -X- _ O
pretrain-ﬁnetune -X- _ O
discrepancy -X- _ O
that -X- _ O
BERT -X- _ B-MethodName
is -X- _ O
subject -X- _ O
to -X- _ O
. -X- _ O
Meanwhile -X- _ O
, -X- _ O
the -X- _ O
autoregressive -X- _ O
objective -X- _ O
also -X- _ O
provides -X- _ O
a -X- _ O
natural -X- _ O
way -X- _ O
to -X- _ O
use -X- _ O
the -X- _ O
product -X- _ O
rule -X- _ O
for -X- _ O
factorizing -X- _ O
the -X- _ O
joint -X- _ O
probability -X- _ O
of -X- _ O
the -X- _ O
predicted -X- _ O
tokens -X- _ O
, -X- _ O
eliminating -X- _ O
the -X- _ O
independence -X- _ O
assumption -X- _ O
made -X- _ O
in -X- _ O
BERT -X- _ B-MethodName
. -X- _ O

• -X- _ O
Firstly -X- _ O
, -X- _ O
instead -X- _ O
of -X- _ O
using -X- _ O
a -X- _ O
ﬁxed -X- _ O
forward -X- _ O
or -X- _ O
backward -X- _ O
factorization -X- _ O
order -X- _ O
as -X- _ O
in -X- _ O
conventional -X- _ O
AR -X- _ O
mod- -X- _ O
els -X- _ O
, -X- _ O
XLNet -X- _ B-MethodName
maximizes -X- _ O
the -X- _ O
expected -X- _ B-MetricName
log -X- _ I-MetricName
likelihood -X- _ I-MetricName
of -X- _ O
a -X- _ O
sequence -X- _ O
w.r.t -X- _ O
. -X- _ O
all -X- _ O
possible -X- _ O
permutations -X- _ O
of -X- _ O
the -X- _ O
factorization -X- _ O
order -X- _ O
. -X- _ O
Thanks -X- _ O
to -X- _ O
the -X- _ O
permutation -X- _ O
operation -X- _ O
, -X- _ O
the -X- _ O
context -X- _ O
for -X- _ O
each -X- _ O
position -X- _ O
can -X- _ O
consist -X- _ O
of -X- _ O
tokens -X- _ O
from -X- _ O
both -X- _ O
left -X- _ O
and -X- _ O
right -X- _ O
. -X- _ O
In -X- _ O
expectation -X- _ O
, -X- _ O
each -X- _ O
position -X- _ O
learns -X- _ O
to -X- _ O
utilize -X- _ O
contextual -X- _ O
information -X- _ O
from -X- _ O
all -X- _ O
positions -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
capturing -X- _ O
bidirectional -X- _ O
context -X- _ O
. -X- _ O

Faced -X- _ O
with -X- _ O
the -X- _ O
pros -X- _ O
and -X- _ O
cons -X- _ O
of -X- _ O
existing -X- _ O
language -X- _ O
pretraining -X- _ O
objectives -X- _ O
, -X- _ O
in -X- _ O
this -X- _ O
work -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
XLNet -X- _ B-MethodName
, -X- _ O
a -X- _ O
generalized -X- _ O
autoregressive -X- _ O
method -X- _ O
that -X- _ O
leverages -X- _ O
the -X- _ O
best -X- _ O
of -X- _ O
both -X- _ O
AR -X- _ O
language -X- _ O
modeling -X- _ O
and -X- _ O
AE -X- _ O
while -X- _ O
avoiding -X- _ O
their -X- _ O
limitations -X- _ O
. -X- _ O

bidirectional -X- _ O
contexts -X- _ O
for -X- _ O
reconstruction -X- _ O
. -X- _ O
As -X- _ O
an -X- _ O
immediate -X- _ O
beneﬁt -X- _ O
, -X- _ O
this -X- _ O
closes -X- _ O
the -X- _ O
aforementioned -X- _ O
bidirectional -X- _ O
information -X- _ O
gap -X- _ O
in -X- _ O
AR -X- _ O
language -X- _ O
modeling -X- _ O
, -X- _ O
leading -X- _ O
to -X- _ O
improved -X- _ O
performance -X- _ O
. -X- _ O
However -X- _ O
, -X- _ O
the -X- _ O
artiﬁcial -X- _ O
symbols -X- _ O
like -X- _ O
[ -X- _ O
MASK -X- _ O
] -X- _ O
used -X- _ O
by -X- _ O
BERT -X- _ B-MethodName
during -X- _ O
pretraining -X- _ O
are -X- _ O
absent -X- _ O
from -X- _ O
real -X- _ O
data -X- _ O
at -X- _ O
ﬁnetuning -X- _ O
time -X- _ O
, -X- _ O
resulting -X- _ O
in -X- _ O
a -X- _ O
pretrain-ﬁnetune -X- _ O
discrepancy -X- _ O
. -X- _ O
Moreover -X- _ O
, -X- _ O
since -X- _ O
the -X- _ O
predicted -X- _ O
tokens -X- _ O
are -X- _ O
masked -X- _ O
in -X- _ O
the -X- _ O
input -X- _ O
, -X- _ O
BERT -X- _ B-MethodName
is -X- _ O
not -X- _ O
able -X- _ O
to -X- _ O
model -X- _ O
the -X- _ O
joint -X- _ O
probability -X- _ O
using -X- _ O
the -X- _ O
product -X- _ O
rule -X- _ O
as -X- _ O
in -X- _ O
AR -X- _ O
language -X- _ O
modeling -X- _ O
. -X- _ O
In -X- _ O
other -X- _ O
words -X- _ O
, -X- _ O
BERT -X- _ B-MethodName
assumes -X- _ O
the -X- _ O
predicted -X- _ O
tokens -X- _ O
are -X- _ O
independent -X- _ O
of -X- _ O
each -X- _ O
other -X- _ O
given -X- _ O
the -X- _ O
unmasked -X- _ O
tokens -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
oversimpliﬁed -X- _ O
as -X- _ O
high -X- _ O
- -X- _ O
order -X- _ O
, -X- _ O
long -X- _ O
- -X- _ O
range -X- _ O
dependency -X- _ O
is -X- _ O
prevalent -X- _ O
in -X- _ O
natural -X- _ O
language -X- _ O
[ -X- _ O
9 -X- _ O
] -X- _ O
. -X- _ O

In -X- _ O
comparison -X- _ O
, -X- _ O
AE -X- _ O
based -X- _ O
pretraining -X- _ O
does -X- _ O
not -X- _ O
perform -X- _ O
explicit -X- _ O
density -X- _ O
estimation -X- _ O
but -X- _ O
instead -X- _ O
aims -X- _ O
to -X- _ O
reconstruct -X- _ O
the -X- _ O
original -X- _ O
data -X- _ O
from -X- _ O
corrupted -X- _ O
input -X- _ O
. -X- _ O
A -X- _ O
notable -X- _ O
example -X- _ O
is -X- _ O
BERT -X- _ B-MethodName
[ -X- _ O
10 -X- _ O
] -X- _ O
, -X- _ O
which -X- _ O
has -X- _ O
been -X- _ O
the -X- _ O
state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
art -X- _ O
pretraining -X- _ O
approach -X- _ O
. -X- _ O
Given -X- _ O
the -X- _ O
input -X- _ O
token -X- _ O
sequence -X- _ O
, -X- _ O
a -X- _ O
certain -X- _ O
portion -X- _ O
of -X- _ O
tokens -X- _ O
are -X- _ O
replaced -X- _ O
by -X- _ O
a -X- _ O
special -X- _ O
symbol -X- _ O
[ -X- _ O
MASK -X- _ O
] -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
model -X- _ O
is -X- _ O
trained -X- _ O
to -X- _ O
recover -X- _ O
the -X- _ O
original -X- _ O
tokens -X- _ O
from -X- _ O
the -X- _ O
corrupted -X- _ O
version -X- _ O
. -X- _ O
Since -X- _ O
density -X- _ O
estimation -X- _ O
is -X- _ O
not -X- _ O
part -X- _ O
of -X- _ O
the -X- _ O
objective -X- _ O
, -X- _ O
BERT -X- _ B-MethodName
is -X- _ O
allowed -X- _ O
to -X- _ O
utilize -X- _ O

AR -X- _ O
language -X- _ O
modeling -X- _ O
seeks -X- _ O
to -X- _ O
estimate -X- _ O
the -X- _ O
probability -X- _ O
distribution -X- _ O
of -X- _ O
a -X- _ O
text -X- _ O
corpus -X- _ O
with -X- _ O
an -X- _ O
au- -X- _ O
toregressive -X- _ O
model -X- _ O
[ -X- _ O
7 -X- _ O
, -X- _ O
27 -X- _ O
, -X- _ O
28 -X- _ O
] -X- _ O
. -X- _ O
Speciﬁcally -X- _ O
, -X- _ O
given -X- _ O
a -X- _ O
text -X- _ O
sequence -X- _ O
x -X- _ O
= -X- _ O
( -X- _ O
x1 -X- _ O
, -X- _ O
· -X- _ O
· -X- _ O
· -X- _ O
, -X- _ O
xT -X- _ O
) -X- _ O
, -X- _ O
AR -X- _ O
language -X- _ O
modeling -X- _ O
factorizes -X- _ O
the -X- _ O
likelihood -X- _ O
into -X- _ O
a -X- _ O
forward -X- _ O
product -X- _ O
p(x -X- _ O
) -X- _ O
= -X- _ O
� -X- _ O
T -X- _ O
t=1 -X- _ O
p(xt -X- _ O
| -X- _ O
x -X- _ O
< -X- _ O
t -X- _ O
) -X- _ O
or -X- _ O
a -X- _ O
backward -X- _ O
one -X- _ O
p(x -X- _ O
) -X- _ O
= -X- _ O
� -X- _ O
1 -X- _ O
t -X- _ O
= -X- _ O
T -X- _ O
p(xt -X- _ O
| -X- _ O
x -X- _ O
> -X- _ O
t -X- _ O
) -X- _ O
. -X- _ O
A -X- _ O
parametric -X- _ O
model -X- _ O
( -X- _ O
e.g. -X- _ O
a -X- _ O
neural -X- _ O
network -X- _ O
) -X- _ O
is -X- _ O
trained -X- _ O
to -X- _ O
model -X- _ O
each -X- _ O
conditional -X- _ O
distribution -X- _ O
. -X- _ O
Since -X- _ O
an -X- _ O
AR -X- _ O
language -X- _ O
model -X- _ O
is -X- _ O
only -X- _ O
trained -X- _ O
to -X- _ O
encode -X- _ O
a -X- _ O
uni -X- _ O
- -X- _ O
directional -X- _ O
con- -X- _ O
text -X- _ O
( -X- _ O
either -X- _ O
forward -X- _ O
or -X- _ O
backward -X- _ O
) -X- _ O
, -X- _ O
it -X- _ O
is -X- _ O
not -X- _ O
effective -X- _ O
at -X- _ O
modeling -X- _ O
deep -X- _ O
bidirectional -X- _ O
contexts -X- _ O
. -X- _ O
On -X- _ O
the -X- _ O
contrary -X- _ O
, -X- _ O
downstream -X- _ O
language -X- _ O
understanding -X- _ O
tasks -X- _ O
often -X- _ O
require -X- _ O
bidirectional -X- _ O
context -X- _ O
information -X- _ O
. -X- _ O
This -X- _ O
results -X- _ O
in -X- _ O
a -X- _ O
gap -X- _ O
between -X- _ O
AR -X- _ O
language -X- _ O
modeling -X- _ O
and -X- _ O
effective -X- _ O
pretraining -X- _ O
. -X- _ O

Unsupervised -X- _ O
representation -X- _ O
learning -X- _ O
has -X- _ O
been -X- _ O
highly -X- _ O
successful -X- _ O
in -X- _ O
the -X- _ O
domain -X- _ O
of -X- _ O
natural -X- _ O
language -X- _ O
processing -X- _ O
[ -X- _ O
7 -X- _ O
, -X- _ O
22 -X- _ O
, -X- _ O
27 -X- _ O
, -X- _ O
28 -X- _ O
, -X- _ O
10 -X- _ O
] -X- _ O
. -X- _ O
Typically -X- _ O
, -X- _ O
these -X- _ O
methods -X- _ O
ﬁrst -X- _ O
pretrain -X- _ O
neural -X- _ O
networks -X- _ O
on -X- _ O
large -X- _ O
- -X- _ O
scale -X- _ O
unlabeled -X- _ O
text -X- _ O
corpora -X- _ O
, -X- _ O
and -X- _ O
then -X- _ O
ﬁnetune -X- _ O
the -X- _ O
models -X- _ O
or -X- _ O
representations -X- _ O
on -X- _ O
downstream -X- _ O
tasks -X- _ O
. -X- _ O
Under -X- _ O
this -X- _ O
shared -X- _ O
high -X- _ O
- -X- _ O
level -X- _ O
idea -X- _ O
, -X- _ O
different -X- _ O
unsupervised -X- _ O
pretraining -X- _ O
objectives -X- _ O
have -X- _ O
been -X- _ O
explored -X- _ O
in -X- _ O
literature -X- _ O
. -X- _ O
Among -X- _ O
them -X- _ O
, -X- _ O
autoregressive -X- _ O
( -X- _ O
AR -X- _ O
) -X- _ O
language -X- _ O
modeling -X- _ O
and -X- _ O
autoencoding -X- _ O
( -X- _ O
AE -X- _ O
) -X- _ O
have -X- _ O
been -X- _ O
the -X- _ O
two -X- _ O
most -X- _ O
successful -X- _ O
pretraining -X- _ O
objectives -X- _ O
. -X- _ O

With -X- _ O
the -X- _ O
capability -X- _ O
of -X- _ O
modeling -X- _ O
bidirectional -X- _ O
contexts -X- _ O
, -X- _ O
denoising -X- _ O
autoencoding -X- _ O
based -X- _ O
pretraining -X- _ O
like -X- _ O
BERT -X- _ B-MethodName
achieves -X- _ O
better -X- _ O
performance -X- _ O
than -X- _ O
pretraining -X- _ O
ap- -X- _ O
proaches -X- _ O
based -X- _ O
on -X- _ O
autoregressive -X- _ O
language -X- _ O
modeling -X- _ O
. -X- _ O
However -X- _ O
, -X- _ O
relying -X- _ O
on -X- _ O
corrupt- -X- _ O
ing -X- _ O
the -X- _ O
input -X- _ O
with -X- _ O
masks -X- _ O
, -X- _ O
BERT -X- _ B-MethodName
neglects -X- _ O
dependency -X- _ O
between -X- _ O
the -X- _ O
masked -X- _ O
positions -X- _ O
and -X- _ O
suffers -X- _ O
from -X- _ O
a -X- _ O
pretrain-ﬁnetune -X- _ O
discrepancy -X- _ O
. -X- _ O
In -X- _ O
light -X- _ O
of -X- _ O
these -X- _ O
pros -X- _ O
and -X- _ O
cons -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
XLNet -X- _ B-MethodName
, -X- _ O
a -X- _ O
generalized -X- _ O
autoregressive -X- _ O
pretraining -X- _ O
method -X- _ O
that -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
enables -X- _ O
learning -X- _ O
bidirectional -X- _ O
contexts -X- _ O
by -X- _ O
maximizing -X- _ O
the -X- _ O
expected -X- _ O
likelihood -X- _ O
over -X- _ O
all -X- _ O
permutations -X- _ O
of -X- _ O
the -X- _ O
factorization -X- _ O
order -X- _ O
and -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
overcomes -X- _ O
the -X- _ O
limitations -X- _ O
of -X- _ O
BERT -X- _ B-MethodName
thanks -X- _ O
to -X- _ O
its -X- _ O
autoregressive -X- _ O
formulation -X- _ O
. -X- _ O
Furthermore -X- _ O
, -X- _ O
XLNet -X- _ B-MethodName
integrates -X- _ O
ideas -X- _ O
from -X- _ O
Transformer -X- _ B-MethodName
- -X- _ I-MethodName
XL -X- _ I-MethodName
, -X- _ O
the -X- _ O
state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
art -X- _ O
autoregressive -X- _ O
model -X- _ O
, -X- _ O
into -X- _ O
pretraining -X- _ O
. -X- _ O
Empirically -X- _ O
, -X- _ O
under -X- _ O
comparable -X- _ O
experiment -X- _ O
settings -X- _ O
, -X- _ O
XLNet -X- _ B-MethodName
outperforms -X- _ O
BERT -X- _ B-MethodName
on -X- _ O
20 -X- _ O
tasks -X- _ O
, -X- _ O
often -X- _ O
by -X- _ O
a -X- _ O
large -X- _ O
margin -X- _ O
, -X- _ O
including -X- _ O
question -X- _ B-TaskName
answering -X- _ I-TaskName
, -X- _ O
natural -X- _ B-TaskName
language -X- _ I-TaskName
inference -X- _ I-TaskName
, -X- _ O
sentiment -X- _ B-TaskName
analysis -X- _ I-TaskName
, -X- _ O
and -X- _ O
document -X- _ B-TaskName
ranking.1 -X- _ I-TaskName
. -X- _ O

XLNet -X- _ B-MethodName
: -X- _ O
Generalized -X- _ O
Autoregressive -X- _ O
Pretraining -X- _ O
for -X- _ O
Language -X- _ B-TaskName
Understanding -X- _ I-TaskName

