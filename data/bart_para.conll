-DOCSTART- -X- O
BART -X- _ B-MethodName
uses -X- _ O
the -X- _ O
standard -X- _ O
sequence -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
sequence -X- _ O
Trans- -X- _ O
former -X- _ O
architecture -X- _ O
from -X- _ O
( -X- _ O
Vaswani -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
, -X- _ O
ex- -X- _ O
cept -X- _ O
, -X- _ O
following -X- _ O
GPT -X- _ B-MethodName
, -X- _ O
that -X- _ O
we -X- _ O
modify -X- _ O
ReLU -X- _ O
activa- -X- _ O
tion -X- _ O
functions -X- _ O
to -X- _ O
GeLUs -X- _ O
( -X- _ O
Hendrycks -X- _ O
& -X- _ O
Gimpel -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
and -X- _ O
initialise -X- _ O
parameters -X- _ O
from -X- _ O
N(0 -X- _ B-HyperparameterName
, -X- _ O
0.02 -X- _ O
) -X- _ O
. -X- _ O
For -X- _ O
our -X- _ O
base -X- _ O
model -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
6 -X- _ B-HyperparameterName
layers -X- _ B-HyperparameterName
in -X- _ O
the -X- _ O
encoder -X- _ O
and -X- _ O
de- -X- _ O

We -X- _ O
introduced -X- _ O
BART -X- _ B-MethodName
, -X- _ O
a -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
approach -X- _ O
that -X- _ O
learns -X- _ O
to -X- _ O
map -X- _ O
corrupted -X- _ O
documents -X- _ O
to -X- _ O
the -X- _ O
original -X- _ O
. -X- _ O
BART -X- _ B-MethodName
achieves -X- _ O
similar -X- _ O
performance -X- _ O
to -X- _ O
RoBERTa -X- _ B-MethodName
on -X- _ O
discriminative -X- _ B-TaskName
tasks -X- _ O
, -X- _ O
while -X- _ O
achieving -X- _ O
new -X- _ O
state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
the- -X- _ O
art -X- _ O
results -X- _ O
on -X- _ O
a -X- _ O
number -X- _ O
of -X- _ O
text -X- _ O
generation -X- _ B-TaskName
tasks -X- _ O
. -X- _ O
Fu- -X- _ O
ture -X- _ O
work -X- _ O
should -X- _ O
explore -X- _ O
new -X- _ O
methods -X- _ O
for -X- _ O
corrupting -X- _ O
documents -X- _ O
for -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
, -X- _ O
perhaps -X- _ O
tailoring -X- _ O
them -X- _ O
to -X- _ O
speciﬁc -X- _ O
end -X- _ O
tasks -X- _ O
. -X- _ O

Several -X- _ O
papers -X- _ O
have -X- _ O
explored -X- _ O
using -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
rep- -X- _ O
resentations -X- _ O
to -X- _ O
improve -X- _ O
machine -X- _ B-TaskName
translation -X- _ I-TaskName
. -X- _ O
The -X- _ O
largest -X- _ O
improvements -X- _ O
have -X- _ O
come -X- _ O
from -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
on -X- _ O
both -X- _ O
source -X- _ O
and -X- _ O
target -X- _ O
languages -X- _ O
( -X- _ O
Song -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Lample -X- _ O
& -X- _ O
Conneau -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
but -X- _ O
this -X- _ O
requires -X- _ O
pre- -X- _ O
training -X- _ O
on -X- _ O
all -X- _ O
languages -X- _ O
of -X- _ O
interest -X- _ O
. -X- _ O
Other -X- _ O
work -X- _ O
has -X- _ O
shown -X- _ O
that -X- _ O
encoders -X- _ O
can -X- _ O
be -X- _ O
improved -X- _ O
using -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
representations -X- _ O
( -X- _ O
Edunov -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
but -X- _ O
gains -X- _ O
in -X- _ O
de- -X- _ O
coders -X- _ O
are -X- _ O
more -X- _ O
limited -X- _ O
. -X- _ O
We -X- _ O
show -X- _ O
how -X- _ O
BART -X- _ B-MethodName
can -X- _ O
be -X- _ O
used -X- _ O
to -X- _ O
improve -X- _ O
machine -X- _ O
translation -X- _ O
decoders -X- _ O
. -X- _ O

dicting -X- _ O
masked -X- _ O
tokens -X- _ O
auto -X- _ O
- -X- _ O
regressively -X- _ O
in -X- _ O
a -X- _ O
permuted -X- _ O
order -X- _ O
. -X- _ O
This -X- _ O
objective -X- _ O
allows -X- _ O
predictions -X- _ O
to -X- _ O
condition -X- _ O
on -X- _ O
both -X- _ O
left -X- _ O
and -X- _ O
right -X- _ O
context -X- _ O
. -X- _ O
In -X- _ O
contrast -X- _ O
, -X- _ O
the -X- _ O
BART -X- _ B-MethodName
de- -X- _ O
coder -X- _ O
works -X- _ O
left -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
right -X- _ O
during -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
, -X- _ O
matching -X- _ O
the -X- _ O
setting -X- _ O
during -X- _ O
generation -X- _ B-TaskName
. -X- _ O

Early -X- _ O
methods -X- _ O
for -X- _ O
pretraining -X- _ O
were -X- _ O
based -X- _ O
on -X- _ O
language -X- _ O
models -X- _ O
. -X- _ O
GPT -X- _ B-MethodName
( -X- _ O
Radford -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
only -X- _ O
models -X- _ O
left- -X- _ O
ward -X- _ O
context -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
problematic -X- _ O
for -X- _ O
some -X- _ O
tasks -X- _ O
. -X- _ O
ELMo -X- _ B-MethodName
( -X- _ O
Peters -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
concatenates -X- _ O
left -X- _ O
- -X- _ O
only -X- _ O
and -X- _ O
right -X- _ O
- -X- _ O
only -X- _ O
representations -X- _ O
, -X- _ O
but -X- _ O
does -X- _ O
not -X- _ O
pre -X- _ O
- -X- _ O
train -X- _ O
inter- -X- _ O
actions -X- _ O
between -X- _ O
these -X- _ O
features -X- _ O
. -X- _ O
Radford -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2019 -X- _ O
) -X- _ O
demonstrated -X- _ O
that -X- _ O
very -X- _ O
large -X- _ O
language -X- _ O
models -X- _ O
can -X- _ O
act -X- _ O
as -X- _ O
unsupervised -X- _ O
multitask -X- _ O
models -X- _ O
. -X- _ O
BERT -X- _ B-MethodName
( -X- _ O
Devlin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
introduced -X- _ O
masked -X- _ O
lan- -X- _ O
guage -X- _ O
modelling -X- _ O
, -X- _ O
which -X- _ O
allows -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
to -X- _ O
learn -X- _ O
in- -X- _ O
teractions -X- _ O
between -X- _ O
left -X- _ O
and -X- _ O
right -X- _ O
context -X- _ O
words -X- _ O
. -X- _ O
Re- -X- _ O
cent -X- _ O
work -X- _ O
has -X- _ O
shown -X- _ O
that -X- _ O
very -X- _ O
strong -X- _ O
performance -X- _ O
can -X- _ O
be -X- _ O
achieved -X- _ O
by -X- _ O
training -X- _ O
for -X- _ O
longer -X- _ O
( -X- _ O
Liu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
by -X- _ O
tying -X- _ O
parameters -X- _ O
across -X- _ O
layers -X- _ O
( -X- _ O
Lan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
by -X- _ O
masking -X- _ O
spans -X- _ O
instead -X- _ O
of -X- _ O
words -X- _ O
( -X- _ O
Joshi -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
. -X- _ O
Predictions -X- _ O
are -X- _ O
not -X- _ O
made -X- _ O
auto -X- _ O
- -X- _ O
regressively -X- _ O
, -X- _ O
re- -X- _ O
ducing -X- _ O
the -X- _ O
effectiveness -X- _ O
of -X- _ O
BERT -X- _ B-MethodName
for -X- _ O
generation -X- _ B-TaskName
tasks -X- _ O
. -X- _ O
UniLM -X- _ B-MethodName
( -X- _ O
Dong -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
ﬁne -X- _ O
- -X- _ O
tunes -X- _ O
BERT -X- _ B-MethodName
with -X- _ O
an -X- _ O
ensemble -X- _ O
of -X- _ O
masks -X- _ O
, -X- _ O
some -X- _ O
of -X- _ O
which -X- _ O
allow -X- _ O
only -X- _ O
leftward -X- _ O
context -X- _ O
. -X- _ O
Like -X- _ O
BART -X- _ B-MethodName
, -X- _ O
this -X- _ O
allows -X- _ O
UniLM -X- _ B-MethodName
to -X- _ O
be -X- _ O
used -X- _ O
for -X- _ O
both -X- _ O
generative -X- _ B-TaskName
and -X- _ O
discriminative -X- _ B-TaskName
tasks -X- _ O
. -X- _ O
A -X- _ O
difference -X- _ O
is -X- _ O
that -X- _ O
UniLM -X- _ B-MethodName
predictions -X- _ O
are -X- _ O
conditionally -X- _ O
indepen- -X- _ O
dent -X- _ O
, -X- _ O
whereas -X- _ O
BART -X- _ B-MethodName
’s -X- _ O
are -X- _ O
autoregressive -X- _ O
. -X- _ O
BART -X- _ B-MethodName
re- -X- _ O
duces -X- _ O
the -X- _ O
mismatch -X- _ O
between -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
and -X- _ O
genera- -X- _ O
tion -X- _ O
tasks -X- _ O
, -X- _ O
because -X- _ O
the -X- _ O
decoder -X- _ O
is -X- _ O
always -X- _ O
trained -X- _ O
on -X- _ O
un- -X- _ O
corrupted -X- _ O
context -X- _ O
. -X- _ O
MASS -X- _ B-MethodName
( -X- _ O
Song -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
is -X- _ O
perhaps -X- _ O
the -X- _ O
most -X- _ O
similar -X- _ O
model -X- _ O
to -X- _ O
BART -X- _ B-MethodName
. -X- _ O
An -X- _ O
input -X- _ O
sequence -X- _ O
where -X- _ O
a -X- _ O
contiguous -X- _ O
span -X- _ O
of -X- _ O
tokens -X- _ O
is -X- _ O
masked -X- _ O
is -X- _ O
mapped -X- _ O
to -X- _ O
a -X- _ O
sequence -X- _ O
con- -X- _ O
sisting -X- _ O
of -X- _ O
the -X- _ O
missing -X- _ O
tokens -X- _ O
. -X- _ O
MASS -X- _ B-MethodName
is -X- _ O
less -X- _ O
effective -X- _ O
for -X- _ O
discriminative -X- _ B-TaskName
tasks -X- _ O
, -X- _ O
because -X- _ O
disjoint -X- _ O
sets -X- _ O
of -X- _ O
tokens -X- _ O
are -X- _ O
fed -X- _ O
into -X- _ O
the -X- _ O
encoder -X- _ O
and -X- _ O
decoder -X- _ O
. -X- _ O
XL -X- _ B-MethodName
- -X- _ I-MethodName
Net -X- _ I-MethodName
( -X- _ O
Yang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
extends -X- _ O
BERT -X- _ B-MethodName
by -X- _ O
pre- -X- _ O

Narayan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2018 -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
remove -X- _ O
the -X- _ O
ﬁrst -X- _ O
sentence -X- _ O
of -X- _ O
the -X- _ O
article -X- _ O
prior -X- _ O
to -X- _ O
summarizing -X- _ O
it -X- _ O
, -X- _ O
so -X- _ O
there -X- _ O
is -X- _ O
no -X- _ O
easy -X- _ O
extractive -X- _ O
summary -X- _ O
of -X- _ O
the -X- _ O
document -X- _ O
. -X- _ O
Unsurprisingly -X- _ O
, -X- _ O
model -X- _ O
output -X- _ O
is -X- _ O
ﬂuent -X- _ O
and -X- _ O
grammat- -X- _ O
ical -X- _ O
English -X- _ O
. -X- _ O
However -X- _ O
, -X- _ O
model -X- _ O
output -X- _ O
is -X- _ O
also -X- _ O
highly -X- _ O
ab- -X- _ O
stractive -X- _ O
, -X- _ O
with -X- _ O
few -X- _ O
phrases -X- _ O
copied -X- _ O
from -X- _ O
the -X- _ O
input -X- _ O
. -X- _ O
The -X- _ O
output -X- _ O
is -X- _ O
also -X- _ O
generally -X- _ O
factually -X- _ O
accurate -X- _ O
, -X- _ O
and -X- _ O
inte- -X- _ O
grates -X- _ O
supporting -X- _ O
evidence -X- _ O
from -X- _ O
across -X- _ O
the -X- _ O
input -X- _ O
doc- -X- _ O
ument -X- _ O
with -X- _ O
background -X- _ O
knowledge -X- _ O
( -X- _ O
for -X- _ O
example -X- _ O
, -X- _ O
cor- -X- _ O
rectly -X- _ O
completing -X- _ O
names -X- _ O
, -X- _ O
or -X- _ O
inferring -X- _ O
that -X- _ O
PG&E -X- _ O
oper- -X- _ O
ates -X- _ O
in -X- _ O
California -X- _ O
) -X- _ O
. -X- _ O
In -X- _ O
the -X- _ O
ﬁrst -X- _ O
example -X- _ O
, -X- _ O
inferring -X- _ O
that -X- _ O
ﬁsh -X- _ O
are -X- _ O
protecting -X- _ O
reefs -X- _ O
from -X- _ O
global -X- _ O
warming -X- _ O
requires -X- _ O
non -X- _ O
- -X- _ O
trivial -X- _ O
inference -X- _ O
from -X- _ O
the -X- _ O
text -X- _ O
. -X- _ O
However -X- _ O
, -X- _ O
the -X- _ O
claim -X- _ O
that -X- _ O
the -X- _ O
work -X- _ O
was -X- _ O
published -X- _ O
in -X- _ O
Science -X- _ O
is -X- _ O
not -X- _ O
supported -X- _ O
by -X- _ O
the -X- _ O
source -X- _ O
. -X- _ O
These -X- _ O
samples -X- _ O
demonstrate -X- _ O
that -X- _ O
the -X- _ O
BART -X- _ O
pretrain- -X- _ O
ing -X- _ O
has -X- _ O
learned -X- _ O
a -X- _ O
strong -X- _ O
combination -X- _ O
of -X- _ O
natural -X- _ B-TaskName
lan- -X- _ I-TaskName
guage -X- _ I-TaskName
understanding -X- _ I-TaskName
and -X- _ O
generation -X- _ B-TaskName
. -X- _ O

Table -X- _ O
7 -X- _ O
shows -X- _ O
example -X- _ O
summaries -X- _ O
generated -X- _ O
by -X- _ O
BART -X- _ B-MethodName
. -X- _ O
Examples -X- _ O
are -X- _ O
taken -X- _ O
from -X- _ O
WikiNews -X- _ B-DatasetName
articles -X- _ O
published -X- _ O
after -X- _ O
the -X- _ O
creation -X- _ O
of -X- _ O
the -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
corpus -X- _ O
, -X- _ O
to -X- _ O
eliminate -X- _ O
the -X- _ O
possibility -X- _ O
of -X- _ O
the -X- _ O
events -X- _ O
described -X- _ O
be- -X- _ O
ing -X- _ O
present -X- _ O
in -X- _ O
the -X- _ O
model -X- _ O
’s -X- _ O
training -X- _ O
data -X- _ O
. -X- _ O
Following -X- _ O

BART -X- _ B-MethodName
shows -X- _ O
large -X- _ O
improvements -X- _ O
on -X- _ O
summarization -X- _ B-MetricName
metrics -X- _ I-MetricName
, -X- _ O
of -X- _ O
up -X- _ O
to -X- _ O
6 -X- _ B-MetricValue
points -X- _ O
over -X- _ O
the -X- _ O
prior -X- _ O
state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
art -X- _ O
. -X- _ O
To -X- _ O
understand -X- _ O
BART -X- _ B-MethodName
’s -X- _ O
performance -X- _ O
beyond -X- _ O
automated -X- _ O
metrics -X- _ O
, -X- _ O
we -X- _ O
analyse -X- _ O
its -X- _ O
generations -X- _ O
qualitatively -X- _ O
. -X- _ O

We -X- _ O
also -X- _ O
evaluated -X- _ O
performance -X- _ O
on -X- _ O
WMT16 -X- _ B-DatasetName
Romanian- -X- _ I-DatasetName
English -X- _ I-DatasetName
, -X- _ O
augmented -X- _ O
with -X- _ O
back -X- _ O
- -X- _ O
translation -X- _ O
data -X- _ O
from -X- _ O
Sennrich -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2016 -X- _ O
) -X- _ O
. -X- _ O
We -X- _ O
use -X- _ O
a -X- _ O
6 -X- _ B-HyperparameterValue
- -X- _ O
layer -X- _ B-HyperparameterName
transformer -X- _ O
source -X- _ O
encoder -X- _ O
to -X- _ O
map -X- _ O
Romanian -X- _ O
into -X- _ O
a -X- _ O
representation -X- _ O
that -X- _ O
BART -X- _ B-MethodName
is -X- _ O
able -X- _ O
to -X- _ O
de -X- _ O
- -X- _ O
noise -X- _ O
into -X- _ O
English -X- _ O
, -X- _ O
following -X- _ O
the -X- _ O
approach -X- _ O
introduced -X- _ O
in -X- _ O
§ -X- _ O
3.4 -X- _ O
. -X- _ O
Experiment -X- _ O
results -X- _ O
are -X- _ O
presented -X- _ O
in -X- _ O
Table -X- _ O
6 -X- _ O
. -X- _ O
We -X- _ O
compare -X- _ O
our -X- _ O
results -X- _ O
against -X- _ O
a -X- _ O
baseline -X- _ O
Transformer -X- _ O
architecture -X- _ O
( -X- _ O
Vaswani -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
with -X- _ O
Transformer- -X- _ O
large -X- _ O
settings -X- _ O
( -X- _ O
the -X- _ O
baseline -X- _ O
row -X- _ O
) -X- _ O
. -X- _ O
We -X- _ O
show -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
both -X- _ O
steps -X- _ O
of -X- _ O
our -X- _ O
model -X- _ O
in -X- _ O
the -X- _ O
ﬁxed -X- _ O
BART -X- _ B-MethodName
and -X- _ O
tuned -X- _ O
BART -X- _ B-MethodName
rows -X- _ O
. -X- _ O
For -X- _ O
each -X- _ O
row -X- _ O
we -X- _ O
experiment -X- _ O
on -X- _ O
the -X- _ O
original -X- _ O
WMT16 -X- _ B-DatasetName
Romanian -X- _ I-DatasetName
- -X- _ I-DatasetName
English -X- _ I-DatasetName
augmented -X- _ O
with -X- _ O
back -X- _ O
- -X- _ O
translation -X- _ O
data -X- _ O
. -X- _ O
We -X- _ O
use -X- _ O
a -X- _ O
beam -X- _ B-HyperparameterName
width -X- _ I-HyperparameterName
of -X- _ O
5 -X- _ B-HyperparameterValue
and -X- _ O
a -X- _ O
length -X- _ B-HyperparameterName
penalty -X- _ I-HyperparameterName
of -X- _ O
α -X- _ B-HyperparameterName
= -X- _ O
1 -X- _ B-HyperparameterValue
. -X- _ O
Preliminary -X- _ O
results -X- _ O
suggested -X- _ O
that -X- _ O
our -X- _ O
approach -X- _ O
was -X- _ O
less -X- _ O
effective -X- _ O
without -X- _ O
back -X- _ O
- -X- _ O
translation -X- _ O
data -X- _ O
, -X- _ O
and -X- _ O
prone -X- _ O
to -X- _ O
overﬁtting -X- _ O
— -X- _ O
future -X- _ O
work -X- _ O
should -X- _ O
explore -X- _ O
additional -X- _ O
regularization -X- _ O
techniques -X- _ O
. -X- _ O

Abstractive -X- _ O
QA -X- _ B-TaskName
We -X- _ O
use -X- _ O
the -X- _ O
recently -X- _ O
proposed -X- _ O
ELI5 -X- _ B-DatasetName
dataset -X- _ O
to -X- _ O
test -X- _ O
the -X- _ O
model -X- _ O
’s -X- _ O
ability -X- _ O
to -X- _ O
generate -X- _ O
long -X- _ O
free- -X- _ O
form -X- _ O
answers -X- _ O
. -X- _ O
We -X- _ O
ﬁnd -X- _ O
BART -X- _ B-MethodName
outperforms -X- _ O
the -X- _ O
best -X- _ O
pre- -X- _ O
vious -X- _ O
work -X- _ O
by -X- _ O
1.2 -X- _ B-MetricValue
ROUGE -X- _ B-MetricName
- -X- _ I-MetricName
L -X- _ I-MetricName
, -X- _ O
but -X- _ O
the -X- _ O
dataset -X- _ O
remains -X- _ O
a -X- _ O
challenging -X- _ O
, -X- _ O
because -X- _ O
answers -X- _ O
are -X- _ O
only -X- _ O
weakly -X- _ O
speci- -X- _ O
ﬁed -X- _ O
by -X- _ O
the -X- _ O
question -X- _ O
. -X- _ O

Dialogue -X- _ O
We -X- _ O
evaluate -X- _ O
dialogue -X- _ B-TaskName
response -X- _ I-TaskName
generation -X- _ I-TaskName
on -X- _ O
CONVAI2 -X- _ B-MethodName
( -X- _ O
Dinan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
in -X- _ O
which -X- _ O
agents -X- _ O
must -X- _ O
generate -X- _ O
responses -X- _ O
conditioned -X- _ O
on -X- _ O
both -X- _ O
the -X- _ O
pre- -X- _ O
vious -X- _ O
context -X- _ O
and -X- _ O
a -X- _ O
textually -X- _ O
- -X- _ O
speciﬁed -X- _ O
persona -X- _ O
. -X- _ O
BART -X- _ B-MethodName
outperforms -X- _ O
previous -X- _ O
work -X- _ O
on -X- _ O
two -X- _ O
automated -X- _ O
metrics -X- _ O
. -X- _ O

Summarization -X- _ B-TaskName
To -X- _ O
provide -X- _ O
a -X- _ O
comparison -X- _ O
with -X- _ O
the -X- _ O
state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
art -X- _ O
in -X- _ O
summarization -X- _ B-TaskName
, -X- _ O
we -X- _ O
present -X- _ O
results -X- _ O
on -X- _ O
two -X- _ O
summarization -X- _ B-TaskName
datasets -X- _ O
, -X- _ O
CNN -X- _ B-DatasetName
/ -X- _ I-DatasetName
DailyMail -X- _ I-DatasetName
and -X- _ O
XSum -X- _ B-DatasetName
, -X- _ O
which -X- _ O
have -X- _ O
distinct -X- _ O
properties -X- _ O
. -X- _ O
Summaries -X- _ O
in -X- _ O
the -X- _ O
CNN -X- _ B-DatasetName
/ -X- _ I-DatasetName
DailyMail -X- _ I-DatasetName
tend -X- _ O
to -X- _ O
resemble -X- _ O
source -X- _ O
sentences -X- _ O
. -X- _ O
Extractive -X- _ O
models -X- _ O
do -X- _ O
well -X- _ O
here -X- _ O
, -X- _ O
and -X- _ O
even -X- _ O
the -X- _ O
baseline -X- _ O
of -X- _ O
the -X- _ O
ﬁrst -X- _ O
- -X- _ O
three -X- _ O
source -X- _ O
sentences -X- _ O
is -X- _ O
highly -X- _ O
competitive -X- _ O
. -X- _ O
Nevertheless -X- _ O
, -X- _ O
BART -X- _ B-MethodName
outperforms -X- _ O
all -X- _ O
existing -X- _ O
work -X- _ O
. -X- _ O
In -X- _ O
contrast -X- _ O
, -X- _ O
XSum -X- _ B-DatasetName
is -X- _ O
highly -X- _ O
abstractive -X- _ O
, -X- _ O
and -X- _ O
extrac- -X- _ O
tive -X- _ O
models -X- _ O
perform -X- _ O
poorly -X- _ O
. -X- _ O
BART -X- _ B-MethodName
outperforms -X- _ O
the -X- _ O
best -X- _ O
previous -X- _ O
work -X- _ O
, -X- _ O
which -X- _ O
leverages -X- _ O
BERT -X- _ B-MethodName
, -X- _ O
by -X- _ O
roughly -X- _ O
6.0 -X- _ B-MetricValue
points -X- _ O
on -X- _ O
all -X- _ O
ROUGE -X- _ B-MetricName
metrics -X- _ O
— -X- _ O
representing -X- _ O
a -X- _ O
sig- -X- _ O
niﬁcant -X- _ O
advance -X- _ O
in -X- _ O
performance -X- _ O
on -X- _ O
this -X- _ O
problem -X- _ O
. -X- _ O
Qual- -X- _ O
itatively -X- _ O
, -X- _ O
sample -X- _ O
quality -X- _ O
is -X- _ O
high -X- _ O
( -X- _ O
see -X- _ O
§ -X- _ O
6 -X- _ O
) -X- _ O
. -X- _ O

We -X- _ O
also -X- _ O
experiment -X- _ O
with -X- _ O
several -X- _ O
text -X- _ B-TaskName
generation -X- _ I-TaskName
tasks -X- _ O
. -X- _ O
BART -X- _ B-MethodName
is -X- _ O
ﬁne -X- _ O
- -X- _ O
tuned -X- _ O
as -X- _ O
a -X- _ O
standard -X- _ O
sequence -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
sequence -X- _ O
model -X- _ O
from -X- _ O
the -X- _ O
input -X- _ O
to -X- _ O
the -X- _ O
output -X- _ O
text -X- _ O
. -X- _ O
During -X- _ O
ﬁne- -X- _ O
tuning -X- _ O
we -X- _ O
use -X- _ O
a -X- _ O
label -X- _ O
smoothed -X- _ O
cross -X- _ B-MetricName
entropy -X- _ I-MetricName
loss -X- _ I-MetricName
( -X- _ O
Pereyra -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
, -X- _ O
with -X- _ O
the -X- _ O
smoothing -X- _ B-HyperparameterName
parameter -X- _ I-HyperparameterName
set -X- _ O
to -X- _ O
0.1 -X- _ B-HyperparameterValue
. -X- _ O
During -X- _ O
generation -X- _ B-TaskName
, -X- _ O
we -X- _ O
set -X- _ O
beam -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
as -X- _ O
5 -X- _ B-HyperparameterValue
, -X- _ O
remove -X- _ O
duplicated -X- _ O
trigrams -X- _ O
in -X- _ O
beam -X- _ O
search -X- _ O
, -X- _ O
and -X- _ O
tuned -X- _ O
the -X- _ O
model -X- _ O
with -X- _ O
min -X- _ O
- -X- _ O
len -X- _ O
, -X- _ O
max -X- _ O
- -X- _ O
len -X- _ O
, -X- _ O
length -X- _ O
penalty -X- _ O
on -X- _ O
the -X- _ O
validation -X- _ O
set -X- _ O
( -X- _ O
Fan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
. -X- _ O

Table -X- _ O
2 -X- _ O
compares -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
BART -X- _ B-MethodName
with -X- _ O
sev- -X- _ O
eral -X- _ O
recent -X- _ O
approaches -X- _ O
on -X- _ O
the -X- _ O
well -X- _ O
- -X- _ O
studied -X- _ O
SQuAD -X- _ B-DatasetName
and -X- _ O
GLUE -X- _ B-DatasetName
tasks -X- _ O
( -X- _ O
Warstadt -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
; -X- _ O
Socher -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2013 -X- _ O
; -X- _ O
Dolan -X- _ O
& -X- _ O
Brockett -X- _ O
, -X- _ O
2005 -X- _ O
; -X- _ O
Agirre -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2007 -X- _ O
; -X- _ O
Williams -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
; -X- _ O
Dagan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2006 -X- _ O
; -X- _ O
Levesque -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2011 -X- _ O
) -X- _ O
. -X- _ O
The -X- _ O
most -X- _ O
directly -X- _ O
comparable -X- _ O
baseline -X- _ O
is -X- _ O
RoBERTa -X- _ B-MethodName
, -X- _ O
which -X- _ O
was -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
with -X- _ O
the -X- _ O
same -X- _ O
resources -X- _ O
, -X- _ O
but -X- _ O
a -X- _ O
different -X- _ O
objective -X- _ O
. -X- _ O
Overall -X- _ O
, -X- _ O
BART -X- _ B-MethodName
performs -X- _ O
simi- -X- _ O
larly -X- _ O
, -X- _ O
with -X- _ O
only -X- _ O
small -X- _ O
differences -X- _ O
between -X- _ O
the -X- _ O
models -X- _ O
on -X- _ O
most -X- _ O
tasks -X- _ O
. -X- _ O
suggesting -X- _ O
that -X- _ O
BART -X- _ B-MethodName
’s -X- _ O
improvements -X- _ O
on -X- _ O
generation -X- _ B-TaskName
tasks -X- _ O
do -X- _ O
not -X- _ O
come -X- _ O
at -X- _ O
the -X- _ O
expense -X- _ O
of -X- _ O
clas- -X- _ B-TaskName
siﬁcation -X- _ I-TaskName
performance -X- _ O
. -X- _ O

on -X- _ O
the -X- _ O
CNN -X- _ B-DatasetName
/ -X- _ I-DatasetName
DM -X- _ I-DatasetName
summarization -X- _ B-TaskName
dataset -X- _ O
, -X- _ O
we -X- _ O
hypothe- -X- _ O
sised -X- _ O
that -X- _ O
larger -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
models -X- _ O
may -X- _ O
be -X- _ O
better -X- _ O
able -X- _ O
to -X- _ O
learn -X- _ O
from -X- _ O
this -X- _ O
task -X- _ O
. -X- _ O
To -X- _ O
help -X- _ O
the -X- _ O
model -X- _ O
better -X- _ O
ﬁt -X- _ O
the -X- _ O
data -X- _ O
, -X- _ O
we -X- _ O
disabled -X- _ O
dropout -X- _ B-HyperparameterName
for -X- _ O
the -X- _ O
ﬁnal -X- _ O
10 -X- _ O
% -X- _ O
of -X- _ O
training -X- _ O
steps -X- _ O
. -X- _ O
We -X- _ O
use -X- _ O
the -X- _ O
same -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
data -X- _ O
as -X- _ O
Liu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
consisting -X- _ O
of -X- _ O
160Gb -X- _ O
of -X- _ O
news -X- _ O
, -X- _ O
books -X- _ O
, -X- _ O
stories -X- _ O
, -X- _ O
and -X- _ O
web -X- _ O
text -X- _ O
. -X- _ O

BART -X- _ B-MethodName
achieves -X- _ O
the -X- _ O
most -X- _ O
consistently -X- _ O
strong -X- _ O
perfor- -X- _ O
mance -X- _ O
. -X- _ O
With -X- _ O
the -X- _ O
exception -X- _ O
of -X- _ O
ELI5 -X- _ B-DatasetName
, -X- _ O
BART -X- _ B-MethodName
models -X- _ O
using -X- _ O
text -X- _ O
- -X- _ O
inﬁlling -X- _ O
perform -X- _ O
well -X- _ O
on -X- _ O
all -X- _ O
tasks -X- _ O
. -X- _ O

We -X- _ O
pre -X- _ O
- -X- _ O
train -X- _ O
a -X- _ O
large -X- _ O
model -X- _ O
with -X- _ O
12 -X- _ B-HyperparameterValue
layers -X- _ B-HyperparameterName
in -X- _ O
each -X- _ O
of -X- _ O
the -X- _ O
encoder -X- _ O
and -X- _ O
decoder -X- _ O
, -X- _ O
and -X- _ O
a -X- _ O
hidden -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
of -X- _ O
1024 -X- _ B-HyperparameterValue
. -X- _ O
Fol- -X- _ O
lowing -X- _ O
RoBERTa -X- _ B-MethodName
( -X- _ O
Liu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
a -X- _ O
batch -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
of -X- _ O
8000 -X- _ B-HyperparameterValue
, -X- _ O
and -X- _ O
train -X- _ O
the -X- _ O
model -X- _ O
for -X- _ O
500000 -X- _ B-HyperparameterValue
steps -X- _ B-HyperparameterName
. -X- _ O
Docu- -X- _ O
ments -X- _ O
are -X- _ O
tokenized -X- _ O
with -X- _ O
the -X- _ O
same -X- _ O
byte -X- _ O
- -X- _ O
pair -X- _ O
encoding -X- _ O
as -X- _ O
GPT-2 -X- _ O
( -X- _ O
Radford -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
. -X- _ O
Based -X- _ O
on -X- _ O
the -X- _ O
results -X- _ O
in -X- _ O
Section -X- _ O
§ -X- _ O
4 -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
a -X- _ O
combination -X- _ O
of -X- _ O
text -X- _ O
inﬁlling -X- _ O
and -X- _ O
sentence -X- _ O
permutation -X- _ O
. -X- _ O
We -X- _ O
mask -X- _ O
30 -X- _ O
% -X- _ O
of -X- _ O
tokens -X- _ O
in -X- _ O
each -X- _ O
document -X- _ O
, -X- _ O
and -X- _ O
permute -X- _ O
all -X- _ O
sentences -X- _ O
. -X- _ O
Although -X- _ O
sen- -X- _ O
tence -X- _ O
permutation -X- _ O
only -X- _ O
shows -X- _ O
signiﬁcant -X- _ O
additive -X- _ O
gains -X- _ O

Recent -X- _ O
work -X- _ O
has -X- _ O
shown -X- _ O
that -X- _ O
downstream -X- _ O
performance -X- _ O
can -X- _ O
dramatically -X- _ O
improve -X- _ O
when -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
is -X- _ O
scaled -X- _ O
to -X- _ O
large -X- _ O
batch -X- _ O
sizes -X- _ O
( -X- _ O
Yang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Liu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
and -X- _ O
corpora -X- _ O
. -X- _ O
To -X- _ O
test -X- _ O
how -X- _ O
well -X- _ O
BART -X- _ B-MethodName
performs -X- _ O
in -X- _ O
this -X- _ O
regime -X- _ O
, -X- _ O
and -X- _ O
to -X- _ O
create -X- _ O
a -X- _ O
useful -X- _ O
model -X- _ O
for -X- _ O
downstream -X- _ O
tasks -X- _ O
, -X- _ O
we -X- _ O
trained -X- _ O
BART -X- _ B-MethodName
using -X- _ O
the -X- _ O
same -X- _ O
scale -X- _ O
as -X- _ O
the -X- _ O
RoBERTa -X- _ B-MethodName
model -X- _ O
. -X- _ O

Pure -X- _ O
language -X- _ O
models -X- _ O
perform -X- _ O
best -X- _ O
on -X- _ O
ELI5 -X- _ B-DatasetName
The -X- _ O
ELI5 -X- _ B-DatasetName
dataset -X- _ O
is -X- _ O
an -X- _ O
outlier -X- _ O
, -X- _ O
with -X- _ O
much -X- _ O
higher -X- _ O
perplex- -X- _ B-MetricName
ities -X- _ I-MetricName
than -X- _ O
other -X- _ O
tasks -X- _ O
, -X- _ O
and -X- _ O
is -X- _ O
the -X- _ O
only -X- _ O
generation -X- _ B-TaskName
task -X- _ O
where -X- _ O
other -X- _ O
models -X- _ O
outperform -X- _ O
BART -X- _ B-MethodName
. -X- _ O
A -X- _ O
pure -X- _ O
lan- -X- _ O
guage -X- _ O
model -X- _ O
performs -X- _ O
best -X- _ O
, -X- _ O
suggesting -X- _ O
that -X- _ O
BART -X- _ B-MethodName
is -X- _ O
less -X- _ O
effective -X- _ O
when -X- _ O
the -X- _ O
output -X- _ O
is -X- _ O
only -X- _ O
loosely -X- _ O
con- -X- _ O
strained -X- _ O
by -X- _ O
the -X- _ O
input -X- _ O
. -X- _ O

The -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
objective -X- _ O
is -X- _ O
not -X- _ O
the -X- _ O
only -X- _ O
important -X- _ O
factor -X- _ O
Our -X- _ O
Permuted -X- _ O
Language -X- _ O
Model -X- _ O
performs -X- _ O
less -X- _ O
well -X- _ O
than -X- _ O
XLNet -X- _ B-MethodName
( -X- _ O
Yang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
. -X- _ O
Some -X- _ O
of -X- _ O
this -X- _ O
dif- -X- _ O
ference -X- _ O
is -X- _ O
likely -X- _ O
due -X- _ O
to -X- _ O
not -X- _ O
including -X- _ O
other -X- _ O
architectural -X- _ O
improvements -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
relative -X- _ O
- -X- _ O
position -X- _ O
embeddings -X- _ O
or -X- _ O
segment -X- _ O
- -X- _ O
level -X- _ O
recurrence -X- _ O
. -X- _ O

Bidirectional -X- _ O
encoders -X- _ O
are -X- _ O
crucial -X- _ O
for -X- _ O
SQuAD -X- _ B-DatasetName
As -X- _ O
noted -X- _ O
in -X- _ O
previous -X- _ O
work -X- _ O
( -X- _ O
Devlin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
just -X- _ O
left -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
right -X- _ O
decoder -X- _ O
performs -X- _ O
poorly -X- _ O
on -X- _ O
SQuAD -X- _ B-DatasetName
, -X- _ O
be- -X- _ O
cause -X- _ O
future -X- _ O
context -X- _ O
is -X- _ O
crucial -X- _ O
in -X- _ O
classiﬁcation -X- _ O
deci- -X- _ O
sions -X- _ O
. -X- _ O
However -X- _ O
, -X- _ O
BART -X- _ B-MethodName
achieves -X- _ O
similar -X- _ O
performance -X- _ O
with -X- _ O
only -X- _ O
half -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
bidirectional -X- _ O
layers -X- _ O
. -X- _ O

Left -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
right -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
improves -X- _ O
generation -X- _ B-TaskName
The -X- _ O
Masked -X- _ O
Language -X- _ O
Model -X- _ O
and -X- _ O
the -X- _ O
Permuted -X- _ O
Language -X- _ O
Model -X- _ O
perform -X- _ O
less -X- _ O
well -X- _ O
than -X- _ O
others -X- _ O
on -X- _ O
generation -X- _ B-TaskName
, -X- _ O
and -X- _ O
are -X- _ O
the -X- _ O
only -X- _ O
models -X- _ O
we -X- _ O
consider -X- _ O
that -X- _ O
do -X- _ O
not -X- _ O
include -X- _ O
left -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
right -X- _ O
auto -X- _ O
- -X- _ O
regressive -X- _ O
language -X- _ O
modelling -X- _ O
during -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
. -X- _ O

Token -X- _ O
masking -X- _ O
is -X- _ O
crucial -X- _ O
Pre -X- _ O
- -X- _ O
training -X- _ O
objectives -X- _ O
based -X- _ O
on -X- _ O
rotating -X- _ O
documents -X- _ O
or -X- _ O
permuting -X- _ O
sentences -X- _ O
perform -X- _ O
poorly -X- _ O
in -X- _ O
isolation -X- _ O
. -X- _ O
The -X- _ O
successful -X- _ O
methods -X- _ O
either -X- _ O
use -X- _ O
token -X- _ O
deletion -X- _ O
or -X- _ O
masking -X- _ O
, -X- _ O
or -X- _ O
self -X- _ O
- -X- _ O
attention -X- _ O
masks -X- _ O
. -X- _ O
Deletion -X- _ O
appears -X- _ O
to -X- _ O
outperform -X- _ O
masking -X- _ O
on -X- _ O
generation -X- _ B-TaskName
tasks -X- _ O
. -X- _ O

Performance -X- _ O
of -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
methods -X- _ O
varies -X- _ O
signiﬁ- -X- _ O
cantly -X- _ O
across -X- _ O
tasks -X- _ O
The -X- _ O
effectiveness -X- _ O
of -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
methods -X- _ O
is -X- _ O
highly -X- _ O
dependent -X- _ O
on -X- _ O
the -X- _ O
task -X- _ O
. -X- _ O
For -X- _ O
exam- -X- _ O
ple -X- _ O
, -X- _ O
a -X- _ O
simple -X- _ O
language -X- _ O
model -X- _ O
achieves -X- _ O
the -X- _ O
best -X- _ O
ELI5 -X- _ B-DatasetName
performance -X- _ O
, -X- _ O
but -X- _ O
the -X- _ O
worst -X- _ O
SQUAD -X- _ B-DatasetName
results -X- _ O
. -X- _ O

CNN -X- _ B-DatasetName
/ -X- _ I-DatasetName
DM -X- _ I-DatasetName
( -X- _ O
Hermann -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
, -X- _ O
a -X- _ O
news -X- _ B-TaskName
summa- -X- _ I-TaskName
rization -X- _ I-TaskName
dataset -X- _ O
. -X- _ O
Summaries -X- _ O
here -X- _ O
are -X- _ O
typically -X- _ O
closely -X- _ O
related -X- _ O
to -X- _ O
source -X- _ O
sentences -X- _ O
. -X- _ O

ConvAI2 -X- _ B-DatasetName
( -X- _ O
Dinan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
a -X- _ O
dialogue -X- _ B-TaskName
response -X- _ I-TaskName
generation -X- _ I-TaskName
task -X- _ O
, -X- _ O
conditioned -X- _ O
on -X- _ O
context -X- _ O
and -X- _ O
a -X- _ O
persona -X- _ O
. -X- _ O

XSum -X- _ B-DatasetName
( -X- _ O
Narayan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
, -X- _ O
a -X- _ O
news -X- _ B-TaskName
summarization -X- _ I-TaskName
dataset -X- _ O
with -X- _ O
highly -X- _ O
abstractive -X- _ O
summaries -X- _ O
. -X- _ O

ELI5 -X- _ B-DatasetName
( -X- _ O
Fan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
a -X- _ O
long -X- _ O
- -X- _ O
form -X- _ O
abstractive -X- _ O
ques- -X- _ B-TaskName
tion -X- _ I-TaskName
answering -X- _ I-TaskName
dataset -X- _ O
. -X- _ O
Models -X- _ O
generate -X- _ O
answers -X- _ O
con- -X- _ O
ditioned -X- _ O
on -X- _ O
the -X- _ O
concatenation -X- _ O
of -X- _ O
a -X- _ O
question -X- _ O
and -X- _ O
sup- -X- _ O
porting -X- _ O
documents -X- _ O
. -X- _ O

MNLI -X- _ B-DatasetName
( -X- _ O
Williams -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
, -X- _ O
a -X- _ O
bitext -X- _ B-TaskName
classiﬁcation -X- _ I-TaskName
task -X- _ O
to -X- _ O
predict -X- _ O
whether -X- _ O
one -X- _ O
sentence -X- _ O
entails -X- _ O
another -X- _ O
. -X- _ O
The -X- _ O
ﬁne -X- _ O
- -X- _ O
tuned -X- _ O
model -X- _ O
concatenates -X- _ O
the -X- _ O
two -X- _ O
sentences -X- _ O
with -X- _ O
appended -X- _ O
an -X- _ O
EOS -X- _ O
token -X- _ O
, -X- _ O
and -X- _ O
passes -X- _ O
them -X- _ O
to -X- _ O
both -X- _ O
the -X- _ O
BART -X- _ B-MethodName
encoder -X- _ O
and -X- _ O
decoder -X- _ O
. -X- _ O
In -X- _ O
contrast -X- _ O
to -X- _ O
BERT -X- _ B-MethodName
, -X- _ O
the -X- _ O
representation -X- _ O
of -X- _ O
the -X- _ O
EOS -X- _ O
token -X- _ O
is -X- _ O
used -X- _ O
to -X- _ O
classify -X- _ O
the -X- _ O
sentences -X- _ O
relations -X- _ O
. -X- _ O

SQuAD -X- _ B-DatasetName
( -X- _ O
Rajpurkar -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016)a -X- _ O
an -X- _ O
extractive -X- _ O
ques- -X- _ B-TaskName
tion -X- _ I-TaskName
answering -X- _ I-TaskName
task -X- _ O
on -X- _ O
Wikipedia -X- _ O
paragraphs -X- _ O
. -X- _ O
Answers -X- _ O
are -X- _ O
text -X- _ O
spans -X- _ O
extracted -X- _ O
from -X- _ O
a -X- _ O
given -X- _ O
document -X- _ O
context -X- _ O
. -X- _ O
Similar -X- _ O
to -X- _ O
BERT -X- _ B-MethodName
( -X- _ O
Devlin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
concate- -X- _ O
nated -X- _ O
question -X- _ O
and -X- _ O
context -X- _ O
as -X- _ O
input -X- _ O
to -X- _ O
the -X- _ O
encoder -X- _ O
of -X- _ O
BART -X- _ B-MethodName
, -X- _ O
and -X- _ O
additionally -X- _ O
pass -X- _ O
them -X- _ O
to -X- _ O
the -X- _ O
decoder -X- _ O
. -X- _ O
The -X- _ O
model -X- _ O
includes -X- _ O
classiﬁers -X- _ O
to -X- _ O
predict -X- _ O
the -X- _ O
start -X- _ O
and -X- _ O
end -X- _ O
indices -X- _ O
of -X- _ O
each -X- _ O
token -X- _ O
. -X- _ O

We -X- _ O
experiment -X- _ O
with -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
treating -X- _ O
the -X- _ O
task -X- _ O
as -X- _ O
a -X- _ O
stan- -X- _ O
dard -X- _ O
sequence -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
sequence -X- _ O
problem -X- _ O
, -X- _ O
where -X- _ O
the -X- _ O
source -X- _ O
input -X- _ O
to -X- _ O
the -X- _ O
encoder -X- _ O
and -X- _ O
the -X- _ O
target -X- _ O
is -X- _ O
the -X- _ O
decoder -X- _ O
out- -X- _ O
put -X- _ O
, -X- _ O
or -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
adding -X- _ O
the -X- _ O
source -X- _ O
as -X- _ O
preﬁx -X- _ O
to -X- _ O
the -X- _ O
target -X- _ O
in -X- _ O
the -X- _ O
decoder -X- _ O
, -X- _ O
with -X- _ O
a -X- _ O
loss -X- _ O
only -X- _ O
on -X- _ O
the -X- _ O
target -X- _ O
part -X- _ O
of -X- _ O
the -X- _ O
sequence -X- _ O
. -X- _ O
We -X- _ O
ﬁnd -X- _ O
the -X- _ O
former -X- _ O
works -X- _ O
better -X- _ O
for -X- _ O
BART -X- _ B-MethodName
models -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
latter -X- _ O
for -X- _ O
other -X- _ O
models -X- _ O
. -X- _ O
To -X- _ O
most -X- _ O
directly -X- _ O
compare -X- _ O
our -X- _ O
models -X- _ O
on -X- _ O
their -X- _ O
ability -X- _ O
to -X- _ O
model -X- _ O
their -X- _ O
ﬁne -X- _ O
- -X- _ O
tuning -X- _ O
objective -X- _ O
( -X- _ O
the -X- _ O
log -X- _ B-MetricName
likelihood -X- _ I-MetricName
of -X- _ O
the -X- _ O
human -X- _ O
text -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
report -X- _ O
perplexity -X- _ B-MetricName
in -X- _ O
Table -X- _ O
1 -X- _ O
. -X- _ O

For -X- _ O
the -X- _ O
Permuted -X- _ O
LM -X- _ O
, -X- _ O
Masked -X- _ O
LM -X- _ O
and -X- _ O
Multitask -X- _ O
Masked -X- _ O
LM -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
two -X- _ O
- -X- _ O
stream -X- _ O
attention -X- _ O
( -X- _ O
Yang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
to -X- _ O
efﬁciently -X- _ O
compute -X- _ O
likelihoods -X- _ O
of -X- _ O
the -X- _ O
output -X- _ O
part -X- _ O
of -X- _ O
the -X- _ O
sequence -X- _ O
( -X- _ O
using -X- _ O
a -X- _ O
diagonal -X- _ O
self -X- _ O
- -X- _ O
attention -X- _ O
mask -X- _ O
on -X- _ O
the -X- _ O
output -X- _ O
to -X- _ O
predict -X- _ O
words -X- _ O
left -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
right -X- _ O
) -X- _ O
. -X- _ O

2019 -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
mask -X- _ O
a -X- _ O
span -X- _ O
containing -X- _ O
50 -X- _ O
% -X- _ O
of -X- _ O
tokens -X- _ O
, -X- _ O
and -X- _ O
train -X- _ O
a -X- _ O
sequence -X- _ O
to -X- _ O
sequence -X- _ O
model -X- _ O
to -X- _ O
predict -X- _ O
the -X- _ O
masked -X- _ O
tokens -X- _ O
. -X- _ O

Multitask -X- _ O
Masked -X- _ O
Language -X- _ O
Model -X- _ O
As -X- _ O
in -X- _ O
UniLM -X- _ B-MethodName
( -X- _ O
Dong -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
train -X- _ O
a -X- _ O
Masked -X- _ O
Language -X- _ O
Model -X- _ O
with -X- _ O
additional -X- _ O
self -X- _ O
- -X- _ O
attention -X- _ O
masks -X- _ O
. -X- _ O
Self -X- _ O
at- -X- _ O
tention -X- _ O
masks -X- _ O
are -X- _ O
chosen -X- _ O
randomly -X- _ O
in -X- _ O
with -X- _ O
the -X- _ O
follow -X- _ O
proportions -X- _ O
: -X- _ O
1/6 -X- _ O
left -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
right -X- _ O
, -X- _ O
1/6 -X- _ O
right -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
left -X- _ O
, -X- _ O
1/3 -X- _ O
un- -X- _ O
masked -X- _ O
, -X- _ O
and -X- _ O
1/3 -X- _ O
with -X- _ O
the -X- _ O
ﬁrst -X- _ O
50 -X- _ O
% -X- _ O
of -X- _ O
tokens -X- _ O
unmasked -X- _ O
and -X- _ O
a -X- _ O
left -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
right -X- _ O
mask -X- _ O
for -X- _ O
the -X- _ O
remainder -X- _ O
. -X- _ O

Masked -X- _ O
Language -X- _ O
Model -X- _ O
Following -X- _ O
BERT -X- _ B-MethodName
( -X- _ O
Devlin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
replace -X- _ O
15 -X- _ O
% -X- _ O
of -X- _ O
tokens -X- _ O
with -X- _ O
[ -X- _ O
MASK -X- _ O
] -X- _ O
symbols -X- _ O
, -X- _ O
and -X- _ O
train -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
independently -X- _ O
predict -X- _ O
the -X- _ O
original -X- _ O
tokens -X- _ O
. -X- _ O

Permuted -X- _ O
Language -X- _ O
Model -X- _ O
Based -X- _ O
on -X- _ O
XLNet -X- _ B-MethodName
( -X- _ O
Yang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
sample -X- _ O
1/6 -X- _ O
of -X- _ O
the -X- _ O
tokens -X- _ O
, -X- _ O
and -X- _ O
gener- -X- _ O
ate -X- _ O
them -X- _ O
in -X- _ O
a -X- _ O
random -X- _ O
order -X- _ O
autoregressively -X- _ O
. -X- _ O
For -X- _ O
con- -X- _ O
sistency -X- _ O
with -X- _ O
other -X- _ O
models -X- _ O
, -X- _ O
we -X- _ O
do -X- _ O
not -X- _ O
implement -X- _ O
the -X- _ O
relative -X- _ O
positional -X- _ O
embeddings -X- _ O
or -X- _ O
attention -X- _ O
across -X- _ O
seg- -X- _ O
ments -X- _ O
from -X- _ O
XLNet -X- _ B-MethodName
. -X- _ O

2018 -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
train -X- _ O
a -X- _ O
left -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
right -X- _ O
Transformer -X- _ O
language -X- _ O
model -X- _ O
. -X- _ O
This -X- _ O
model -X- _ O
is -X- _ O
equivalent -X- _ O
to -X- _ O
the -X- _ O
BART -X- _ B-MethodName
decoder -X- _ O
, -X- _ O
without -X- _ O
cross -X- _ O
- -X- _ O
attention -X- _ O
. -X- _ O

re -X- _ O
- -X- _ O
implement -X- _ O
strong -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
approaches -X- _ O
recently -X- _ O
proposed -X- _ O
for -X- _ O
discriminative -X- _ B-TaskName
and -X- _ O
generation -X- _ B-TaskName
tasks -X- _ O
. -X- _ O
We -X- _ O
aim -X- _ O
, -X- _ O
as -X- _ O
much -X- _ O
as -X- _ O
possible -X- _ O
, -X- _ O
to -X- _ O
control -X- _ O
for -X- _ O
differences -X- _ O
un- -X- _ O
related -X- _ O
to -X- _ O
the -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
objective -X- _ O
. -X- _ O
However -X- _ O
, -X- _ O
we -X- _ O
do -X- _ O
make -X- _ O
minor -X- _ O
changes -X- _ O
to -X- _ O
the -X- _ O
learning -X- _ B-HyperparameterName
rate -X- _ I-HyperparameterName
and -X- _ O
usage -X- _ O
of -X- _ O
layer -X- _ B-HyperparameterName
normalisation -X- _ I-HyperparameterName
in -X- _ O
order -X- _ O
to -X- _ O
improve -X- _ O
performance -X- _ O
( -X- _ O
tuning -X- _ O
these -X- _ O
separately -X- _ O
for -X- _ O
each -X- _ O
objective -X- _ O
) -X- _ O
. -X- _ O
For -X- _ O
refer- -X- _ O
ence -X- _ O
, -X- _ O
we -X- _ O
compare -X- _ O
our -X- _ O
implementations -X- _ O
with -X- _ O
published -X- _ O
numbers -X- _ O
from -X- _ O
BERT -X- _ B-MethodName
, -X- _ O
which -X- _ O
was -X- _ O
also -X- _ O
trained -X- _ O
for -X- _ O
1 -X- _ O
M -X- _ O
steps -X- _ O
on -X- _ O
a -X- _ O
combination -X- _ O
of -X- _ O
books -X- _ O
and -X- _ O
Wikipedia -X- _ B-DatasetName
data -X- _ I-DatasetName
. -X- _ O
We -X- _ O
compare -X- _ O
the -X- _ O
following -X- _ O
approaches -X- _ O
: -X- _ O

While -X- _ O
many -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
objectives -X- _ O
have -X- _ O
been -X- _ O
pro- -X- _ O
posed -X- _ O
, -X- _ O
fair -X- _ O
comparisons -X- _ O
between -X- _ O
these -X- _ O
have -X- _ O
been -X- _ O
dif- -X- _ O
ﬁcult -X- _ O
to -X- _ O
perform -X- _ O
, -X- _ O
at -X- _ O
least -X- _ O
in -X- _ O
part -X- _ O
due -X- _ O
to -X- _ O
differences -X- _ O
in -X- _ O
training -X- _ O
data -X- _ O
, -X- _ O
training -X- _ O
resources -X- _ O
, -X- _ O
architectural -X- _ O
differ- -X- _ O
ences -X- _ O
between -X- _ O
models -X- _ O
, -X- _ O
and -X- _ O
ﬁne -X- _ O
- -X- _ O
tuning -X- _ O
procedures -X- _ O
. -X- _ O
We -X- _ O

BART -X- _ B-MethodName
supports -X- _ O
a -X- _ O
much -X- _ O
wider -X- _ O
range -X- _ O
of -X- _ O
noising -X- _ O
schemes -X- _ O
during -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
than -X- _ O
previous -X- _ O
work -X- _ O
. -X- _ O
We -X- _ O
compare -X- _ O
a -X- _ O
range -X- _ O
of -X- _ O
options -X- _ O
using -X- _ O
base -X- _ O
- -X- _ O
size -X- _ O
models -X- _ O
( -X- _ O
6 -X- _ B-HyperparameterValue
encoder -X- _ O
and -X- _ O
6 -X- _ B-HyperparameterValue
decoder -X- _ O
layers -X- _ B-HyperparameterName
, -X- _ O
with -X- _ O
a -X- _ O
hidden -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
of -X- _ O
768 -X- _ B-HyperparameterValue
) -X- _ O
, -X- _ O
evaluated -X- _ O
on -X- _ O
a -X- _ O
representative -X- _ O
subset -X- _ O
of -X- _ O
the -X- _ O
tasks -X- _ O
we -X- _ O
will -X- _ O
consider -X- _ O
for -X- _ O
the -X- _ O
full -X- _ O
large -X- _ O
scale -X- _ O
experiments -X- _ O
in -X- _ O
§ -X- _ O
5 -X- _ O
. -X- _ O

We -X- _ O
also -X- _ O
explore -X- _ O
using -X- _ O
BART -X- _ B-MethodName
to -X- _ O
improve -X- _ O
machine -X- _ B-TaskName
trans- -X- _ I-TaskName
lation -X- _ I-TaskName
decoders -X- _ O
for -X- _ O
translating -X- _ O
into -X- _ O
English -X- _ O
. -X- _ O
Previous -X- _ O
work -X- _ O
Edunov -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
( -X- _ O
2019 -X- _ O
) -X- _ O
has -X- _ O
shown -X- _ O
that -X- _ O
models -X- _ O
can -X- _ O
be -X- _ O
improved -X- _ O
by -X- _ O
incorporating -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
encoders -X- _ O
, -X- _ O
but -X- _ O
gains -X- _ O
from -X- _ O
using -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
language -X- _ O
models -X- _ O
in -X- _ O
de- -X- _ O
coders -X- _ O
have -X- _ O
been -X- _ O
limited -X- _ O
. -X- _ O
We -X- _ O
show -X- _ O
that -X- _ O
it -X- _ O
is -X- _ O
possible -X- _ O
to -X- _ O
use -X- _ O
the -X- _ O
entire -X- _ O
BART -X- _ B-MethodName
model -X- _ O
( -X- _ O
both -X- _ O
encoder -X- _ O
and -X- _ O
de- -X- _ O
coder -X- _ O
) -X- _ O
as -X- _ O
a -X- _ O
single -X- _ O
pretrained -X- _ O
decoder -X- _ O
for -X- _ O
machine -X- _ O
trans- -X- _ O
lation -X- _ O
, -X- _ O
by -X- _ O
adding -X- _ O
a -X- _ O
new -X- _ O
set -X- _ O
of -X- _ O
encoder -X- _ O
parameters -X- _ O
that -X- _ O
are -X- _ O
learned -X- _ O
from -X- _ O
bitext -X- _ O
( -X- _ O
see -X- _ O
Figure -X- _ O
3b -X- _ O
) -X- _ O
. -X- _ O
More -X- _ O
precisely -X- _ O
, -X- _ O
we -X- _ O
replace -X- _ O
BART -X- _ B-MethodName
’s -X- _ O
encoder -X- _ O
embed- -X- _ O
ding -X- _ O
layer -X- _ O
with -X- _ O
a -X- _ O
new -X- _ O
randomly -X- _ O
initialized -X- _ O
encoder -X- _ O
. -X- _ O
The -X- _ O
model -X- _ O
is -X- _ O
trained -X- _ O
end -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
end -X- _ O
, -X- _ O
which -X- _ O
trains -X- _ O
the -X- _ O
new -X- _ O
encoder -X- _ O
to -X- _ O
map -X- _ O
foreign -X- _ O
words -X- _ O
into -X- _ O
an -X- _ O
input -X- _ O
that -X- _ O
BART -X- _ B-MethodName
can -X- _ O
de -X- _ O
- -X- _ O
noise -X- _ O
to -X- _ O
English -X- _ O
. -X- _ O
The -X- _ O
new -X- _ O
encoder -X- _ O
can -X- _ O
use -X- _ O
a -X- _ O
separate -X- _ O
vocabulary -X- _ O
from -X- _ O
the -X- _ O
original -X- _ O
BART -X- _ O
model -X- _ O
. -X- _ O
We -X- _ O
train -X- _ O
the -X- _ O
source -X- _ O
encoder -X- _ O
in -X- _ O
two -X- _ O
steps -X- _ O
, -X- _ O
in -X- _ O
both -X- _ O
cases -X- _ O
backpropagating -X- _ O
the -X- _ O
cross -X- _ B-MetricName
- -X- _ I-MetricName
entropy -X- _ I-MetricName
loss -X- _ I-MetricName
from -X- _ O
the -X- _ O
output -X- _ O
of -X- _ O
the -X- _ O
BART -X- _ B-MethodName
model -X- _ O
. -X- _ O
In -X- _ O
the -X- _ O
ﬁrst -X- _ O
step -X- _ O
, -X- _ O
we -X- _ O
freeze -X- _ O
most -X- _ O
of -X- _ O
BART -X- _ B-MethodName
parameters -X- _ O
and -X- _ O
only -X- _ O
update -X- _ O
the -X- _ O
ran- -X- _ O
domly -X- _ O
initialized -X- _ O
source -X- _ O
encoder -X- _ O
, -X- _ O
the -X- _ O
BART -X- _ B-MethodName
positional -X- _ O
embeddings -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
self -X- _ O
- -X- _ O
attention -X- _ O
input -X- _ O
projection -X- _ O
ma- -X- _ O
trix -X- _ O
of -X- _ O
BART -X- _ O
’s -X- _ O
encoder -X- _ O
ﬁrst -X- _ O
layer -X- _ O
. -X- _ O
In -X- _ O
the -X- _ O
second -X- _ O
step -X- _ O
, -X- _ O
we -X- _ O
train -X- _ O
all -X- _ O
model -X- _ O
parameters -X- _ O
for -X- _ O
a -X- _ O
small -X- _ O
number -X- _ O
of -X- _ O
iterations -X- _ B-HyperparameterName
. -X- _ O

input -X- _ O
but -X- _ O
manipulated -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
closely -X- _ O
related -X- _ O
to -X- _ O
the -X- _ O
denoising -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
objective -X- _ O
. -X- _ O
Here -X- _ O
, -X- _ O
the -X- _ O
encoder -X- _ O
in- -X- _ O
put -X- _ O
is -X- _ O
the -X- _ O
input -X- _ O
sequence -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
decoder -X- _ O
generates -X- _ O
outputs -X- _ O
autoregressively -X- _ O
. -X- _ O

Because -X- _ O
BART -X- _ B-MethodName
has -X- _ O
an -X- _ O
autoregressive -X- _ O
decoder -X- _ O
, -X- _ O
it -X- _ O
can -X- _ O
be -X- _ O
directly -X- _ O
ﬁne -X- _ O
tuned -X- _ O
for -X- _ O
sequence -X- _ B-MethodName
generation -X- _ I-MethodName
tasks -X- _ O
such -X- _ O
as -X- _ O
abstractive -X- _ O
question -X- _ B-MethodName
answering -X- _ I-MethodName
and -X- _ O
summarization -X- _ B-MethodName
. -X- _ O
In -X- _ O
both -X- _ O
of -X- _ O
these -X- _ O
tasks -X- _ O
, -X- _ O
information -X- _ O
is -X- _ O
copied -X- _ O
from -X- _ O
the -X- _ O

For -X- _ O
token -X- _ B-TaskName
classiﬁcation -X- _ I-TaskName
tasks -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
answer -X- _ O
endpoint -X- _ O
classiﬁcation -X- _ O
for -X- _ O
SQuAD -X- _ B-DatasetName
, -X- _ O
we -X- _ O
feed -X- _ O
the -X- _ O
complete -X- _ O
doc- -X- _ O
ument -X- _ O
into -X- _ O
the -X- _ O
encoder -X- _ O
and -X- _ O
decoder -X- _ O
, -X- _ O
and -X- _ O
use -X- _ O
the -X- _ O
top -X- _ O
hidden -X- _ O
state -X- _ O
of -X- _ O
the -X- _ O
decoder -X- _ O
as -X- _ O
a -X- _ O
representation -X- _ O
for -X- _ O
each -X- _ O
word -X- _ O
. -X- _ O
This -X- _ O
representation -X- _ O
is -X- _ O
used -X- _ O
to -X- _ O
classify -X- _ O
the -X- _ O
token -X- _ O
. -X- _ O

For -X- _ O
sequence -X- _ B-TaskName
classiﬁcation -X- _ I-TaskName
tasks -X- _ O
, -X- _ O
the -X- _ O
same -X- _ O
input -X- _ O
is -X- _ O
fed -X- _ O
into -X- _ O
the -X- _ O
encoder -X- _ O
and -X- _ O
decoder -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
ﬁnal -X- _ O
hidden -X- _ O
state -X- _ O
of -X- _ O
the -X- _ O
ﬁnal -X- _ O
decoder -X- _ O
token -X- _ O
is -X- _ O
fed -X- _ O
into -X- _ O
new -X- _ O
multi -X- _ O
- -X- _ O
class -X- _ O
linear -X- _ O
classiﬁer -X- _ O
. -X- _ O
This -X- _ O
approach -X- _ O
is -X- _ O
related -X- _ O
to -X- _ O
the -X- _ O
CLS -X- _ O
token -X- _ O
in -X- _ O
BERT -X- _ B-MethodName
; -X- _ O
however -X- _ O
we -X- _ O
add -X- _ O
the -X- _ O
additional -X- _ O
token -X- _ O
to -X- _ O
the -X- _ O
end -X- _ O
so -X- _ O
that -X- _ O
representation -X- _ O
for -X- _ O
the -X- _ O
token -X- _ O
in -X- _ O
the -X- _ O
decoder -X- _ O
can -X- _ O
attend -X- _ O
to -X- _ O
decoder -X- _ O
states -X- _ O
from -X- _ O
the -X- _ O
complete -X- _ O
input -X- _ O
( -X- _ O
Figure -X- _ O
3a -X- _ O
) -X- _ O
. -X- _ O

The -X- _ O
representations -X- _ O
produced -X- _ O
by -X- _ O
BART -X- _ B-MethodName
can -X- _ O
be -X- _ O
used -X- _ O
in -X- _ O
several -X- _ O
ways -X- _ O
for -X- _ O
downstream -X- _ O
applications -X- _ O
. -X- _ O

Document -X- _ O
Rotation -X- _ O
A -X- _ O
token -X- _ O
is -X- _ O
chosen -X- _ O
uniformly -X- _ O
at -X- _ O
random -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
document -X- _ O
is -X- _ O
rotated -X- _ O
so -X- _ O
that -X- _ O
it -X- _ O
begins -X- _ O
with -X- _ O
that -X- _ O
token -X- _ O
. -X- _ O
This -X- _ O
task -X- _ O
trains -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
identify -X- _ O
the -X- _ O
start -X- _ O
of -X- _ O
the -X- _ O
document -X- _ O
. -X- _ O

Sentence -X- _ O
Permutation -X- _ O
A -X- _ O
document -X- _ O
is -X- _ O
divided -X- _ O
into -X- _ O
sentences -X- _ O
based -X- _ O
on -X- _ O
full -X- _ O
stops -X- _ O
, -X- _ O
and -X- _ O
these -X- _ O
sentences -X- _ O
are -X- _ O
shufﬂed -X- _ O
in -X- _ O
a -X- _ O
random -X- _ O
order -X- _ O
. -X- _ O

Text -X- _ O
Inﬁlling -X- _ O
A -X- _ O
number -X- _ O
of -X- _ O
text -X- _ O
spans -X- _ O
are -X- _ O
sampled -X- _ O
, -X- _ O
with -X- _ O
span -X- _ O
lengths -X- _ O
drawn -X- _ O
from -X- _ O
a -X- _ O
Poisson -X- _ O
distribution -X- _ O
( -X- _ O
λ -X- _ O
= -X- _ O
3 -X- _ O
) -X- _ O
. -X- _ O
Each -X- _ O
span -X- _ O
is -X- _ O
replaced -X- _ O
with -X- _ O
a -X- _ O
single -X- _ O
[ -X- _ O
MASK -X- _ O
] -X- _ O
token -X- _ O
. -X- _ O
0 -X- _ O
- -X- _ O
length -X- _ O
spans -X- _ O
correspond -X- _ O
to -X- _ O
the -X- _ O
insertion -X- _ O
of -X- _ O
[ -X- _ O
MASK -X- _ O
] -X- _ O
tokens -X- _ O
. -X- _ O
Text -X- _ O
inﬁlling -X- _ O
is -X- _ O
inspired -X- _ O
by -X- _ O
Span- -X- _ B-MethodName
BERT -X- _ I-MethodName
( -X- _ O
Joshi -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
but -X- _ O
SpanBERT -X- _ B-MethodName
samples -X- _ O
span -X- _ O
lengths -X- _ O
from -X- _ O
a -X- _ O
different -X- _ O
( -X- _ O
clamped -X- _ O
geometric -X- _ O
) -X- _ O
dis- -X- _ O
tribution -X- _ O
, -X- _ O
and -X- _ O
replaces -X- _ O
each -X- _ O
span -X- _ O
with -X- _ O
a -X- _ O
sequence -X- _ O
of -X- _ O
[ -X- _ O
MASK -X- _ O
] -X- _ O
tokens -X- _ O
of -X- _ O
exactly -X- _ O
the -X- _ O
same -X- _ O
length -X- _ O
. -X- _ O
Text -X- _ O
inﬁll- -X- _ O
ing -X- _ O
teaches -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
predict -X- _ O
how -X- _ O
many -X- _ O
tokens -X- _ O
are -X- _ O
missing -X- _ O
from -X- _ O
a -X- _ O
span -X- _ O
. -X- _ O

Token -X- _ O
Deletion -X- _ O
Random -X- _ O
tokens -X- _ O
are -X- _ O
deleted -X- _ O
from -X- _ O
the -X- _ O
input -X- _ O
. -X- _ O
In -X- _ O
contrast -X- _ O
to -X- _ O
token -X- _ O
masking -X- _ O
, -X- _ O
the -X- _ O
model -X- _ O
must -X- _ O
decide -X- _ O
which -X- _ O
positions -X- _ O
are -X- _ O
missing -X- _ O
inputs -X- _ O
. -X- _ O

2019 -X- _ O
) -X- _ O
, -X- _ O
random -X- _ O
tokens -X- _ O
are -X- _ O
sampled -X- _ O
and -X- _ O
replaced -X- _ O
with -X- _ O
[ -X- _ O
MASK -X- _ O
] -X- _ O
elements -X- _ O
. -X- _ O

BART -X- _ B-MethodName
is -X- _ O
trained -X- _ O
by -X- _ O
corrupting -X- _ O
documents -X- _ O
and -X- _ O
then -X- _ O
op- -X- _ O
timizing -X- _ O
a -X- _ O
reconstruction -X- _ O
loss -X- _ O
— -X- _ O
the -X- _ O
cross -X- _ B-MetricName
- -X- _ I-MetricName
entropy -X- _ I-MetricName
be- -X- _ O
tween -X- _ O
the -X- _ O
decoder -X- _ O
’s -X- _ O
output -X- _ O
and -X- _ O
the -X- _ O
original -X- _ O
document -X- _ O
. -X- _ O
Unlike -X- _ O
existing -X- _ O
denoising -X- _ O
autoencoders -X- _ O
, -X- _ O
which -X- _ O
are -X- _ O
tai- -X- _ O
lored -X- _ O
to -X- _ O
speciﬁc -X- _ O
noising -X- _ O
schemes -X- _ O
, -X- _ O
BART -X- _ B-MethodName
allows -X- _ O
us -X- _ O
to -X- _ O
apply -X- _ O
any -X- _ O
type -X- _ O
of -X- _ O
document -X- _ O
corruption -X- _ O
. -X- _ O
In -X- _ O
the -X- _ O
extreme -X- _ O
case -X- _ O
, -X- _ O
where -X- _ O
all -X- _ O
information -X- _ O
about -X- _ O
the -X- _ O
source -X- _ O
is -X- _ O
lost -X- _ O
, -X- _ O
BART -X- _ B-MethodName
is -X- _ O
equivalent -X- _ O
to -X- _ O
a -X- _ O
language -X- _ O
model -X- _ O
. -X- _ O
We -X- _ O
experiment -X- _ O
with -X- _ O
several -X- _ O
previously -X- _ O
proposed -X- _ O
and -X- _ O
novel -X- _ O
transformations -X- _ O
, -X- _ O
but -X- _ O
we -X- _ O
believe -X- _ O
there -X- _ O
is -X- _ O
a -X- _ O
sig- -X- _ O
niﬁcant -X- _ O
potential -X- _ O
for -X- _ O
development -X- _ O
of -X- _ O
other -X- _ O
new -X- _ O
alter- -X- _ O
natives -X- _ O
. -X- _ O
The -X- _ O
transformations -X- _ O
we -X- _ O
used -X- _ O
are -X- _ O
summarized -X- _ O
below -X- _ O
, -X- _ O
and -X- _ O
examples -X- _ O
are -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
2 -X- _ O
. -X- _ O

coder -X- _ O
, -X- _ O
and -X- _ O
for -X- _ O
our -X- _ O
large -X- _ O
model -X- _ O
we -X- _ O
use -X- _ O
12 -X- _ B-HyperparameterValue
layers -X- _ B-HyperparameterName
in -X- _ O
each -X- _ O
. -X- _ O
The -X- _ O
architecture -X- _ O
is -X- _ O
closely -X- _ O
related -X- _ O
to -X- _ O
that -X- _ O
used -X- _ O
in -X- _ O
BERT -X- _ B-MethodName
, -X- _ O
with -X- _ O
the -X- _ O
following -X- _ O
differences -X- _ O
: -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
each -X- _ O
layer -X- _ O
of -X- _ O
the -X- _ O
decoder -X- _ O
additionally -X- _ O
performs -X- _ O
cross -X- _ O
- -X- _ O
attention -X- _ O
over -X- _ O
the -X- _ O
ﬁnal -X- _ O
hidden -X- _ O
layer -X- _ O
of -X- _ O
the -X- _ O
encoder -X- _ O
( -X- _ O
as -X- _ O
in -X- _ O
the -X- _ O
trans- -X- _ O
former -X- _ O
sequence -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
sequence -X- _ O
model -X- _ O
) -X- _ O
; -X- _ O
and -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
BERT -X- _ B-MethodName
uses -X- _ O
an -X- _ O
additional -X- _ O
feed -X- _ O
- -X- _ O
forward -X- _ O
network -X- _ O
before -X- _ O
word- -X- _ O
prediction -X- _ O
, -X- _ O
which -X- _ O
BART -X- _ B-MethodName
does -X- _ O
not -X- _ O
. -X- _ O
In -X- _ O
total -X- _ O
, -X- _ O
BART -X- _ B-MethodName
con- -X- _ O
tains -X- _ O
roughly -X- _ O
10 -X- _ O
% -X- _ O
more -X- _ O
parameters -X- _ O
than -X- _ O
the -X- _ O
equiva- -X- _ O
lently -X- _ O
sized -X- _ O
BERT -X- _ B-MethodName
model -X- _ O
. -X- _ O

BART -X- _ B-MethodName
is -X- _ O
a -X- _ O
denoising -X- _ O
autoencoder -X- _ O
that -X- _ O
maps -X- _ O
a -X- _ O
corrupted -X- _ O
document -X- _ O
to -X- _ O
the -X- _ O
original -X- _ O
document -X- _ O
it -X- _ O
was -X- _ O
derived -X- _ O
from -X- _ O
. -X- _ O
It -X- _ O
is -X- _ O
implemented -X- _ O
as -X- _ O
a -X- _ O
sequence -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
sequence -X- _ O
model -X- _ O
with -X- _ O
a -X- _ O
bidirectional -X- _ O
encoder -X- _ O
over -X- _ O
corrupted -X- _ O
text -X- _ O
and -X- _ O
a -X- _ O
left -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
right -X- _ O
autoregressive -X- _ O
decoder -X- _ O
. -X- _ O
For -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
, -X- _ O
we -X- _ O
optimize -X- _ O
the -X- _ O
negative -X- _ B-MetricValue
log -X- _ I-MetricValue
likelihood -X- _ I-MetricValue
of -X- _ O
the -X- _ O
original -X- _ O
document -X- _ O
. -X- _ O

English -X- _ O
, -X- _ O
by -X- _ O
propagation -X- _ O
through -X- _ O
BART -X- _ B-MethodName
, -X- _ O
thereby -X- _ O
us- -X- _ O
ing -X- _ O
BART -X- _ B-MethodName
as -X- _ O
a -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
target -X- _ O
- -X- _ O
side -X- _ O
language -X- _ O
model -X- _ O
. -X- _ O
This -X- _ O
approach -X- _ O
improves -X- _ O
performance -X- _ O
over -X- _ O
a -X- _ O
strong -X- _ O
back -X- _ O
- -X- _ O
translation -X- _ O
MT -X- _ B-MethodName
baseline -X- _ O
by -X- _ O
1.1 -X- _ B-MetricValue
BLEU -X- _ B-MetricName
on -X- _ O
the -X- _ O
WMT -X- _ B-DatasetName
Romanian -X- _ I-DatasetName
- -X- _ I-DatasetName
English -X- _ I-DatasetName
benchmark -X- _ O
. -X- _ O
To -X- _ O
better -X- _ O
understand -X- _ O
these -X- _ O
effects -X- _ O
, -X- _ O
we -X- _ O
also -X- _ O
report -X- _ O
an -X- _ O
ablation -X- _ O
analysis -X- _ O
that -X- _ O
replicates -X- _ O
other -X- _ O
recently -X- _ O
pro- -X- _ O
posed -X- _ O
training -X- _ O
objectives -X- _ O
. -X- _ O
This -X- _ O
study -X- _ O
allows -X- _ O
us -X- _ O
to -X- _ O
care- -X- _ O
fully -X- _ O
control -X- _ O
for -X- _ O
a -X- _ O
number -X- _ O
of -X- _ O
factors -X- _ O
, -X- _ O
including -X- _ O
data -X- _ O
and -X- _ O
optimization -X- _ O
parameters -X- _ O
, -X- _ O
which -X- _ O
have -X- _ O
been -X- _ O
shown -X- _ O
to -X- _ O
be -X- _ O
as -X- _ O
important -X- _ O
for -X- _ O
overall -X- _ O
performance -X- _ O
as -X- _ O
the -X- _ O
se- -X- _ O
lection -X- _ O
of -X- _ O
training -X- _ O
objectives -X- _ O
( -X- _ O
Liu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
. -X- _ O
We -X- _ O
ﬁnd -X- _ O
that -X- _ O
BART -X- _ B-MethodName
exhibits -X- _ O
the -X- _ O
most -X- _ O
consistently -X- _ O
strong -X- _ O
perfor- -X- _ O
mance -X- _ O
across -X- _ O
the -X- _ O
full -X- _ O
range -X- _ O
of -X- _ O
tasks -X- _ O
we -X- _ O
consider -X- _ O
. -X- _ O

masked -X- _ O
tokens -X- _ O
are -X- _ O
predicted -X- _ O
( -X- _ O
Yang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
available -X- _ O
context -X- _ O
for -X- _ O
replacing -X- _ O
masked -X- _ O
tokens -X- _ O
( -X- _ O
Dong -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
. -X- _ O
However -X- _ O
, -X- _ O
these -X- _ O
methods -X- _ O
typically -X- _ O
focus -X- _ O
on -X- _ O
particular -X- _ O
types -X- _ O
of -X- _ O
end -X- _ O
tasks -X- _ O
( -X- _ O
e.g. -X- _ O
span -X- _ B-TaskName
prediction -X- _ I-TaskName
, -X- _ O
generation -X- _ B-TaskName
, -X- _ O
etc -X- _ O
. -X- _ O
) -X- _ O
, -X- _ O
limiting -X- _ O
their -X- _ O
applicability -X- _ O
. -X- _ O
In -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
present -X- _ O
BART -X- _ B-MethodName
, -X- _ O
which -X- _ O
pre -X- _ O
- -X- _ O
trains -X- _ O
a -X- _ O
model -X- _ O
combining -X- _ O
Bidirectional -X- _ B-MethodName
and -X- _ I-MethodName
Auto -X- _ I-MethodName
- -X- _ I-MethodName
Regressive -X- _ I-MethodName
Transformers -X- _ I-MethodName
. -X- _ O
BART -X- _ B-MethodName
is -X- _ O
a -X- _ O
denoising -X- _ O
autoencoder -X- _ O
built -X- _ O
with -X- _ O
a -X- _ O
sequence -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
sequence -X- _ O
model -X- _ O
that -X- _ O
is -X- _ O
applicable -X- _ O
to -X- _ O
a -X- _ O
very -X- _ O
wide -X- _ O
range -X- _ O
of -X- _ O
end -X- _ O
tasks -X- _ O
. -X- _ O
Pretraining -X- _ O
has -X- _ O
two -X- _ O
stages -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
text -X- _ O
is -X- _ O
corrupted -X- _ O
with -X- _ O
an -X- _ O
arbitrary -X- _ O
nois- -X- _ O
ing -X- _ O
function -X- _ O
, -X- _ O
and -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
a -X- _ O
sequence -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
sequence -X- _ O
model -X- _ O
is -X- _ O
learned -X- _ O
to -X- _ O
reconstruct -X- _ O
the -X- _ O
original -X- _ O
text -X- _ O
. -X- _ O
BART -X- _ B-MethodName
uses -X- _ O
a -X- _ O
standard -X- _ O
Tranformer -X- _ O
- -X- _ O
based -X- _ O
neural -X- _ O
machine -X- _ O
translation -X- _ O
architecture -X- _ O
which -X- _ O
, -X- _ O
despite -X- _ O
its -X- _ O
simplicity -X- _ O
, -X- _ O
can -X- _ O
be -X- _ O
seen -X- _ O
as -X- _ O
generalizing -X- _ O
BERT -X- _ B-MethodName
( -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
bidirectional -X- _ O
encoder -X- _ O
) -X- _ O
, -X- _ O
GPT -X- _ B-MethodName
( -X- _ O
with -X- _ O
the -X- _ O
left -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
right -X- _ O
decoder -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
many -X- _ O
other -X- _ O
more -X- _ O
recent -X- _ O
pretraining -X- _ O
schemes -X- _ O
( -X- _ O
see -X- _ O
Figure -X- _ O
1 -X- _ O
) -X- _ O
. -X- _ O
A -X- _ O
key -X- _ O
advantage -X- _ O
of -X- _ O
this -X- _ O
setup -X- _ O
is -X- _ O
the -X- _ O
noising -X- _ O
ﬂexibil- -X- _ O
ity -X- _ O
; -X- _ O
arbitrary -X- _ O
transformations -X- _ O
can -X- _ O
be -X- _ O
applied -X- _ O
to -X- _ O
the -X- _ O
orig- -X- _ O
inal -X- _ O
text -X- _ O
, -X- _ O
including -X- _ O
changing -X- _ O
its -X- _ O
length -X- _ O
. -X- _ O
We -X- _ O
evaluate -X- _ O
a -X- _ O
number -X- _ O
of -X- _ O
noising -X- _ O
approaches -X- _ O
, -X- _ O
ﬁnding -X- _ O
the -X- _ O
best -X- _ O
per- -X- _ O
formance -X- _ O
by -X- _ O
both -X- _ O
randomly -X- _ O
shufﬂing -X- _ O
the -X- _ O
order -X- _ O
of -X- _ O
the -X- _ O
original -X- _ O
sentences -X- _ O
and -X- _ O
using -X- _ O
a -X- _ O
novel -X- _ O
in-ﬁlling -X- _ O
scheme -X- _ O
, -X- _ O
where -X- _ O
arbitrary -X- _ O
length -X- _ O
spans -X- _ O
of -X- _ O
text -X- _ O
( -X- _ O
including -X- _ O
zero -X- _ O
length -X- _ O
) -X- _ O
are -X- _ O
replaced -X- _ O
with -X- _ O
a -X- _ O
single -X- _ O
mask -X- _ O
token -X- _ O
. -X- _ O
This -X- _ O
ap- -X- _ O
proach -X- _ O
generalizes -X- _ O
the -X- _ O
original -X- _ O
word -X- _ O
masking -X- _ O
and -X- _ O
next -X- _ O
sentence -X- _ O
prediction -X- _ O
objectives -X- _ O
in -X- _ O
BERT -X- _ B-MethodName
by -X- _ O
forcing -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
reason -X- _ O
more -X- _ O
about -X- _ O
overall -X- _ O
sentence -X- _ O
length -X- _ O
and -X- _ O
make -X- _ O
longer -X- _ O
range -X- _ O
transformations -X- _ O
to -X- _ O
the -X- _ O
input -X- _ O
. -X- _ O
BART -X- _ B-MethodName
is -X- _ O
particularly -X- _ O
effective -X- _ O
when -X- _ O
ﬁne -X- _ O
tuned -X- _ O
for -X- _ O
text -X- _ O
generation -X- _ O
but -X- _ O
also -X- _ O
works -X- _ O
well -X- _ O
for -X- _ O
comprehen- -X- _ O
sion -X- _ O
tasks -X- _ O
. -X- _ O
It -X- _ O
matches -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
RoBERTa -X- _ B-MethodName
( -X- _ O
Liu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
with -X- _ O
comparable -X- _ O
training -X- _ O
resources -X- _ O
on -X- _ O
GLUE -X- _ B-DatasetName
( -X- _ O
Wang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
and -X- _ O
SQuAD -X- _ B-DatasetName
( -X- _ O
Rajpurkar -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
achieves -X- _ O
new -X- _ O
state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
art -X- _ O
results -X- _ O
on -X- _ O
a -X- _ O
range -X- _ O
of -X- _ O
abstractive -X- _ O
dialogue -X- _ O
, -X- _ O
question -X- _ O
answer- -X- _ O
ing -X- _ O
, -X- _ O
and -X- _ O
summarization -X- _ O
tasks -X- _ O
. -X- _ O
For -X- _ O
example -X- _ O
, -X- _ O
it -X- _ O
im- -X- _ O
proves -X- _ O
performance -X- _ O
by -X- _ O
6 -X- _ B-MetricValue
ROUGE -X- _ B-MetricName
over -X- _ O
previous -X- _ O
work -X- _ O
on -X- _ O
XSum -X- _ B-DatasetName
( -X- _ O
Narayan -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
. -X- _ O
BART -X- _ B-MethodName
also -X- _ O
opens -X- _ O
up -X- _ O
new -X- _ O
ways -X- _ O
of -X- _ O
thinking -X- _ O
about -X- _ O
ﬁne -X- _ O
tuning -X- _ O
. -X- _ O
We -X- _ O
present -X- _ O
a -X- _ O
new -X- _ O
scheme -X- _ O
for -X- _ O
machine -X- _ B-TaskName
transla- -X- _ I-TaskName
tion -X- _ I-TaskName
where -X- _ O
a -X- _ O
BART -X- _ B-MethodName
model -X- _ O
is -X- _ O
stacked -X- _ O
above -X- _ O
a -X- _ O
few -X- _ O
ad- -X- _ O
ditional -X- _ O
transformer -X- _ O
layers -X- _ O
. -X- _ O
These -X- _ O
layers -X- _ O
are -X- _ O
trained -X- _ O
to -X- _ O
essentially -X- _ O
translate -X- _ O
the -X- _ O
foreign -X- _ O
language -X- _ O
to -X- _ O
noised -X- _ O

Self -X- _ O
- -X- _ O
supervised -X- _ O
methods -X- _ O
have -X- _ O
achieved -X- _ O
remarkable -X- _ O
success -X- _ O
in -X- _ O
a -X- _ O
wide -X- _ O
range -X- _ O
of -X- _ O
NLP -X- _ O
tasks -X- _ O
( -X- _ O
Mikolov -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2013 -X- _ O
; -X- _ O
Peters -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2018 -X- _ O
; -X- _ O
Devlin -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Joshi -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Yang -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
; -X- _ O
Liu -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
. -X- _ O
The -X- _ O
most -X- _ O
successful -X- _ O
approaches -X- _ O
have -X- _ O
been -X- _ O
variants -X- _ O
of -X- _ O
masked -X- _ O
language -X- _ O
models -X- _ O
, -X- _ O
which -X- _ O
are -X- _ O
denoising -X- _ O
autoen- -X- _ O
coders -X- _ O
that -X- _ O
are -X- _ O
trained -X- _ O
to -X- _ O
reconstruct -X- _ O
text -X- _ O
where -X- _ O
a -X- _ O
ran- -X- _ O
dom -X- _ O
subset -X- _ O
of -X- _ O
the -X- _ O
words -X- _ O
has -X- _ O
been -X- _ O
masked -X- _ O
out -X- _ O
. -X- _ O
Recent -X- _ O
work -X- _ O
has -X- _ O
shown -X- _ O
gains -X- _ O
by -X- _ O
improving -X- _ O
the -X- _ O
distribution -X- _ O
of -X- _ O
masked -X- _ O
tokens -X- _ O
( -X- _ O
Joshi -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
the -X- _ O
order -X- _ O
in -X- _ O
which -X- _ O

We -X- _ O
present -X- _ O
BART -X- _ B-MethodName
, -X- _ O
a -X- _ O
denoising -X- _ O
autoencoder -X- _ O
for -X- _ O
pretraining -X- _ O
sequence -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
sequence -X- _ O
models -X- _ O
. -X- _ O
BART -X- _ B-MethodName
is -X- _ O
trained -X- _ O
by -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
corrupting -X- _ O
text -X- _ O
with -X- _ O
an -X- _ O
arbitrary -X- _ O
noising -X- _ O
function -X- _ O
, -X- _ O
and -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
learning -X- _ O
a -X- _ O
model -X- _ O
to -X- _ O
reconstruct -X- _ O
the -X- _ O
original -X- _ O
text -X- _ O
. -X- _ O
It -X- _ O
uses -X- _ O
a -X- _ O
standard -X- _ O
Tranformer -X- _ O
- -X- _ O
based -X- _ O
neural -X- _ O
machine -X- _ O
translation -X- _ O
architecture -X- _ O
which -X- _ O
, -X- _ O
despite -X- _ O
its -X- _ O
sim- -X- _ O
plicity -X- _ O
, -X- _ O
can -X- _ O
be -X- _ O
seen -X- _ O
as -X- _ O
generalizing -X- _ O
BERT -X- _ B-MethodName
( -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
bidirectional -X- _ O
encoder -X- _ O
) -X- _ O
, -X- _ O
GPT -X- _ B-MethodName
( -X- _ O
with -X- _ O
the -X- _ O
left -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
right -X- _ O
decoder -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
many -X- _ O
other -X- _ O
more -X- _ O
re- -X- _ O
cent -X- _ O
pretraining -X- _ O
schemes -X- _ O
. -X- _ O
We -X- _ O
evaluate -X- _ O
a -X- _ O
num- -X- _ O
ber -X- _ O
of -X- _ O
noising -X- _ O
approaches -X- _ O
, -X- _ O
ﬁnding -X- _ O
the -X- _ O
best -X- _ O
per- -X- _ O
formance -X- _ O
by -X- _ O
both -X- _ O
randomly -X- _ O
shufﬂing -X- _ O
the -X- _ O
or- -X- _ O
der -X- _ O
of -X- _ O
the -X- _ O
original -X- _ O
sentences -X- _ O
and -X- _ O
using -X- _ O
a -X- _ O
novel -X- _ O
in-ﬁlling -X- _ O
scheme -X- _ O
, -X- _ O
where -X- _ O
spans -X- _ O
of -X- _ O
text -X- _ O
are -X- _ O
re- -X- _ O
placed -X- _ O
with -X- _ O
a -X- _ O
single -X- _ O
mask -X- _ O
token -X- _ O
. -X- _ O
BART -X- _ B-MethodName
is -X- _ O
particularly -X- _ O
effective -X- _ O
when -X- _ O
ﬁne -X- _ O
tuned -X- _ O
for -X- _ O
text -X- _ O
generation -X- _ O
but -X- _ O
also -X- _ O
works -X- _ O
well -X- _ O
for -X- _ O
compre- -X- _ O
hension -X- _ O
tasks -X- _ O
. -X- _ O
It -X- _ O
matches -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
RoBERTa -X- _ B-MethodName
with -X- _ O
comparable -X- _ O
training -X- _ O
resources -X- _ O
on -X- _ O
GLUE -X- _ B-DatasetName
and -X- _ O
SQuAD -X- _ B-DatasetName
, -X- _ O
achieves -X- _ O
new -X- _ O
state- -X- _ O
of -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
art -X- _ O
results -X- _ O
on -X- _ O
a -X- _ O
range -X- _ O
of -X- _ O
abstractive -X- _ O
di- -X- _ O
alogue -X- _ O
, -X- _ O
question -X- _ O
answering -X- _ O
, -X- _ O
and -X- _ O
summariza- -X- _ O
tion -X- _ O
tasks -X- _ O
, -X- _ O
with -X- _ O
gains -X- _ O
of -X- _ O
up -X- _ O
to -X- _ O
6 -X- _ B-MetricValue
ROUGE -X- _ B-MetricName
. -X- _ O
BART -X- _ B-MethodName
also -X- _ O
provides -X- _ O
a -X- _ O
1.1 -X- _ B-MetricValue
BLEU -X- _ B-MetricName
increase -X- _ O
over -X- _ O
a -X- _ O
back -X- _ O
- -X- _ O
translation -X- _ O
system -X- _ O
for -X- _ O
machine -X- _ B-TaskName
transla- -X- _ I-TaskName
tion -X- _ I-TaskName
, -X- _ O
with -X- _ O
only -X- _ O
target -X- _ O
language -X- _ O
pretraining -X- _ O
. -X- _ O
We -X- _ O
also -X- _ O
report -X- _ O
ablation -X- _ O
experiments -X- _ O
that -X- _ O
replicate -X- _ O
other -X- _ O
pretraining -X- _ O
schemes -X- _ O
within -X- _ O
the -X- _ O
BART -X- _ B-MethodName
framework -X- _ O
, -X- _ O
to -X- _ O
better -X- _ O
measure -X- _ O
which -X- _ O
factors -X- _ O
most -X- _ O
inﬂuence -X- _ O
end -X- _ O
- -X- _ O
task -X- _ O
performance -X- _ O
. -X- _ O

Mike -X- _ O
Lewis -X- _ O
* -X- _ O
, -X- _ O
Yinhan -X- _ O
Liu -X- _ O
* -X- _ O
, -X- _ O
Naman -X- _ O
Goyal -X- _ O
* -X- _ O
, -X- _ O
Marjan -X- _ O
Ghazvininejad -X- _ O
, -X- _ O
Abdelrahman -X- _ O
Mohamed -X- _ O
, -X- _ O
Omer -X- _ O
Levy -X- _ O
, -X- _ O
Ves -X- _ O
Stoyanov -X- _ O
, -X- _ O
Luke -X- _ O
Zettlemoyer -X- _ O
Facebook -X- _ O
AI -X- _ O
{ -X- _ O
mikelewis,yinhanliu,naman}@fb.com -X- _ O

BART -X- _ B-MethodName
: -X- _ O
Denoising -X- _ O
Sequence -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
Sequence -X- _ O
Pre -X- _ O
- -X- _ O
training -X- _ O
for -X- _ O
Natural -X- _ B-TaskName
Language -X- _ I-TaskName
Generation -X- _ I-TaskName
, -X- _ O
Translation -X- _ B-TaskName
, -X- _ O
and -X- _ O
Comprehension -X- _ B-TaskName

