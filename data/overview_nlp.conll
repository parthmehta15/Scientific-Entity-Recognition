-DOCSTART- -X- O
Despite -X- _ O
the -X- _ O
excellent -X- _ O
performances -X- _ O
of -X- _ O
the -X- _ O
Transformer -X- _ O
ar- -X- _ O
chitecture -X- _ O
, -X- _ O
new -X- _ O
layers -X- _ O
aiming -X- _ O
to -X- _ O
improve -X- _ O
the -X- _ O
performance -X- _ O
and -X- _ O
the -X- _ O
complexity -X- _ O
have -X- _ O
been -X- _ O
released -X- _ O
. -X- _ O
The -X- _ O
Transformer -X- _ O
uses -X- _ O
a -X- _ O
gradient -X- _ O
- -X- _ O
based -X- _ O
optimization -X- _ O
proce- -X- _ O
dure -X- _ O
. -X- _ O
Thus -X- _ O
, -X- _ O
it -X- _ O
needs -X- _ O
to -X- _ O
save -X- _ O
the -X- _ O
activation -X- _ O
value -X- _ O
of -X- _ O
all -X- _ O
the -X- _ O
neurons -X- _ O
to -X- _ O
be -X- _ O
used -X- _ O
during -X- _ O
the -X- _ O
back -X- _ O
- -X- _ O
propagation -X- _ O
. -X- _ O
Because -X- _ O
of -X- _ O
the -X- _ O
massive -X- _ O
size -X- _ O
of -X- _ O
the -X- _ O
Transformer -X- _ O
models -X- _ O
, -X- _ O
the -X- _ O
GPU -X- _ B-MethodName
/ -X- _ O
TPU -X- _ B-MethodName
’s -X- _ O
memory -X- _ O
is -X- _ O
rapidly -X- _ O
saturated -X- _ O
. -X- _ O
The -X- _ O
Reformer -X- _ O
[ -X- _ O
41 -X- _ O
] -X- _ O
counter -X- _ O
the -X- _ O
memory -X- _ O
problem -X- _ O
of -X- _ O
the -X- _ O
Transformer -X- _ O
by -X- _ O
recomputing -X- _ O
the -X- _ O
input -X- _ O
of -X- _ O
each -X- _ O
layer -X- _ O
during -X- _ O
the -X- _ O
back -X- _ O
- -X- _ O
propagation -X- _ O
instead -X- _ O
of -X- _ O
storing -X- _ O
the -X- _ O
information -X- _ O
. -X- _ O
The -X- _ O
Reformer -X- _ O
can -X- _ O
also -X- _ O
reduce -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
operations -X- _ O
during -X- _ O
the -X- _ O
forward -X- _ O
pass -X- _ O
by -X- _ O
computing -X- _ O
a -X- _ O
hash -X- _ O
function -X- _ O
that -X- _ O
pairs -X- _ O
similar -X- _ O
inputs -X- _ O
together -X- _ O
. -X- _ O
Like -X- _ O
that -X- _ O
, -X- _ O
it -X- _ O
does -X- _ O
not -X- _ O
compute -X- _ O
all -X- _ O
pairs -X- _ O
of -X- _ O
vectors -X- _ O
to -X- _ O
ﬁnd -X- _ O
the -X- _ O
related -X- _ O
ones -X- _ O
. -X- _ O
Therefore -X- _ O
, -X- _ O
it -X- _ O
increases -X- _ O
the -X- _ O
size -X- _ O
of -X- _ O
the -X- _ O
text -X- _ O
it -X- _ O
can -X- _ O
treat -X- _ O
at -X- _ O
once -X- _ O
. -X- _ O
Another -X- _ O
way -X- _ O
to -X- _ O
improve -X- _ O
the -X- _ O
architecture -X- _ O
of -X- _ O
a -X- _ O
network -X- _ O
is -X- _ O
by -X- _ O
using -X- _ O
an -X- _ O
evolving -X- _ O
algorithm -X- _ O
as -X- _ O
proposed -X- _ O
by -X- _ O
[ -X- _ O
42 -X- _ O
] -X- _ O
. -X- _ O
To -X- _ O
create -X- _ O
a -X- _ O
new -X- _ O
architecture -X- _ O
designed -X- _ O
automatically -X- _ O
, -X- _ O
they -X- _ O
evolve -X- _ O
a -X- _ O
population -X- _ O
of -X- _ O
Transformers -X- _ O
based -X- _ O
on -X- _ O
their -X- _ O
accuracy -X- _ O
. -X- _ O
Using -X- _ O
the -X- _ O
Progressive -X- _ O
Dynamic -X- _ O
Hurdles -X- _ O
( -X- _ O
PDH -X- _ O
) -X- _ O
, -X- _ O
they -X- _ O
could -X- _ O
reduce -X- _ O
the -X- _ O
search -X- _ O
space -X- _ O
and -X- _ O
the -X- _ O
training -X- _ O
time -X- _ O
. -X- _ O
With -X- _ O
this -X- _ O
technique -X- _ O
and -X- _ O
an -X- _ O
extensive -X- _ O
amount -X- _ O
of -X- _ O
computational -X- _ O
power -X- _ O
( -X- _ O
around -X- _ O
200 -X- _ O
TPUs -X- _ O
) -X- _ O
, -X- _ O
they -X- _ O
could -X- _ O
ﬁnd -X- _ O
a -X- _ O
new -X- _ O
architecture -X- _ O
that -X- _ O
outperforms -X- _ O
the -X- _ O
previous -X- _ O
one -X- _ O
. -X- _ O

Authorized -X- _ O
licensed -X- _ O
use -X- _ O
limited -X- _ O
to -X- _ O
: -X- _ O
Carnegie -X- _ O
Mellon -X- _ O
Libraries -X- _ O
. -X- _ O
Downloaded -X- _ O
on -X- _ O
October -X- _ O
18,2022 -X- _ O
at -X- _ O
00:17:46 -X- _ O
UTC -X- _ O
from -X- _ O
IEEE -X- _ O
Xplore -X- _ O
. -X- _ O
Restrictions -X- _ O
apply -X- _ O
. -X- _ O

ANTHONY -X- _ O
GILLIOZ -X- _ O
ET -X- _ O
AL -X- _ O
. -X- _ O
: -X- _ O
OVERVIEW -X- _ O
OF -X- _ O
THE -X- _ O
TRANSFORMER -X- _ O
- -X- _ O
BASED -X- _ O
MODELS -X- _ O
FOR -X- _ O
NLP -X- _ O
TASKS -X- _ O
183 -X- _ O

The -X- _ O
Transformer -X- _ O
- -X- _ O
based -X- _ O
networks -X- _ O
have -X- _ O
pushed -X- _ O
the -X- _ O
reasoning -X- _ O
- -X- _ O
skills -X- _ O
to -X- _ O
human -X- _ O
- -X- _ O
level -X- _ O
abilities -X- _ O
. -X- _ O
It -X- _ O
can -X- _ O
even -X- _ O
excel -X- _ O
the -X- _ O
human -X- _ O
capabilities -X- _ O
on -X- _ O
a -X- _ O
few -X- _ O
tasks -X- _ O
of -X- _ O
GLUE -X- _ B-TaskName
. -X- _ O
Transformer- -X- _ O
based -X- _ O
networks -X- _ O
have -X- _ O
changed -X- _ O
the -X- _ O
face -X- _ O
of -X- _ O
NLP -X- _ B-TaskName
tasks -X- _ O
. -X- _ O
They -X- _ O
can -X- _ O
go -X- _ O
far -X- _ O
beyond -X- _ O
the -X- _ O
results -X- _ O
obtained -X- _ O
with -X- _ O
RNNs -X- _ B-MethodName
, -X- _ O
and -X- _ O
they -X- _ O
can -X- _ O
do -X- _ O
it -X- _ O
faster -X- _ O
. -X- _ O
They -X- _ O
have -X- _ O
helped -X- _ O
solve -X- _ O
many -X- _ O
problems -X- _ O
at -X- _ O
the -X- _ O
same -X- _ O
time -X- _ O
by -X- _ O
providing -X- _ O
a -X- _ O
direct -X- _ O
and -X- _ O
efﬁcient -X- _ O
way -X- _ O
to -X- _ O
combine -X- _ O
several -X- _ O
downstream -X- _ O
tasks -X- _ O
. -X- _ O
Nevertheless -X- _ O
, -X- _ O
much -X- _ O
work -X- _ O
remains -X- _ O
before -X- _ O
having -X- _ O
a -X- _ O
system -X- _ O
with -X- _ O
a -X- _ O
human -X- _ O
- -X- _ O
level -X- _ O
comprehension -X- _ O
of -X- _ O
the -X- _ O
underlying -X- _ O
meaning -X- _ O
of -X- _ O
texts -X- _ O
, -X- _ O
that -X- _ O
is -X- _ O
also -X- _ O
sufﬁciently -X- _ O
small -X- _ O
to -X- _ O
run -X- _ O
on -X- _ O
devices -X- _ O
with -X- _ O
low -X- _ O
computational -X- _ O
power -X- _ O
. -X- _ O

Authorized -X- _ O
licensed -X- _ O
use -X- _ O
limited -X- _ O
to -X- _ O
: -X- _ O
Carnegie -X- _ O
Mellon -X- _ O
Libraries -X- _ O
. -X- _ O
Downloaded -X- _ O
on -X- _ O
October -X- _ O
18,2022 -X- _ O
at -X- _ O
00:17:46 -X- _ O
UTC -X- _ O
from -X- _ O
IEEE -X- _ O
Xplore -X- _ O
. -X- _ O
Restrictions -X- _ O
apply -X- _ O
. -X- _ O

XLM -X- _ O
[ -X- _ O
40 -X- _ O
] -X- _ O
aims -X- _ O
to -X- _ O
build -X- _ O
a -X- _ O
universal -X- _ O
cross -X- _ O
- -X- _ O
language -X- _ O
sentence -X- _ O
embedding -X- _ O
. -X- _ O
The -X- _ O
goal -X- _ O
is -X- _ O
to -X- _ O
align -X- _ O
sentence -X- _ O
representations -X- _ O
to -X- _ O
improve -X- _ O
the -X- _ O
translation -X- _ O
between -X- _ O
languages -X- _ O
. -X- _ O
To -X- _ O
do -X- _ O
so -X- _ O
, -X- _ O
a -X- _ O
Transformer -X- _ O
architecture -X- _ O
with -X- _ O
two -X- _ O
unsupervised -X- _ O
tasks -X- _ O
and -X- _ O
one -X- _ O
supervised -X- _ O
is -X- _ O
used -X- _ O
. -X- _ O
The -X- _ O
effectiveness -X- _ O
of -X- _ O
cross -X- _ O
- -X- _ O
language -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
in -X- _ O
order -X- _ O
to -X- _ O
improve -X- _ O
the -X- _ O
multilingual -X- _ B-TaskName
machine -X- _ I-TaskName
translation -X- _ I-TaskName
is -X- _ O
shown -X- _ O
. -X- _ O

In -X- _ O
order -X- _ O
to -X- _ O
tackle -X- _ O
speciﬁc -X- _ O
languages -X- _ O
problems -X- _ O
, -X- _ O
different -X- _ O
monolingual -X- _ O
versions -X- _ O
of -X- _ O
BERT -X- _ B-MethodName
were -X- _ O
trained -X- _ O
in -X- _ O
different -X- _ O
languages -X- _ O
. -X- _ O
For -X- _ O
example -X- _ O
BERTje -X- _ B-MethodName
[ -X- _ O
36 -X- _ O
] -X- _ O
is -X- _ O
a -X- _ O
Dutch -X- _ O
version -X- _ O
, -X- _ O
AlBERTo -X- _ B-MethodName
[ -X- _ O
37 -X- _ O
] -X- _ O
is -X- _ O
an -X- _ O
Italian -X- _ O
version -X- _ O
, -X- _ O
and -X- _ O
CamemBERT -X- _ B-MethodName
[ -X- _ O
38 -X- _ O
] -X- _ O
and -X- _ O
FlauBERT -X- _ B-MethodName
[ -X- _ O
39 -X- _ O
] -X- _ O
are -X- _ O
two -X- _ O
different -X- _ O
models -X- _ O
for -X- _ O
French -X- _ O
. -X- _ O
These -X- _ O
models -X- _ O
outperform -X- _ O
vanilla -X- _ O
BERT -X- _ B-MethodName
in -X- _ O
different -X- _ O
NLP -X- _ B-TaskName
tasks -X- _ O
speciﬁc -X- _ O
to -X- _ O
these -X- _ O
languages -X- _ O
. -X- _ O

word -X- _ O
has -X- _ O
a -X- _ O
capitalized -X- _ O
ﬁrst -X- _ O
letter -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
Token -X- _ B-TaskName
- -X- _ I-TaskName
Document -X- _ I-TaskName
Relation -X- _ I-TaskName
Prediction -X- _ I-TaskName
Task -X- _ I-TaskName
( -X- _ O
i.e. -X- _ O
it -X- _ O
predicts -X- _ O
if -X- _ O
a -X- _ O
token -X- _ O
of -X- _ O
a -X- _ O
sentence -X- _ O
belongs -X- _ O
to -X- _ O
a -X- _ O
document -X- _ O
where -X- _ O
the -X- _ O
sentence -X- _ O
initially -X- _ O
appears -X- _ O
) -X- _ O
. -X- _ O
Structure -X- _ O
- -X- _ O
Aware -X- _ O
Tasks -X- _ O
: -X- _ O
It -X- _ O
learns -X- _ O
the -X- _ O
relationship -X- _ O
between -X- _ O
sentences -X- _ O
: -X- _ O
sentence -X- _ B-TaskName
reordering -X- _ I-TaskName
task -X- _ I-TaskName
( -X- _ O
i.e. -X- _ O
split -X- _ O
and -X- _ O
shufﬂe -X- _ O
a -X- _ O
sentence -X- _ O
and -X- _ O
must -X- _ O
ﬁnd -X- _ O
the -X- _ O
correct -X- _ O
order -X- _ O
) -X- _ O
, -X- _ O
sentence -X- _ B-TaskName
distance -X- _ I-TaskName
task -X- _ I-TaskName
( -X- _ O
i.e. -X- _ O
it -X- _ O
must -X- _ O
ﬁnd -X- _ O
if -X- _ O
two -X- _ O
sentences -X- _ O
are -X- _ O
adjacent -X- _ O
, -X- _ O
belong -X- _ O
to -X- _ O
the -X- _ O
same -X- _ O
document -X- _ O
or -X- _ O
if -X- _ O
they -X- _ O
are -X- _ O
entirely -X- _ O
unrelated -X- _ O
) -X- _ O
. -X- _ O
Semantic -X- _ O
- -X- _ O
Aware -X- _ O
Tasks -X- _ O
: -X- _ O
It -X- _ O
learns -X- _ O
a -X- _ O
higher -X- _ O
order -X- _ O
of -X- _ O
knowl- -X- _ O
edge -X- _ O
: -X- _ O
discourse -X- _ B-TaskName
relation -X- _ I-TaskName
task -X- _ I-TaskName
( -X- _ O
i.e. -X- _ O
it -X- _ O
predicts -X- _ O
the -X- _ O
semantic -X- _ O
or -X- _ O
rhetorical -X- _ O
relation -X- _ O
of -X- _ O
sentences -X- _ O
) -X- _ O
, -X- _ O
IR -X- _ B-TaskName
relevance -X- _ I-TaskName
task -X- _ I-TaskName
( -X- _ O
i.e. -X- _ O
ﬁnd -X- _ O
the -X- _ O
relevance -X- _ O
of -X- _ O
information -X- _ O
retrieval -X- _ O
in -X- _ O
texts -X- _ O
) -X- _ O
. -X- _ O

BERT -X- _ B-MethodName
learns -X- _ O
several -X- _ O
tasks -X- _ O
sequentially -X- _ O
and -X- _ O
increases -X- _ O
the -X- _ O
overall -X- _ O
performance -X- _ O
of -X- _ O
the -X- _ O
downstream -X- _ O
end -X- _ O
- -X- _ O
tasks -X- _ O
. -X- _ O
The -X- _ O
main -X- _ O
is- -X- _ O
sue -X- _ O
with -X- _ O
the -X- _ O
continual -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
method -X- _ O
is -X- _ O
that -X- _ O
it -X- _ O
must -X- _ O
learn -X- _ O
efﬁciently -X- _ O
and -X- _ O
quickly -X- _ O
newly -X- _ O
introduced -X- _ O
sub -X- _ O
- -X- _ O
tasks -X- _ O
, -X- _ O
and -X- _ O
it -X- _ O
must -X- _ O
remember -X- _ O
what -X- _ O
has -X- _ O
been -X- _ O
learned -X- _ O
previously -X- _ O
. -X- _ O
The -X- _ O
Multi -X- _ B-TaskName
- -X- _ I-TaskName
task -X- _ I-TaskName
Learning -X- _ I-TaskName
( -X- _ O
MTL -X- _ B-TaskName
) -X- _ O
principle -X- _ O
is -X- _ O
based -X- _ O
on -X- _ O
human -X- _ O
consideration -X- _ O
. -X- _ O
If -X- _ O
you -X- _ O
learn -X- _ O
how -X- _ O
to -X- _ O
do -X- _ O
a -X- _ O
ﬁrst -X- _ O
task -X- _ O
, -X- _ O
then -X- _ O
a -X- _ O
second -X- _ O
related -X- _ O
task -X- _ O
is -X- _ O
going -X- _ O
to -X- _ O
be -X- _ O
more -X- _ O
accessible -X- _ O
to -X- _ O
master -X- _ O
. -X- _ O
There -X- _ O
are -X- _ O
two -X- _ O
main -X- _ O
trends -X- _ O
in -X- _ O
MTL -X- _ O
. -X- _ O
The -X- _ O
ﬁrst -X- _ O
one -X- _ O
uses -X- _ O
an -X- _ O
MTL -X- _ O
scheme -X- _ O
during -X- _ O
the -X- _ O
ﬁne -X- _ O
- -X- _ O
tuning -X- _ O
phase -X- _ O
. -X- _ O
MT -X- _ O
- -X- _ O
DNN -X- _ O
[ -X- _ O
35 -X- _ O
] -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
backbone -X- _ O
of -X- _ O
BERT -X- _ B-MethodName
is -X- _ O
using -X- _ O
the -X- _ O
same -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
procedure -X- _ O
, -X- _ O
but -X- _ O
during -X- _ O
the -X- _ O
ﬁne -X- _ O
- -X- _ O
tuning -X- _ O
step -X- _ O
, -X- _ O
it -X- _ O
uses -X- _ O
four -X- _ O
multi -X- _ O
- -X- _ O
tasks -X- _ O
. -X- _ O
Training -X- _ O
on -X- _ O
all -X- _ O
the -X- _ O
GLUE -X- _ B-MetricName
tasks -X- _ O
at -X- _ O
the -X- _ O
same -X- _ O
time -X- _ O
makes -X- _ O
it -X- _ O
gain -X- _ O
an -X- _ O
efﬁcient -X- _ O
generalization -X- _ O
ability -X- _ O
. -X- _ O
On -X- _ O
the -X- _ O
opposite -X- _ O
[ -X- _ O
10 -X- _ O
] -X- _ O
proposes -X- _ O
an -X- _ O
MTL -X- _ O
process -X- _ O
directly -X- _ O
during -X- _ O
the -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
step -X- _ O
; -X- _ O
ERNIE -X- _ O
2.0 -X- _ O
introduces -X- _ O
a -X- _ O
continual -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
framework -X- _ O
. -X- _ O
More -X- _ O
speciﬁcally -X- _ O
, -X- _ O
it -X- _ O
uses -X- _ O
a -X- _ O
Sequential -X- _ O
Multi -X- _ O
- -X- _ O
task -X- _ O
Learning -X- _ O
where -X- _ O
it -X- _ O
begins -X- _ O
to -X- _ O
learn -X- _ O
a -X- _ O
ﬁrst -X- _ O
task -X- _ O
. -X- _ O
When -X- _ O
this -X- _ O
ﬁrst -X- _ O
task -X- _ O
is -X- _ O
mastered -X- _ O
, -X- _ O
a -X- _ O
new -X- _ O
task -X- _ O
is -X- _ O
introduced -X- _ O
in -X- _ O
the -X- _ O
continual -X- _ O
learning -X- _ O
process -X- _ O
. -X- _ O
The -X- _ O
previously -X- _ O
optimized -X- _ O
parameters -X- _ O
are -X- _ O
used -X- _ O
to -X- _ O
initiate -X- _ O
the -X- _ O
model -X- _ O
, -X- _ O
the -X- _ O
new -X- _ O
task -X- _ O
and -X- _ O
the -X- _ O
previous -X- _ O
tasks -X- _ O
are -X- _ O
trained -X- _ O
concurrently -X- _ O
. -X- _ O
There -X- _ O
are -X- _ O
three -X- _ O
groups -X- _ O
of -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
tasks -X- _ O
, -X- _ O
and -X- _ O
each -X- _ O
of -X- _ O
them -X- _ O
aims -X- _ O
to -X- _ O
capture -X- _ O
a -X- _ O
different -X- _ O
level -X- _ O
of -X- _ O
semantic -X- _ O
: -X- _ O
Word -X- _ O
- -X- _ O
Aware -X- _ O
Tasks -X- _ O
: -X- _ O
It -X- _ O
captures -X- _ O
the -X- _ O
lexical -X- _ O
information -X- _ O
of -X- _ O
the -X- _ O
text -X- _ O
: -X- _ O
the -X- _ O
Knowledge -X- _ B-TaskName
Masking -X- _ I-TaskName
Task -X- _ I-TaskName
( -X- _ O
i.e. -X- _ O
it -X- _ O
masks -X- _ O
phrases -X- _ O
and -X- _ O
entities -X- _ O
) -X- _ O
, -X- _ O
the -X- _ O
Capitalization -X- _ B-TaskName
prediction -X- _ I-TaskName
( -X- _ O
i.e. -X- _ O
it -X- _ O
predicts -X- _ O
if -X- _ O
a -X- _ O

It -X- _ O
is -X- _ O
then -X- _ O
necessary -X- _ O
to -X- _ O
ﬁnd -X- _ O
smaller -X- _ O
systems -X- _ O
that -X- _ O
maintain -X- _ O
the -X- _ O
high -X- _ O
performances -X- _ O
of -X- _ O
the -X- _ O
bigger -X- _ O
ones -X- _ O
. -X- _ O
Working -X- _ O
with -X- _ O
smaller -X- _ O
models -X- _ O
has -X- _ O
multiple -X- _ O
advantages -X- _ O
. -X- _ O
If -X- _ O
the -X- _ O
model -X- _ O
size -X- _ O
is -X- _ O
shrunk -X- _ O
, -X- _ O
it -X- _ O
trains -X- _ O
faster -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
inference -X- _ O
time -X- _ O
will -X- _ O
also -X- _ O
be -X- _ O
reduced -X- _ O
. -X- _ O
If -X- _ O
it -X- _ O
is -X- _ O
small -X- _ O
enough -X- _ O
, -X- _ O
it -X- _ O
can -X- _ O
be -X- _ O
run -X- _ O
on -X- _ O
smartphones -X- _ O
or -X- _ O
IoT -X- _ O
devices -X- _ O
in -X- _ O
real -X- _ O
- -X- _ O
time -X- _ O
. -X- _ O
One -X- _ O
technique -X- _ O
introduced -X- _ O
to -X- _ O
reduce -X- _ O
the -X- _ O
size -X- _ O
of -X- _ O
those -X- _ O
big -X- _ O
networks -X- _ O
is -X- _ O
the -X- _ O
knowledge -X- _ O
distillation -X- _ O
. -X- _ O
It -X- _ O
is -X- _ O
a -X- _ O
compression -X- _ O
method -X- _ O
that -X- _ O
consists -X- _ O
of -X- _ O
a -X- _ O
small -X- _ O
network -X- _ O
( -X- _ O
student -X- _ O
) -X- _ O
trained -X- _ O
to -X- _ O
reproduce -X- _ O
the -X- _ O
behaviour -X- _ O
of -X- _ O
a -X- _ O
bigger -X- _ O
version -X- _ O
of -X- _ O
itself -X- _ O
( -X- _ O
teacher -X- _ O
) -X- _ O
. -X- _ O
The -X- _ O
teacher -X- _ O
is -X- _ O
primarily -X- _ O
trained -X- _ O
as -X- _ O
a -X- _ O
regular -X- _ O
network -X- _ O
, -X- _ O
and -X- _ O
after -X- _ O
that -X- _ O
, -X- _ O
it -X- _ O
is -X- _ O
distilled -X- _ O
to -X- _ O
reduce -X- _ O
its -X- _ O
size -X- _ O
. -X- _ O
DistilBERT -X- _ B-MethodName
[ -X- _ O
33 -X- _ O
] -X- _ O
is -X- _ O
a -X- _ O
distilled -X- _ O
version -X- _ O
of -X- _ O
BERT -X- _ B-MethodName
that -X- _ O
reduces -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
layers -X- _ O
by -X- _ O
a -X- _ O
factor -X- _ O
of -X- _ O
2 -X- _ O
. -X- _ O
It -X- _ O
retains -X- _ O
97 -X- _ O
% -X- _ O
of -X- _ O
BERT -X- _ B-MethodName
on -X- _ O
the -X- _ O
GLUE -X- _ B-MetricName
benchmark -X- _ O
while -X- _ O
being -X- _ O
40 -X- _ O
% -X- _ O
smaller -X- _ O
and -X- _ O
60 -X- _ O
% -X- _ O
faster -X- _ O
at -X- _ O
the -X- _ O
inference -X- _ O
time -X- _ O
. -X- _ O
Another -X- _ O
way -X- _ O
to -X- _ O
reduce -X- _ O
the -X- _ O
size -X- _ O
of -X- _ O
BERT -X- _ O
is -X- _ O
by -X- _ O
changing -X- _ O
the -X- _ O
architecture -X- _ O
itself -X- _ O
. -X- _ O
AlBERT -X- _ B-MethodName
[ -X- _ O
34 -X- _ O
] -X- _ O
proposes -X- _ O
two -X- _ O
ideas -X- _ O
to -X- _ O
decrease -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
parameters -X- _ O
. -X- _ O
The -X- _ O
ﬁrst -X- _ O
approach -X- _ O
factor- -X- _ O
izes -X- _ O
the -X- _ O
embedding -X- _ O
of -X- _ O
the -X- _ O
parameters -X- _ O
. -X- _ O
It -X- _ O
separates -X- _ O
the -X- _ O
large -X- _ O
vocabulary -X- _ O
embedding -X- _ O
matrix -X- _ O
into -X- _ O
two -X- _ O
smaller -X- _ O
matrices -X- _ O
. -X- _ O
The -X- _ O
size -X- _ O
of -X- _ O
the -X- _ O
hidden -X- _ O
layer -X- _ O
is -X- _ O
separated -X- _ O
from -X- _ O
the -X- _ O
size -X- _ O
of -X- _ O
the -X- _ O
vocabulary -X- _ O
representation -X- _ O
. -X- _ O
The -X- _ O
second -X- _ O
method -X- _ O
is -X- _ O
a -X- _ O
cross -X- _ O
- -X- _ O
layer -X- _ O
parameter -X- _ O
sharing -X- _ O
. -X- _ O
This -X- _ O
technique -X- _ O
prevents -X- _ O
the -X- _ O
parameters -X- _ O
from -X- _ O
growing -X- _ O
with -X- _ O
the -X- _ O
depth -X- _ O
of -X- _ O
the -X- _ O
network -X- _ O
. -X- _ O
With -X- _ O
those -X- _ O
two -X- _ O
tricks -X- _ O
, -X- _ O
it -X- _ O
allows -X- _ O
reducing -X- _ O
the -X- _ O
size -X- _ O
of -X- _ O
the -X- _ O
large -X- _ O
BERT -X- _ B-MethodName
version -X- _ O
by -X- _ O
18 -X- _ O
% -X- _ O
without -X- _ O
a -X- _ O
loss -X- _ O
of -X- _ O
performance -X- _ O
. -X- _ O
Since -X- _ O
this -X- _ O
architecture -X- _ O
is -X- _ O
smaller -X- _ O
, -X- _ O
the -X- _ O
training -X- _ O
time -X- _ O
is -X- _ O
also -X- _ O
faster -X- _ O
. -X- _ O

Authorized -X- _ O
licensed -X- _ O
use -X- _ O
limited -X- _ O
to -X- _ O
: -X- _ O
Carnegie -X- _ O
Mellon -X- _ O
Libraries -X- _ O
. -X- _ O
Downloaded -X- _ O
on -X- _ O
October -X- _ O
18,2022 -X- _ O
at -X- _ O
00:17:46 -X- _ O
UTC -X- _ O
from -X- _ O
IEEE -X- _ O
Xplore -X- _ O
. -X- _ O
Restrictions -X- _ O
apply -X- _ O
. -X- _ O

ANTHONY -X- _ O
GILLIOZ -X- _ O
ET -X- _ O
AL -X- _ O
. -X- _ O
: -X- _ O
OVERVIEW -X- _ O
OF -X- _ O
THE -X- _ O
TRANSFORMER -X- _ O
- -X- _ O
BASED -X- _ O
MODELS -X- _ O
FOR -X- _ O
NLP -X- _ B-TaskName
TASKS -X- _ O
181 -X- _ O

with -X- _ O
8 -X- _ O
billion -X- _ O
parameters -X- _ O
, -X- _ O
it -X- _ O
became -X- _ O
infeasible -X- _ O
for -X- _ O
small -X- _ O
tech -X- _ O
companies -X- _ O
or -X- _ O
small -X- _ O
labs -X- _ O
to -X- _ O
train -X- _ O
a -X- _ O
network -X- _ O
as -X- _ O
huge -X- _ O
as -X- _ O
that -X- _ O
. -X- _ O

Since -X- _ O
the -X- _ O
Transformer -X- _ O
’s -X- _ O
revolution -X- _ O
, -X- _ O
state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
art -X- _ O
networks -X- _ O
have -X- _ O
become -X- _ O
bigger -X- _ O
and -X- _ O
bigger -X- _ O
. -X- _ O
Accordingly -X- _ O
, -X- _ O
to -X- _ O
have -X- _ O
a -X- _ O
better -X- _ O
language -X- _ O
representation -X- _ O
and -X- _ O
better -X- _ O
end -X- _ O
- -X- _ O
task -X- _ O
results -X- _ O
, -X- _ O
the -X- _ O
models -X- _ O
must -X- _ O
grow -X- _ O
to -X- _ O
catch -X- _ O
the -X- _ O
high -X- _ O
complexity -X- _ O
of -X- _ O
texts -X- _ O
. -X- _ O
This -X- _ O
ex- -X- _ O
pansion -X- _ O
of -X- _ O
the -X- _ O
network -X- _ O
’s -X- _ O
size -X- _ O
has -X- _ O
a -X- _ O
high -X- _ O
computational -X- _ O
cost -X- _ O
. -X- _ O
More -X- _ O
powerful -X- _ O
GPUs -X- _ B-MethodName
and -X- _ O
TPUs -X- _ B-MethodName
are -X- _ O
required -X- _ O
to -X- _ O
train -X- _ O
those -X- _ O
large -X- _ O
models -X- _ O
. -X- _ O
If -X- _ O
we -X- _ O
take -X- _ O
, -X- _ O
for -X- _ O
example -X- _ O
, -X- _ O
the -X- _ O
Nvidia -X- _ O
’s -X- _ O
GPT-8B -X- _ B-MethodName
1 -X- _ O

Further -X- _ O
, -X- _ O
studies -X- _ O
have -X- _ O
been -X- _ O
done -X- _ O
to -X- _ O
improve -X- _ O
the -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
phase -X- _ O
of -X- _ O
BERT -X- _ B-MethodName
. -X- _ O
The -X- _ O
post -X- _ O
- -X- _ O
BERT -X- _ B-MethodName
model -X- _ O
RoBERTa -X- _ B-MethodName
[ -X- _ O
16 -X- _ O
] -X- _ O
pro- -X- _ O
poses -X- _ O
three -X- _ O
simple -X- _ O
modiﬁcations -X- _ O
of -X- _ O
the -X- _ O
training -X- _ O
procedure -X- _ O
. -X- _ O
( -X- _ O
I -X- _ O
) -X- _ O
Based -X- _ O
on -X- _ O
their -X- _ O
empirical -X- _ O
results -X- _ O
, -X- _ O
[ -X- _ O
16 -X- _ O
] -X- _ O
shows -X- _ O
that -X- _ O
BERT -X- _ B-MethodName
is -X- _ O
un- -X- _ O
dertrained -X- _ O
. -X- _ O
To -X- _ O
alleviate -X- _ O
this -X- _ O
problem -X- _ O
, -X- _ O
they -X- _ O
propose -X- _ O
to -X- _ O
increase -X- _ O
the -X- _ O
length -X- _ O
of -X- _ O
the -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
phase -X- _ O
. -X- _ O
By -X- _ O
learning -X- _ O
longer -X- _ O
, -X- _ O
the -X- _ O
outcomes -X- _ O
are -X- _ O
more -X- _ O
accurate -X- _ O
. -X- _ O
( -X- _ O
II -X- _ O
) -X- _ O
As -X- _ O
the -X- _ O
results -X- _ O
of -X- _ O
[ -X- _ O
32 -X- _ O
] -X- _ O
and -X- _ O
[ -X- _ O
14 -X- _ O
] -X- _ O
demonstrate -X- _ O
, -X- _ O
the -X- _ O
accuracy -X- _ B-MetricName
of -X- _ O
the -X- _ O
end -X- _ O
- -X- _ O
task -X- _ O
performance -X- _ O
relies -X- _ O
on -X- _ O
the -X- _ O
wide -X- _ O
variety -X- _ O
of -X- _ O
trained -X- _ O
data -X- _ O
. -X- _ O
Therefore -X- _ O
, -X- _ O
BERT -X- _ B-MethodName
must -X- _ O
be -X- _ O
trained -X- _ O
on -X- _ O
larger -X- _ O
datasets -X- _ O
. -X- _ O
( -X- _ O
III -X- _ O
) -X- _ O
In -X- _ O
order -X- _ O
to -X- _ O
improve -X- _ O
the -X- _ O
optimization -X- _ O
of -X- _ O
the -X- _ O
model -X- _ O
, -X- _ O
they -X- _ O
propose -X- _ O
to -X- _ O
increase -X- _ O
the -X- _ O
batch -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
. -X- _ O
There -X- _ O
are -X- _ O
two -X- _ O
advantages -X- _ O
to -X- _ O
have -X- _ O
a -X- _ O
bigger -X- _ O
batch -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
; -X- _ O
First -X- _ O
, -X- _ O
the -X- _ O
large -X- _ O
batch -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
is -X- _ O
easier -X- _ O
to -X- _ O
parallelize -X- _ O
, -X- _ O
and -X- _ O
second -X- _ O
, -X- _ O
it -X- _ O
increases -X- _ O
the -X- _ O
perplexity -X- _ O
of -X- _ O
the -X- _ O
MLM -X- _ B-MethodName
objective -X- _ O
. -X- _ O

Due -X- _ O
to -X- _ O
the -X- _ O
high -X- _ O
performance -X- _ O
of -X- _ O
BERT -X- _ B-MethodName
on -X- _ O
11 -X- _ O
NLP -X- _ B-MethodName
tasks -X- _ O
, -X- _ O
a -X- _ O
lot -X- _ O
of -X- _ O
researchers -X- _ O
inspired -X- _ O
by -X- _ O
BERT -X- _ B-MethodName
’s -X- _ O
architecture -X- _ O
applied -X- _ O
it -X- _ O
and -X- _ O
tweaked -X- _ O
it -X- _ O
to -X- _ O
their -X- _ O
needs -X- _ O
[ -X- _ O
30 -X- _ O
] -X- _ O
, -X- _ O
[ -X- _ O
31 -X- _ O
] -X- _ O
. -X- _ O

( -X- _ O
BERT -X- _ B-MethodName
) -X- _ O
. -X- _ O
This -X- _ O
model -X- _ O
can -X- _ O
fuse -X- _ O
the -X- _ O
left -X- _ O
and -X- _ O
the -X- _ O
right -X- _ O
context -X- _ O
of -X- _ O
a -X- _ O
sentence -X- _ O
, -X- _ O
providing -X- _ O
a -X- _ O
bidirectional -X- _ O
representation -X- _ O
and -X- _ O
allow -X- _ O
a -X- _ O
better -X- _ O
context -X- _ O
extractor -X- _ O
for -X- _ O
reasoning -X- _ O
tasks -X- _ O
. -X- _ O
The -X- _ O
architecture -X- _ O
of -X- _ O
BERT -X- _ B-MethodName
is -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
Multi -X- _ O
- -X- _ O
Head -X- _ O
Attention -X- _ O
layers -X- _ O
encoder -X- _ O
like -X- _ O
proposed -X- _ O
in -X- _ O
[ -X- _ O
7 -X- _ O
] -X- _ O
. -X- _ O
Originally -X- _ O
[ -X- _ O
9 -X- _ O
] -X- _ O
proposed -X- _ O
two -X- _ O
versions -X- _ O
of -X- _ O
BERT -X- _ B-MethodName
, -X- _ O
the -X- _ O
base -X- _ O
version -X- _ O
with -X- _ O
110 -X- _ O
M -X- _ O
of -X- _ O
parameters -X- _ O
and -X- _ O
the -X- _ O
large -X- _ O
version -X- _ O
with -X- _ O
340 -X- _ O
M -X- _ O
parameters -X- _ O
. -X- _ O
Like -X- _ O
GPT -X- _ B-MethodName
and -X- _ O
GPT-2 -X- _ B-MethodName
, -X- _ O
BERT -X- _ B-MethodName
has -X- _ O
an -X- _ O
unsupervised -X- _ O
pre- -X- _ O
training -X- _ O
phase -X- _ O
where -X- _ O
it -X- _ O
learns -X- _ O
its -X- _ O
language -X- _ O
representation -X- _ O
. -X- _ O
Nevertheless -X- _ O
, -X- _ O
due -X- _ O
to -X- _ O
its -X- _ O
inherent -X- _ O
bidirectional -X- _ O
architecture -X- _ O
, -X- _ O
it -X- _ O
can -X- _ O
not -X- _ O
be -X- _ O
trained -X- _ O
using -X- _ O
the -X- _ O
standard -X- _ O
Language -X- _ O
Model -X- _ O
objective -X- _ O
. -X- _ O
Indeed -X- _ O
, -X- _ O
the -X- _ O
bidirectionality -X- _ O
of -X- _ O
BERT -X- _ B-MethodName
allows -X- _ O
each -X- _ O
word -X- _ O
to -X- _ O
see -X- _ O
itself -X- _ O
, -X- _ O
and -X- _ O
therefore -X- _ O
it -X- _ O
can -X- _ O
trivially -X- _ O
predict -X- _ O
the -X- _ O
next -X- _ O
token -X- _ O
. -X- _ O
To -X- _ O
overcome -X- _ O
this -X- _ O
issue -X- _ O
and -X- _ O
pre -X- _ O
- -X- _ O
train -X- _ O
their -X- _ O
model -X- _ O
, -X- _ O
[ -X- _ O
9 -X- _ O
] -X- _ O
use -X- _ O
two -X- _ O
unsupervised -X- _ O
objective -X- _ O
tasks -X- _ O
: -X- _ O
the -X- _ O
Masked -X- _ B-MethodName
Language -X- _ I-MethodName
Model -X- _ I-MethodName
( -X- _ O
MLM -X- _ B-MethodName
) -X- _ O
and -X- _ O
the -X- _ O
Next -X- _ B-TaskName
Sentence -X- _ I-TaskName
Prediction -X- _ I-TaskName
( -X- _ O
NSP -X- _ B-TaskName
) -X- _ O
. -X- _ O
Once -X- _ O
the -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
phase -X- _ O
is -X- _ O
over -X- _ O
, -X- _ O
it -X- _ O
remains -X- _ O
to -X- _ O
ﬁne -X- _ O
- -X- _ O
tune -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
the -X- _ O
downstream -X- _ O
tasks -X- _ O
. -X- _ O
Thanks -X- _ O
to -X- _ O
BERT -X- _ B-MethodName
’s -X- _ O
Trans- -X- _ O
former -X- _ O
architecture -X- _ O
, -X- _ O
the -X- _ O
downstream -X- _ O
can -X- _ O
be -X- _ O
straightforwardly -X- _ O
done -X- _ O
because -X- _ O
the -X- _ O
same -X- _ O
structure -X- _ O
is -X- _ O
used -X- _ O
for -X- _ O
the -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
and -X- _ O
the -X- _ O
ﬁne -X- _ O
- -X- _ O
tuning -X- _ O
. -X- _ O
It -X- _ O
merely -X- _ O
needs -X- _ O
to -X- _ O
change -X- _ O
the -X- _ O
ﬁnal -X- _ O
layer -X- _ O
to -X- _ O
match -X- _ O
the -X- _ O
requirements -X- _ O
of -X- _ O
the -X- _ O
downstream -X- _ O
task -X- _ O
. -X- _ O

C. -X- _ O
BERT -X- _ B-MethodName
GPT -X- _ B-MethodName
and -X- _ O
GPT-2 -X- _ B-MethodName
use -X- _ O
a -X- _ O
unidirectional -X- _ O
language -X- _ O
model -X- _ O
; -X- _ O
they -X- _ O
can -X- _ O
only -X- _ O
reach -X- _ O
the -X- _ O
left -X- _ O
context -X- _ O
of -X- _ O
the -X- _ O
evaluated -X- _ O
token -X- _ O
. -X- _ O
That -X- _ O
property -X- _ O
can -X- _ O
harm -X- _ O
the -X- _ O
overall -X- _ O
performance -X- _ O
of -X- _ O
those -X- _ O
models -X- _ O
in -X- _ O
reasoning -X- _ O
or -X- _ O
question -X- _ O
answering -X- _ O
tasks -X- _ O
. -X- _ O
Because -X- _ O
, -X- _ O
in -X- _ O
those -X- _ O
topics -X- _ O
, -X- _ O
both -X- _ O
sides -X- _ O
of -X- _ O
the -X- _ O
sentence -X- _ O
are -X- _ O
crucial -X- _ O
to -X- _ O
getting -X- _ O
an -X- _ O
optimal -X- _ O
sentence -X- _ O
- -X- _ O
level -X- _ O
understanding -X- _ O
. -X- _ O
To -X- _ O
counter -X- _ O
this -X- _ O
unidirectional -X- _ O
constraint -X- _ O
, -X- _ O
[ -X- _ O
9 -X- _ O
] -X- _ O
introduced -X- _ O
the -X- _ O
Bidirectional -X- _ B-MethodName
Encoder -X- _ I-MethodName
Representations -X- _ I-MethodName
from -X- _ I-MethodName
Transformers -X- _ I-MethodName

This -X- _ O
LM -X- _ B-MethodName
function -X- _ O
maximizes -X- _ O
the -X- _ O
likelihood -X- _ O
of -X- _ O
the -X- _ O
condi- -X- _ O
tional -X- _ O
probability -X- _ O
P. -X- _ O
Where -X- _ O
X -X- _ O
is -X- _ O
the -X- _ O
input -X- _ O
sequence -X- _ O
, -X- _ O
k -X- _ O
is -X- _ O
the -X- _ O
context -X- _ O
window -X- _ O
, -X- _ O
and -X- _ O
Θ -X- _ O
are -X- _ O
the -X- _ O
parameters -X- _ O
of -X- _ O
the -X- _ O
Neural -X- _ O
Network -X- _ O
. -X- _ O
Various -X- _ O
models -X- _ O
are -X- _ O
using -X- _ O
this -X- _ O
property -X- _ O
coupled -X- _ O
with -X- _ O
the -X- _ O
Transformer -X- _ O
architecture -X- _ O
to -X- _ O
produce -X- _ O
accurate -X- _ O
Language -X- _ O
Model -X- _ O
languages -X- _ O
( -X- _ O
i.e. -X- _ O
it -X- _ O
determines -X- _ O
the -X- _ O
statistical -X- _ O
distribution -X- _ O
of -X- _ O
the -X- _ O
learned -X- _ O
texts -X- _ O
) -X- _ O
. -X- _ O
The -X- _ O
ﬁrst -X- _ O
auto -X- _ O
- -X- _ O
regressive -X- _ O
model -X- _ O
using -X- _ O
the -X- _ O
Transformer -X- _ O
architecture -X- _ O
is -X- _ O
GPT -X- _ B-MethodName
[ -X- _ O
8 -X- _ O
] -X- _ O
. -X- _ O
It -X- _ O
has -X- _ O
a -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
Language -X- _ O
Modeling -X- _ O
phase -X- _ O
where -X- _ O
it -X- _ O
learns -X- _ O
on -X- _ O
raw -X- _ O
texts -X- _ O
. -X- _ O
In -X- _ O
the -X- _ O
second -X- _ O
learning -X- _ O
phase -X- _ O
, -X- _ O
it -X- _ O
uses -X- _ O
supervised -X- _ O
ﬁne -X- _ O
- -X- _ O
tuning -X- _ O
to -X- _ O
adjust -X- _ O
the -X- _ O
network -X- _ O
to -X- _ O
the -X- _ O
downstream -X- _ O
tasks -X- _ O
. -X- _ O
GPT-2 -X- _ B-MethodName
[ -X- _ O
14 -X- _ O
] -X- _ O
uses -X- _ O
the -X- _ O
same -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
principles -X- _ O
than -X- _ O
GPT -X- _ B-MethodName
. -X- _ O
Though -X- _ O
, -X- _ O
this -X- _ O
time -X- _ O
it -X- _ O
tries -X- _ O
to -X- _ O
achieve -X- _ O
the -X- _ O
same -X- _ O
results -X- _ O
in -X- _ O
a -X- _ O
zero -X- _ O
- -X- _ O
shot -X- _ O
fashion -X- _ O
( -X- _ O
i.e. -X- _ O
without -X- _ O
ﬁne -X- _ O
- -X- _ O
tuning -X- _ O
the -X- _ O
network -X- _ O
to -X- _ O
the -X- _ O
downstream -X- _ O
tasks -X- _ O
) -X- _ O
. -X- _ O
To -X- _ O
accomplish -X- _ O
that -X- _ O
goal -X- _ O
, -X- _ O
it -X- _ O
must -X- _ O
capture -X- _ O
the -X- _ O
full -X- _ O
complexity -X- _ O
of -X- _ O
textual -X- _ O
data -X- _ O
. -X- _ O
To -X- _ O
do -X- _ O
so -X- _ O
, -X- _ O
it -X- _ O
needs -X- _ O
a -X- _ O
wider -X- _ O
system -X- _ O
with -X- _ O
more -X- _ O
parameters -X- _ O
. -X- _ O
The -X- _ O
results -X- _ O
of -X- _ O
this -X- _ O
model -X- _ O
are -X- _ O
competitive -X- _ O
to -X- _ O
some -X- _ O
other -X- _ O
supervised -X- _ O
tasks -X- _ O
on -X- _ O
a -X- _ O
few -X- _ O
subjects -X- _ O
( -X- _ O
e.g. -X- _ O
reading -X- _ O
comprehension -X- _ O
) -X- _ O
but -X- _ O
are -X- _ O
far -X- _ O
from -X- _ O
being -X- _ O
usable -X- _ O
on -X- _ O
other -X- _ O
jobs -X- _ O
such -X- _ O
as -X- _ O
summarization -X- _ B-TaskName
. -X- _ O
Another -X- _ O
auto -X- _ O
- -X- _ O
regressive -X- _ O
network -X- _ O
is -X- _ O
XLNet -X- _ B-MethodName
[ -X- _ O
28 -X- _ O
] -X- _ O
. -X- _ O
It -X- _ O
aims -X- _ O
to -X- _ O
use -X- _ O
the -X- _ O
strength -X- _ O
of -X- _ O
the -X- _ O
language -X- _ O
modeling -X- _ O
of -X- _ O
the -X- _ O
auto- -X- _ O
regressive -X- _ O
model -X- _ O
and -X- _ O
at -X- _ O
the -X- _ O
same -X- _ O
time -X- _ O
, -X- _ O
use -X- _ O
the -X- _ O
bidirectionality -X- _ O
of -X- _ O
BERT -X- _ B-MethodName
[ -X- _ O
9 -X- _ O
] -X- _ O
. -X- _ O
To -X- _ O
do -X- _ O
so -X- _ O
, -X- _ O
it -X- _ O
relies -X- _ O
on -X- _ O
transformer -X- _ O
- -X- _ O
XL -X- _ O
[ -X- _ O
29 -X- _ O
] -X- _ O
, -X- _ O
the -X- _ O
state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
art -X- _ O
model -X- _ O
for -X- _ O
the -X- _ O
auto -X- _ O
- -X- _ O
regressive -X- _ O
network -X- _ O
. -X- _ O

B. -X- _ O
Auto -X- _ O
- -X- _ O
Regressive -X- _ O
Models -X- _ O
The -X- _ O
auto -X- _ O
- -X- _ O
regressive -X- _ O
models -X- _ O
take -X- _ O
the -X- _ O
previous -X- _ O
outputs -X- _ O
to -X- _ O
produce -X- _ O
the -X- _ O
next -X- _ O
outcome -X- _ O
. -X- _ O
It -X- _ O
has -X- _ O
the -X- _ O
particularity -X- _ O
to -X- _ O
be -X- _ O
a -X- _ O
unidirectional -X- _ O
network -X- _ O
; -X- _ O
it -X- _ O
can -X- _ O
only -X- _ O
reach -X- _ O
the -X- _ O
left -X- _ O
context -X- _ O
of -X- _ O
the -X- _ O
evaluated -X- _ O
token -X- _ O
. -X- _ O
However -X- _ O
, -X- _ O
despite -X- _ O
this -X- _ O
ﬂaw -X- _ O
, -X- _ O
it -X- _ O
can -X- _ O
learn -X- _ O
accurate -X- _ O
sentence -X- _ O
representations -X- _ O
. -X- _ O
It -X- _ O
relies -X- _ O
on -X- _ O
the -X- _ O
regular -X- _ O
Language -X- _ B-MethodName
Modeling -X- _ I-MethodName
( -X- _ O
LM -X- _ B-MethodName
) -X- _ O
task -X- _ O
as -X- _ O
an -X- _ O
unsupervised -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
objective -X- _ O
: -X- _ O

word -X- _ O
YM -X- _ O
used -X- _ O
the -X- _ O
latent -X- _ O
representation -X- _ O
Z -X- _ O
and -X- _ O
the -X- _ O
previously -X- _ O
created -X- _ O
sequence -X- _ O
YM−1 -X- _ O
= -X- _ O
( -X- _ O
y1 -X- _ O
, -X- _ O
... -X- _ O
, -X- _ O
yM−1 -X- _ O
) -X- _ O
to -X- _ O
be -X- _ O
generated -X- _ O
. -X- _ O
The -X- _ O
Encoder -X- _ O
and -X- _ O
the -X- _ O
Decoder -X- _ O
are -X- _ O
using -X- _ O
the -X- _ O
same -X- _ O
Multi -X- _ O
- -X- _ O
Head -X- _ O
Attention -X- _ O
layer -X- _ O
. -X- _ O
A -X- _ O
single -X- _ O
Attention -X- _ O
layer -X- _ O
maps -X- _ O
a -X- _ O
query -X- _ O
Q -X- _ O
and -X- _ O
keys -X- _ O
K -X- _ O
to -X- _ O
a -X- _ O
weighted -X- _ O
sum -X- _ O
of -X- _ O
the -X- _ O
values -X- _ O
V -X- _ O
. -X- _ O
For -X- _ O
technical -X- _ O
reason -X- _ O
there -X- _ O
is -X- _ O
a -X- _ O
scaling -X- _ O
factor -X- _ O
1 -X- _ O
√dk -X- _ O
. -X- _ O

Authorized -X- _ O
licensed -X- _ O
use -X- _ O
limited -X- _ O
to -X- _ O
: -X- _ O
Carnegie -X- _ O
Mellon -X- _ O
Libraries -X- _ O
. -X- _ O
Downloaded -X- _ O
on -X- _ O
October -X- _ O
18,2022 -X- _ O
at -X- _ O
00:17:46 -X- _ O
UTC -X- _ O
from -X- _ O
IEEE -X- _ O
Xplore -X- _ O
. -X- _ O
Restrictions -X- _ O
apply -X- _ O
. -X- _ O

The -X- _ O
Transformer -X- _ O
is -X- _ O
based -X- _ O
on -X- _ O
an -X- _ O
encoder -X- _ O
- -X- _ O
decoder -X- _ O
structure -X- _ O
, -X- _ O
where -X- _ O
it -X- _ O
takes -X- _ O
a -X- _ O
sequence -X- _ O
X -X- _ O
= -X- _ O
( -X- _ O
x1 -X- _ O
, -X- _ O
... -X- _ O
, -X- _ O
xN -X- _ O
) -X- _ O
and -X- _ O
produce -X- _ O
a -X- _ O
latent -X- _ O
representation -X- _ O
Z -X- _ O
= -X- _ O
( -X- _ O
z1 -X- _ O
, -X- _ O
... -X- _ O
, -X- _ O
zN -X- _ O
) -X- _ O
. -X- _ O
Due -X- _ O
to -X- _ O
the -X- _ O
auto- -X- _ O
regressive -X- _ O
property -X- _ O
of -X- _ O
this -X- _ O
model -X- _ O
, -X- _ O
the -X- _ O
output -X- _ O
sequence -X- _ O
YM -X- _ O
= -X- _ O
( -X- _ O
y1 -X- _ O
, -X- _ O
... -X- _ O
, -X- _ O
yM -X- _ O
) -X- _ O
is -X- _ O
produced -X- _ O
one -X- _ O
element -X- _ O
at -X- _ O
a -X- _ O
time -X- _ O
. -X- _ O
i.e. -X- _ O
the -X- _ O

The -X- _ O
RNNs -X- _ B-MethodName
( -X- _ O
LSTM -X- _ B-MethodName
, -X- _ O
GRU -X- _ B-MethodName
) -X- _ O
have -X- _ O
a -X- _ O
recurrent -X- _ O
underlying -X- _ O
structure -X- _ O
and -X- _ O
are -X- _ O
, -X- _ O
by -X- _ O
deﬁnition -X- _ O
recurrent -X- _ O
. -X- _ O
It -X- _ O
is -X- _ O
then -X- _ O
hard -X- _ O
to -X- _ O
parallelize -X- _ O
the -X- _ O
learning -X- _ O
process -X- _ O
because -X- _ O
of -X- _ O
this -X- _ O
fundamental -X- _ O
property -X- _ O
. -X- _ O
To -X- _ O
overcome -X- _ O
this -X- _ O
issue -X- _ O
, -X- _ O
[ -X- _ O
7 -X- _ O
] -X- _ O
proposed -X- _ O
a -X- _ O
new -X- _ O
archi- -X- _ O
tecture -X- _ O
solely -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
attention -X- _ O
layers -X- _ O
; -X- _ O
the -X- _ O
Transformer -X- _ O
. -X- _ O
It -X- _ O
has -X- _ O
the -X- _ O
advantage -X- _ O
to -X- _ O
catch -X- _ O
the -X- _ O
long -X- _ O
- -X- _ O
range -X- _ O
dependencies -X- _ O
of -X- _ O
a -X- _ O
sentence -X- _ O
and -X- _ O
to -X- _ O
be -X- _ O
parallelizable -X- _ O
. -X- _ O

generalize -X- _ O
correctly -X- _ O
. -X- _ O
That -X- _ O
is -X- _ O
the -X- _ O
idea -X- _ O
that -X- _ O
promotes -X- _ O
the -X- _ O
creation -X- _ O
of -X- _ O
GLUE -X- _ B-DatasetName
, -X- _ O
SQuAD -X- _ B-DatasetName
V1.1 -X- _ O
/ -X- _ O
V2.0 -X- _ O
and -X- _ O
RACE -X- _ B-DatasetName
to -X- _ O
have -X- _ O
benchmarks -X- _ O
able -X- _ O
to -X- _ O
check -X- _ O
the -X- _ O
reliability -X- _ O
of -X- _ O
models -X- _ O
on -X- _ O
various -X- _ O
tasks -X- _ O
. -X- _ O
GLUE -X- _ B-TaskName
: -X- _ O
The -X- _ B-TaskName
General -X- _ I-TaskName
Language -X- _ I-TaskName
Understanding -X- _ I-TaskName
Evaluation -X- _ I-TaskName
( -X- _ O
GLUE -X- _ B-TaskName
) -X- _ O
[ -X- _ O
23 -X- _ O
] -X- _ O
is -X- _ O
a -X- _ O
collection -X- _ O
of -X- _ O
nine -X- _ O
tasks -X- _ O
created -X- _ O
to -X- _ O
test -X- _ O
the -X- _ O
generalization -X- _ O
of -X- _ O
modern -X- _ O
NLP -X- _ B-TaskName
models -X- _ O
. -X- _ O
It -X- _ O
reviews -X- _ O
a -X- _ O
wide -X- _ O
range -X- _ O
of -X- _ O
NLP -X- _ B-TaskName
problems -X- _ O
like -X- _ O
Sentiment -X- _ B-TaskName
Analysis -X- _ I-TaskName
, -X- _ O
Question -X- _ B-TaskName
Answer- -X- _ I-TaskName
ing -X- _ I-TaskName
and -X- _ O
inference -X- _ O
tasks -X- _ O
. -X- _ O
Because -X- _ O
of -X- _ O
the -X- _ O
rapid -X- _ O
improvement -X- _ O
of -X- _ O
the -X- _ O
state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
art -X- _ O
on -X- _ O
GLUE -X- _ B-TaskName
, -X- _ O
SuperGLUE -X- _ B-TaskName
[ -X- _ O
24 -X- _ O
] -X- _ O
is -X- _ O
a -X- _ O
new -X- _ O
proposed -X- _ O
benchmark -X- _ O
to -X- _ O
check -X- _ O
general -X- _ O
language -X- _ O
systems -X- _ O
but -X- _ O
with -X- _ O
more -X- _ O
complicated -X- _ O
more -X- _ O
laborious -X- _ O
tasks -X- _ O
. -X- _ O
SQuAD -X- _ B-DatasetName
: -X- _ O
Stanford -X- _ B-DatasetName
Question -X- _ I-DatasetName
Answering -X- _ I-DatasetName
Dataset -X- _ I-DatasetName
( -X- _ O
SQuAD -X- _ B-DatasetName
) -X- _ O
V1.1 -X- _ O
[ -X- _ O
25 -X- _ O
] -X- _ O
is -X- _ O
a -X- _ O
benchmark -X- _ O
designed -X- _ O
to -X- _ O
resolve -X- _ O
Reading -X- _ O
Com- -X- _ O
prehension -X- _ O
( -X- _ O
RC -X- _ O
) -X- _ O
challenges -X- _ O
. -X- _ O
There -X- _ O
are -X- _ O
more -X- _ O
than -X- _ O
100,000 -X- _ O
+ -X- _ O
questions -X- _ O
in -X- _ O
the -X- _ O
data -X- _ O
set -X- _ O
. -X- _ O
There -X- _ O
is -X- _ O
no -X- _ O
proposed -X- _ O
answer -X- _ O
like -X- _ O
in -X- _ O
the -X- _ O
other -X- _ O
RD -X- _ O
datasets -X- _ O
. -X- _ O
The -X- _ O
task -X- _ O
contains -X- _ O
a -X- _ O
document -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
model -X- _ O
has -X- _ O
to -X- _ O
ﬁnd -X- _ O
the -X- _ O
answer -X- _ O
directly -X- _ O
in -X- _ O
the -X- _ O
text -X- _ O
passage -X- _ O
. -X- _ O
SQuAD -X- _ B-DatasetName
v2.0 -X- _ I-DatasetName
[ -X- _ O
26 -X- _ O
] -X- _ O
is -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
same -X- _ O
principle -X- _ O
than -X- _ O
the -X- _ O
V1.1 -X- _ O
, -X- _ O
but -X- _ O
this -X- _ O
time -X- _ O
the -X- _ O
answer -X- _ O
is -X- _ O
not -X- _ O
necessarily -X- _ O
in -X- _ O
the -X- _ O
questions -X- _ O
. -X- _ O
RACE -X- _ B-DatasetName
: -X- _ O
Reading -X- _ B-DatasetName
Comprehension -X- _ I-DatasetName
From -X- _ I-DatasetName
Examinations -X- _ I-DatasetName
( -X- _ O
RACE -X- _ B-DatasetName
) -X- _ O
[ -X- _ O
27 -X- _ O
] -X- _ O
is -X- _ O
a -X- _ O
collection -X- _ O
of -X- _ O
English -X- _ O
questions -X- _ O
set -X- _ O
aside -X- _ O
to -X- _ O
Chinese -X- _ O
students -X- _ O
from -X- _ O
middle -X- _ O
school -X- _ O
up -X- _ O
to -X- _ O
high -X- _ O
school -X- _ O
. -X- _ O
Each -X- _ O
item -X- _ O
is -X- _ O
divided -X- _ O
into -X- _ O
two -X- _ O
parts -X- _ O
, -X- _ O
a -X- _ O
passage -X- _ O
that -X- _ O
the -X- _ O
student -X- _ O
must -X- _ O
read -X- _ O
and -X- _ O
a -X- _ O
set -X- _ O
of -X- _ O
4 -X- _ O
potential -X- _ O
answers -X- _ O
. -X- _ O
Considering -X- _ O
that -X- _ O
the -X- _ O
questions -X- _ O
are -X- _ O
intended -X- _ O
to -X- _ O
teenagers -X- _ O
, -X- _ O
it -X- _ O
requires -X- _ O
keen -X- _ O
reasoning -X- _ O
skills -X- _ O
to -X- _ O
answer -X- _ O
correctly -X- _ O
to -X- _ O
most -X- _ O
of -X- _ O
the -X- _ O
problems -X- _ O
. -X- _ O
The -X- _ O
reasoning -X- _ O
subjects -X- _ O
present -X- _ O
in -X- _ O
RACE -X- _ B-DatasetName
cover -X- _ O
almost -X- _ O
all -X- _ O
human -X- _ O
knowledge -X- _ O
. -X- _ O

BookCorpus -X- _ B-DatasetName
[ -X- _ O
18 -X- _ O
] -X- _ O
plus -X- _ O
English -X- _ B-DatasetName
Wikipedia -X- _ I-DatasetName
13 -X- _ O
GB -X- _ O
3.87B -X- _ O
Giga5 -X- _ O
[ -X- _ O
19 -X- _ O
] -X- _ O
16 -X- _ O
GB -X- _ O
4.75B -X- _ O
ClueWeb09 -X- _ B-DatasetName
[ -X- _ O
20 -X- _ O
] -X- _ O
19 -X- _ O
GB -X- _ O
4.3B -X- _ O
OpenWebText -X- _ B-DatasetName
[ -X- _ O
21 -X- _ O
] -X- _ O
38 -X- _ O
GB -X- _ O
- -X- _ O
Real -X- _ O
- -X- _ O
News -X- _ O
[ -X- _ O
22 -X- _ O
] -X- _ O
120 -X- _ O
GB -X- _ O
‡ -X- _ O
- -X- _ O

TABLE -X- _ O
I -X- _ O
DATASETS -X- _ O
COMMONLY -X- _ O
USED -X- _ O
WITH -X- _ O
TRANSFORMER -X- _ O
- -X- _ O
BASED -X- _ O
MODELS -X- _ O
. -X- _ O
( -X- _ O
† -X- _ O
: -X- _ O
TOKENIZATION -X- _ O
DONE -X- _ O
WITH -X- _ O
SENTENCEPIECE -X- _ O
, -X- _ O
‡ -X- _ O
: -X- _ O
UNCOMPRESSED -X- _ O
DATA -X- _ O
) -X- _ O

During -X- _ O
an -X- _ O
extended -X- _ O
period -X- _ O
, -X- _ O
the -X- _ O
deep -X- _ O
learning -X- _ O
models -X- _ O
have -X- _ O
been -X- _ O
trained -X- _ O
to -X- _ O
resolve -X- _ O
one -X- _ O
problem -X- _ O
at -X- _ O
a -X- _ O
time -X- _ O
. -X- _ O
Further -X- _ O
, -X- _ O
when -X- _ O
those -X- _ O
models -X- _ O
were -X- _ O
used -X- _ O
in -X- _ O
another -X- _ O
domain -X- _ O
, -X- _ O
they -X- _ O
struggle -X- _ O
to -X- _ O

The -X- _ O
dominant -X- _ O
strategy -X- _ O
in -X- _ O
the -X- _ O
creation -X- _ O
of -X- _ O
deep -X- _ O
learning -X- _ O
sys- -X- _ O
tems -X- _ O
is -X- _ O
to -X- _ O
gather -X- _ O
a -X- _ O
corpus -X- _ O
corresponding -X- _ O
to -X- _ O
a -X- _ O
given -X- _ O
problem -X- _ O
. -X- _ O
The -X- _ O
next -X- _ O
step -X- _ O
is -X- _ O
to -X- _ O
label -X- _ O
this -X- _ O
data -X- _ O
and -X- _ O
build -X- _ O
a -X- _ O
network -X- _ O
that -X- _ O
is -X- _ O
supposedly -X- _ O
able -X- _ O
to -X- _ O
explain -X- _ O
them -X- _ O
. -X- _ O
This -X- _ O
method -X- _ O
is -X- _ O
not -X- _ O
suitable -X- _ O
if -X- _ O
we -X- _ O
want -X- _ O
to -X- _ O
create -X- _ O
a -X- _ O
more -X- _ O
comprehensive -X- _ O
system -X- _ O
( -X- _ O
i.e. -X- _ O
a -X- _ O
system -X- _ O
that -X- _ O
can -X- _ O
solve -X- _ O
multiple -X- _ O
problems -X- _ O
without -X- _ O
a -X- _ O
signiﬁcant -X- _ O
architecture -X- _ O
change -X- _ O
) -X- _ O
. -X- _ O
That -X- _ O
is -X- _ O
then -X- _ O
essential -X- _ O
to -X- _ O
learn -X- _ O
on -X- _ O
heterogeneous -X- _ O
data -X- _ O
to -X- _ O
create -X- _ O
general -X- _ O
NLP -X- _ B-TaskName
models -X- _ O
. -X- _ O
If -X- _ O
we -X- _ O
want -X- _ O
systems -X- _ O
that -X- _ O
can -X- _ O
resolve -X- _ O
several -X- _ O
tasks -X- _ O
at -X- _ O
the -X- _ O
same -X- _ O
time -X- _ O
, -X- _ O
it -X- _ O
is -X- _ O
necessary -X- _ O
to -X- _ O
train -X- _ O
this -X- _ O
model -X- _ O
on -X- _ O
a -X- _ O
wide -X- _ O
variety -X- _ O
of -X- _ O
subjects -X- _ O
. -X- _ O
Hopefully -X- _ O
, -X- _ O
in -X- _ O
our -X- _ O
ubiquitous -X- _ O
data -X- _ O
world -X- _ O
, -X- _ O
a -X- _ O
large -X- _ O
number -X- _ O
of -X- _ O
raw -X- _ O
texts -X- _ O
are -X- _ O
available -X- _ O
online -X- _ O
( -X- _ O
e.g. -X- _ O
Wikipedia -X- _ O
, -X- _ O
Web -X- _ O
blogs -X- _ O
, -X- _ O
Reddit -X- _ O
) -X- _ O
. -X- _ O
Table -X- _ O
I -X- _ O
shows -X- _ O
the -X- _ O
most -X- _ O
commonly -X- _ O
used -X- _ O
datasets -X- _ O
with -X- _ O
their -X- _ O
size -X- _ O
and -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
tokens -X- _ O
they -X- _ O
contain -X- _ O
. -X- _ O
The -X- _ O
tokenization -X- _ O
is -X- _ O
done -X- _ O
with -X- _ O
SentencePiece -X- _ O
[ -X- _ O
15 -X- _ O
] -X- _ O
. -X- _ O
In -X- _ O
a -X- _ O
few -X- _ O
cases -X- _ O
, -X- _ O
for -X- _ O
example -X- _ O
, -X- _ O
in -X- _ O
[ -X- _ O
16 -X- _ O
] -X- _ O
, -X- _ O
the -X- _ O
authors -X- _ O
only -X- _ O
used -X- _ O
a -X- _ O
subset -X- _ O
of -X- _ O
those -X- _ O
datasets -X- _ O
( -X- _ O
e.g. -X- _ O
Stories -X- _ O
[ -X- _ O
17 -X- _ O
] -X- _ O
is -X- _ O
a -X- _ O
subset -X- _ O
of -X- _ O
CommonCrawl -X- _ B-DatasetName
dataset -X- _ O
) -X- _ O
. -X- _ O

Primarily -X- _ O
proposed -X- _ O
by -X- _ O
[ -X- _ O
5 -X- _ O
] -X- _ O
, -X- _ O
the -X- _ O
attention -X- _ O
mechanism -X- _ O
aims -X- _ O
to -X- _ O
catch -X- _ O
the -X- _ O
long -X- _ O
- -X- _ O
term -X- _ O
dependencies -X- _ O
of -X- _ O
sentences -X- _ O
. -X- _ O
The -X- _ O
relation- -X- _ O
ships -X- _ O
between -X- _ O
entities -X- _ O
in -X- _ O
phrases -X- _ O
are -X- _ O
hard -X- _ O
to -X- _ O
spot -X- _ O
. -X- _ O
Furthermore -X- _ O
, -X- _ O
it -X- _ O
is -X- _ O
necessary -X- _ O
to -X- _ O
get -X- _ O
a -X- _ O
strong -X- _ O
understanding -X- _ O
of -X- _ O
the -X- _ O
underlying -X- _ O
structure -X- _ O
of -X- _ O
sentences -X- _ O
. -X- _ O
Indeed -X- _ O
, -X- _ O
if -X- _ O
we -X- _ O
can -X- _ O
have -X- _ O
a -X- _ O
method -X- _ O
that -X- _ O
can -X- _ O
tell -X- _ O
us -X- _ O
how -X- _ O
the -X- _ O
units -X- _ O
of -X- _ O
a -X- _ O
sentence -X- _ O
are -X- _ O
correlated -X- _ O
in -X- _ O
a -X- _ O
phrase -X- _ O
, -X- _ O
the -X- _ O
language -X- _ O
understanding -X- _ O
tasks -X- _ O
would -X- _ O
be -X- _ O
more -X- _ O
straightforward -X- _ O
. -X- _ O
The -X- _ O
attention -X- _ O
mechanism -X- _ O
computes -X- _ O
a -X- _ O
relation -X- _ O
mask -X- _ O
between -X- _ O
the -X- _ O
words -X- _ O
of -X- _ O
a -X- _ O
sentence -X- _ O
and -X- _ O
uses -X- _ O
this -X- _ O
mask -X- _ O
in -X- _ O
an -X- _ O
encoder- -X- _ O
decoder -X- _ O
architecture -X- _ O
to -X- _ O
detect -X- _ O
which -X- _ O
words -X- _ O
are -X- _ O
related -X- _ O
within -X- _ O
each -X- _ O
other -X- _ O
. -X- _ O
Using -X- _ O
this -X- _ O
process -X- _ O
, -X- _ O
the -X- _ O
NLP -X- _ B-TaskName
tasks -X- _ O
such -X- _ O
as -X- _ O
automatic -X- _ O
translation -X- _ O
are -X- _ O
more -X- _ O
ﬂexible -X- _ O
because -X- _ O
they -X- _ O
can -X- _ O
have -X- _ O
access -X- _ O
to -X- _ O
the -X- _ O
dependencies -X- _ O
of -X- _ O
the -X- _ O
sentence -X- _ O
. -X- _ O
In -X- _ O
a -X- _ O
translation -X- _ O
context -X- _ O
, -X- _ O
it -X- _ O
is -X- _ O
a -X- _ O
genuine -X- _ O
advantage -X- _ O
. -X- _ O
Another -X- _ O
notable -X- _ O
beneﬁt -X- _ O
of -X- _ O
the -X- _ O
attention -X- _ O
mechanism -X- _ O
is -X- _ O
the -X- _ O
straightforward -X- _ O
human -X- _ O
- -X- _ O
visualization -X- _ O
of -X- _ O
the -X- _ O
model -X- _ O
’s -X- _ O
outcome -X- _ O
. -X- _ O

produce -X- _ O
a -X- _ O
unique -X- _ O
word -X- _ O
representation -X- _ O
in -X- _ O
a -X- _ O
high -X- _ O
dimensional -X- _ O
space -X- _ O
. -X- _ O
Byte -X- _ O
Pair -X- _ O
Encoding -X- _ O
( -X- _ O
BPE -X- _ O
) -X- _ O
[ -X- _ O
13 -X- _ O
] -X- _ O
is -X- _ O
another -X- _ O
word -X- _ O
embedding -X- _ O
technique -X- _ O
using -X- _ O
subwords -X- _ O
units -X- _ O
out -X- _ O
of -X- _ O
character -X- _ O
- -X- _ O
level -X- _ O
and -X- _ O
word -X- _ O
- -X- _ O
level -X- _ O
representation -X- _ O
. -X- _ O
[ -X- _ O
14 -X- _ O
] -X- _ O
changed -X- _ O
the -X- _ O
implementation -X- _ O
of -X- _ O
BPE -X- _ O
to -X- _ O
be -X- _ O
based -X- _ O
on -X- _ O
bytes -X- _ O
instead -X- _ O
of -X- _ O
Unicode -X- _ O
characters -X- _ O
. -X- _ O
Thus -X- _ O
, -X- _ O
he -X- _ O
could -X- _ O
reduce -X- _ O
the -X- _ O
vocabulary -X- _ O
size -X- _ O
from -X- _ O
100K+ -X- _ O
to -X- _ O
approximately -X- _ O
50 -X- _ O
K -X- _ O
tokens -X- _ O
. -X- _ O
That -X- _ O
has -X- _ O
the -X- _ O
advantage -X- _ O
not -X- _ O
to -X- _ O
introduce -X- _ O
[ -X- _ O
UKN -X- _ O
] -X- _ O
( -X- _ O
unknown -X- _ O
) -X- _ O
symbols -X- _ O
. -X- _ O
Besides -X- _ O
that -X- _ O
, -X- _ O
it -X- _ O
does -X- _ O
not -X- _ O
involve -X- _ O
a -X- _ O
heuristic -X- _ O
preprocessing -X- _ O
of -X- _ O
the -X- _ O
input -X- _ O
vocabulary -X- _ O
. -X- _ O
It -X- _ O
is -X- _ O
used -X- _ O
when -X- _ O
the -X- _ O
amount -X- _ O
of -X- _ O
corpus -X- _ O
to -X- _ O
treat -X- _ O
is -X- _ O
too -X- _ O
large -X- _ O
and -X- _ O
a -X- _ O
more -X- _ O
efﬁcient -X- _ O
technique -X- _ O
than -X- _ O
Word2Vec -X- _ B-MethodName
or -X- _ O
Glove -X- _ B-MethodName
is -X- _ O
required -X- _ O
. -X- _ O

Authorized -X- _ O
licensed -X- _ O
use -X- _ O
limited -X- _ O
to -X- _ O
: -X- _ O
Carnegie -X- _ O
Mellon -X- _ O
Libraries -X- _ O
. -X- _ O
Downloaded -X- _ O
on -X- _ O
October -X- _ O
18,2022 -X- _ O
at -X- _ O
00:17:46 -X- _ O
UTC -X- _ O
from -X- _ O
IEEE -X- _ O
Xplore -X- _ O
. -X- _ O
Restrictions -X- _ O
apply -X- _ O
. -X- _ O

Proceedings -X- _ O
of -X- _ O
the -X- _ O
Federated -X- _ O
Conference -X- _ O
on -X- _ O
Computer -X- _ O
Science -X- _ O
and -X- _ O
Information -X- _ O
Systems -X- _ O
pp -X- _ O
. -X- _ O
179–183 -X- _ O
DOI -X- _ O
: -X- _ O
10.15439/2020F20 -X- _ O
ISSN -X- _ O
2300 -X- _ O
- -X- _ O
5963 -X- _ O
ACSIS -X- _ O
, -X- _ O
Vol -X- _ O
. -X- _ O
21 -X- _ O

The -X- _ O
recent -X- _ O
signiﬁcant -X- _ O
increase -X- _ O
in -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
NLP -X- _ B-TaskName
models -X- _ O
is -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
use -X- _ O
of -X- _ O
word -X- _ O
embeddings -X- _ O
. -X- _ O
It -X- _ O
consists -X- _ O
of -X- _ O
representing -X- _ O
a -X- _ O
word -X- _ O
as -X- _ O
a -X- _ O
unique -X- _ O
vector -X- _ O
. -X- _ O
The -X- _ O
terms -X- _ O
with -X- _ O
the -X- _ O
same -X- _ O
meaning -X- _ O
are -X- _ O
located -X- _ O
in -X- _ O
a -X- _ O
close -X- _ O
area -X- _ O
of -X- _ O
each -X- _ O
other -X- _ O
. -X- _ O
Word2Vec -X- _ B-MethodName
[ -X- _ O
11 -X- _ O
] -X- _ O
and -X- _ O
Glove -X- _ B-MethodName
[ -X- _ O
12 -X- _ O
] -X- _ O
are -X- _ O
the -X- _ O
most -X- _ O
frequently -X- _ O
used -X- _ O
word -X- _ O
embedding -X- _ O
methods -X- _ O
. -X- _ O
They -X- _ O
treat -X- _ O
a -X- _ O
large -X- _ O
corpus -X- _ O
of -X- _ O
text -X- _ O
and -X- _ O

The -X- _ O
unsupervised -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
is -X- _ O
a -X- _ O
particular -X- _ O
case -X- _ O
of -X- _ O
semi- -X- _ O
supervised -X- _ O
learning -X- _ O
. -X- _ O
That -X- _ O
is -X- _ O
massively -X- _ O
used -X- _ O
to -X- _ O
train -X- _ O
the -X- _ O
Trans- -X- _ O
former -X- _ O
models -X- _ O
. -X- _ O
That -X- _ O
principle -X- _ O
works -X- _ O
in -X- _ O
two -X- _ O
steps -X- _ O
; -X- _ O
the -X- _ O
ﬁrst -X- _ O
one -X- _ O
is -X- _ O
the -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
phase -X- _ O
. -X- _ O
It -X- _ O
computes -X- _ O
a -X- _ O
general -X- _ O
representation -X- _ O
from -X- _ O
raw -X- _ O
data -X- _ O
in -X- _ O
an -X- _ O
unsupervised -X- _ O
fashion -X- _ O
. -X- _ O
Second -X- _ O
, -X- _ O
once -X- _ O
it -X- _ O
is -X- _ O
computed -X- _ O
, -X- _ O
it -X- _ O
can -X- _ O
be -X- _ O
adapted -X- _ O
to -X- _ O
a -X- _ O
downstream -X- _ O
task -X- _ O
via -X- _ O
ﬁne- -X- _ O
tuning -X- _ O
techniques -X- _ O
. -X- _ O
The -X- _ O
principal -X- _ O
challenge -X- _ O
is -X- _ O
to -X- _ O
ﬁnd -X- _ O
an -X- _ O
unsupervised -X- _ O
objective -X- _ O
function -X- _ O
that -X- _ O
generates -X- _ O
a -X- _ O
good -X- _ O
representation -X- _ O
. -X- _ O
There -X- _ O
is -X- _ O
no -X- _ O
consensus -X- _ O
on -X- _ O
which -X- _ O
task -X- _ O
provides -X- _ O
the -X- _ O
most -X- _ O
efﬁcient -X- _ O
textual -X- _ O
de- -X- _ O
scription -X- _ O
. -X- _ O
[ -X- _ O
8 -X- _ O
] -X- _ O
propose -X- _ O
a -X- _ O
language -X- _ O
modelling -X- _ O
task -X- _ O
, -X- _ O
[ -X- _ O
9 -X- _ O
] -X- _ O
introduce -X- _ O
a -X- _ O
masked -X- _ O
language -X- _ O
modeling -X- _ O
objective -X- _ O
, -X- _ O
[ -X- _ O
10 -X- _ O
] -X- _ O
use -X- _ O
a -X- _ O
multi -X- _ O
- -X- _ O
tasks -X- _ O
language -X- _ O
modeling -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
section -X- _ O
, -X- _ O
we -X- _ O
introduce -X- _ O
a -X- _ O
general -X- _ O
NLP -X- _ B-TaskName
background -X- _ O
. -X- _ O
It -X- _ O
gives -X- _ O
a -X- _ O
broad -X- _ O
insight -X- _ O
into -X- _ O
the -X- _ O
unsupervised -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
and -X- _ O
the -X- _ O
NLP -X- _ B-TaskName
state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
art -X- _ O
pre -X- _ O
- -X- _ O
Transformers -X- _ O
. -X- _ O

providing -X- _ O
a -X- _ O
more -X- _ O
direct -X- _ O
way -X- _ O
to -X- _ O
the -X- _ O
backpropagation -X- _ O
of -X- _ O
the -X- _ O
gradient -X- _ O
. -X- _ O
It -X- _ O
helps -X- _ O
the -X- _ O
computation -X- _ O
when -X- _ O
the -X- _ O
sentences -X- _ O
are -X- _ O
long -X- _ O
. -X- _ O
The -X- _ O
high -X- _ O
versatility -X- _ O
of -X- _ O
those -X- _ O
networks -X- _ O
can -X- _ O
solve -X- _ O
a -X- _ O
wide -X- _ O
variety -X- _ O
of -X- _ O
problems -X- _ O
[ -X- _ O
6 -X- _ O
] -X- _ O
. -X- _ O
Unfortunately -X- _ O
, -X- _ O
those -X- _ O
models -X- _ O
are -X- _ O
not -X- _ O
perfect -X- _ O
; -X- _ O
the -X- _ O
inherent -X- _ O
recurrent -X- _ O
structure -X- _ O
made -X- _ O
them -X- _ O
hard -X- _ O
to -X- _ O
parallelize -X- _ O
on -X- _ O
multiple -X- _ O
processes -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
treatment -X- _ O
of -X- _ O
very -X- _ O
long -X- _ O
clauses -X- _ O
is -X- _ O
also -X- _ O
problematic -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
vanishing -X- _ O
gradient -X- _ O
. -X- _ O
To -X- _ O
counter -X- _ O
those -X- _ O
two -X- _ O
limiting -X- _ O
constraints -X- _ O
, -X- _ O
[ -X- _ O
7 -X- _ O
] -X- _ O
introduced -X- _ O
a -X- _ O
new -X- _ O
model -X- _ O
architecture -X- _ O
: -X- _ O
the -X- _ O
Transformer -X- _ B-MethodName
. -X- _ O
The -X- _ O
proposed -X- _ O
tech- -X- _ O
nique -X- _ O
get -X- _ O
rid -X- _ O
of -X- _ O
the -X- _ O
recurrent -X- _ O
architecture -X- _ O
to -X- _ O
rely -X- _ O
on -X- _ O
attention -X- _ O
mechanism -X- _ O
solely -X- _ O
. -X- _ O
Furthermore -X- _ O
, -X- _ O
it -X- _ O
does -X- _ O
not -X- _ O
suffer -X- _ O
from -X- _ O
the -X- _ O
gradient -X- _ O
vanishing -X- _ O
nor -X- _ O
the -X- _ O
hard -X- _ O
parallelization -X- _ O
issue -X- _ O
. -X- _ O
That -X- _ O
facilitates -X- _ O
and -X- _ O
accelerates -X- _ O
the -X- _ O
training -X- _ O
of -X- _ O
broader -X- _ O
networks -X- _ O
. -X- _ O
This -X- _ O
work -X- _ O
aims -X- _ O
to -X- _ O
provide -X- _ O
a -X- _ O
survey -X- _ O
and -X- _ O
an -X- _ O
explanation -X- _ O
of -X- _ O
the -X- _ O
latest -X- _ O
Transformer -X- _ O
- -X- _ O
based -X- _ O
models -X- _ O
. -X- _ O

I. -X- _ O
INTRODUCTION -X- _ O
T -X- _ O
HE -X- _ O
understanding -X- _ O
and -X- _ O
the -X- _ O
treatment -X- _ O
of -X- _ O
the -X- _ O
ubiquitous -X- _ O
textual -X- _ O
data -X- _ O
is -X- _ O
a -X- _ O
major -X- _ O
research -X- _ O
challenge -X- _ O
. -X- _ O
The -X- _ O
tremen- -X- _ O
dous -X- _ O
amount -X- _ O
of -X- _ O
data -X- _ O
produced -X- _ O
by -X- _ O
our -X- _ O
society -X- _ O
through -X- _ O
social -X- _ O
media -X- _ O
and -X- _ O
companies -X- _ O
has -X- _ O
exploded -X- _ O
over -X- _ O
the -X- _ O
past -X- _ O
years -X- _ O
. -X- _ O
All -X- _ O
those -X- _ O
information -X- _ O
are -X- _ O
most -X- _ O
of -X- _ O
the -X- _ O
time -X- _ O
stored -X- _ O
under -X- _ O
textual -X- _ O
format -X- _ O
. -X- _ O
The -X- _ O
human -X- _ O
brain -X- _ O
can -X- _ O
extract -X- _ O
the -X- _ O
meaning -X- _ O
out -X- _ O
of -X- _ O
text -X- _ O
effortlessly -X- _ O
, -X- _ O
but -X- _ O
this -X- _ O
is -X- _ O
not -X- _ O
the -X- _ O
case -X- _ O
for -X- _ O
a -X- _ O
computer -X- _ O
. -X- _ O
It -X- _ O
is -X- _ O
then -X- _ O
required -X- _ O
to -X- _ O
have -X- _ O
performing -X- _ O
and -X- _ O
reliable -X- _ O
techniques -X- _ O
to -X- _ O
treat -X- _ O
this -X- _ O
data -X- _ O
. -X- _ O
The -X- _ O
Natural -X- _ B-TaskName
Language -X- _ I-TaskName
Processing -X- _ I-TaskName
( -X- _ O
NLP -X- _ B-TaskName
) -X- _ O
domain -X- _ O
aims -X- _ O
to -X- _ O
provide -X- _ O
a -X- _ O
set -X- _ O
of -X- _ O
techniques -X- _ O
able -X- _ O
to -X- _ O
explain -X- _ O
a -X- _ O
wide -X- _ O
variety -X- _ O
of -X- _ O
Natural -X- _ O
Language -X- _ O
tasks -X- _ O
such -X- _ O
as -X- _ O
Automatic -X- _ B-TaskName
Translation -X- _ I-TaskName
[ -X- _ O
1 -X- _ O
] -X- _ O
, -X- _ O
Text -X- _ B-TaskName
Summarization -X- _ I-TaskName
[ -X- _ O
2 -X- _ O
] -X- _ O
, -X- _ O
Text -X- _ B-TaskName
Generation -X- _ I-TaskName
[ -X- _ O
3 -X- _ O
] -X- _ O
. -X- _ O
All -X- _ O
those -X- _ O
tasks -X- _ O
have -X- _ O
in -X- _ O
common -X- _ O
the -X- _ O
meaning -X- _ O
extraction -X- _ O
process -X- _ O
to -X- _ O
be -X- _ O
suc- -X- _ O
cessful -X- _ O
. -X- _ O
Undoubtedly -X- _ O
, -X- _ O
if -X- _ O
a -X- _ O
technique -X- _ O
were -X- _ O
able -X- _ O
to -X- _ O
understand -X- _ O
the -X- _ O
underlying -X- _ O
semantic -X- _ O
of -X- _ O
texts -X- _ O
, -X- _ O
this -X- _ O
would -X- _ O
help -X- _ O
to -X- _ O
resolve -X- _ O
the -X- _ O
majority -X- _ O
of -X- _ O
the -X- _ O
modern -X- _ O
NLP -X- _ B-TaskName
problems -X- _ O
. -X- _ O
A -X- _ O
big -X- _ O
concern -X- _ O
that -X- _ O
restricts -X- _ O
a -X- _ O
general -X- _ O
NLP -X- _ B-TaskName
resolver -X- _ O
is -X- _ O
the -X- _ O
single -X- _ O
- -X- _ O
task -X- _ O
training -X- _ O
scheme -X- _ O
. -X- _ O
Gathering -X- _ O
data -X- _ O
and -X- _ O
crafting -X- _ O
a -X- _ O
speciﬁc -X- _ O
model -X- _ O
to -X- _ O
solve -X- _ O
a -X- _ O
precise -X- _ O
problem -X- _ O
works -X- _ O
successfully -X- _ O
. -X- _ O
However -X- _ O
, -X- _ O
it -X- _ O
forces -X- _ O
us -X- _ O
to -X- _ O
come -X- _ O
up -X- _ O
with -X- _ O
a -X- _ O
solution -X- _ O
not -X- _ O
only -X- _ O
each -X- _ O
time -X- _ O
a -X- _ O
new -X- _ O
issue -X- _ O
arises -X- _ O
but -X- _ O
also -X- _ O
to -X- _ O
apply -X- _ O
the -X- _ O
model -X- _ O
on -X- _ O
another -X- _ O
domain -X- _ O
. -X- _ O
A -X- _ O
general -X- _ O
multi -X- _ O
- -X- _ O
task -X- _ O
solver -X- _ O
may -X- _ O
be -X- _ O
preferable -X- _ O
to -X- _ O
avoid -X- _ O
this -X- _ O
time -X- _ O
- -X- _ O
consuming -X- _ O
point -X- _ O
. -X- _ O
Recurrent -X- _ B-MethodName
Neural -X- _ I-MethodName
Networks -X- _ I-MethodName
( -X- _ O
RNN -X- _ B-MethodName
) -X- _ O
were -X- _ O
massively -X- _ O
used -X- _ O
to -X- _ O
solve -X- _ O
NLP -X- _ B-TaskName
problems -X- _ O
. -X- _ O
They -X- _ O
have -X- _ O
been -X- _ O
popular -X- _ O
for -X- _ O
a -X- _ O
few -X- _ O
years -X- _ O
in -X- _ O
supervised -X- _ O
NLP -X- _ B-TaskName
models -X- _ O
for -X- _ O
classiﬁcation -X- _ O
and -X- _ O
regression -X- _ O
. -X- _ O
The -X- _ O
success -X- _ O
of -X- _ O
RNNs -X- _ B-MethodName
is -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
Long -X- _ B-MethodName
Short -X- _ I-MethodName
Term -X- _ I-MethodName
Memory -X- _ I-MethodName
( -X- _ O
LSTM -X- _ B-MethodName
) -X- _ O
[ -X- _ O
4 -X- _ O
] -X- _ O
and -X- _ O
Gated -X- _ B-MethodName
Recurrent -X- _ I-MethodName
Unit -X- _ I-MethodName
( -X- _ O
GRU -X- _ B-MethodName
) -X- _ O
[ -X- _ O
5 -X- _ O
] -X- _ O
architectures -X- _ O
. -X- _ O
Those -X- _ O
two -X- _ O
units -X- _ O
prevent -X- _ O
the -X- _ O
vanishing -X- _ O
gradient -X- _ O
issue -X- _ O
by -X- _ O

Abstract -X- _ O
— -X- _ O
In -X- _ O
2017 -X- _ O
, -X- _ O
Vaswani -X- _ O
et -X- _ O
al -X- _ O
. -X- _ O
proposed -X- _ O
a -X- _ O
new -X- _ O
neural -X- _ O
network -X- _ O
architecture -X- _ O
named -X- _ O
Transformer -X- _ O
. -X- _ O
That -X- _ O
modern -X- _ O
archi- -X- _ O
tecture -X- _ O
quickly -X- _ O
revolutionized -X- _ O
the -X- _ O
natural -X- _ O
language -X- _ O
processing -X- _ O
world -X- _ O
. -X- _ O
Models -X- _ O
like -X- _ O
GPT -X- _ B-MethodName
and -X- _ O
BERT -X- _ B-MethodName
relying -X- _ O
on -X- _ O
this -X- _ O
Transformer -X- _ O
architecture -X- _ O
have -X- _ O
fully -X- _ O
outperformed -X- _ O
the -X- _ O
previous -X- _ O
state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
the- -X- _ O
art -X- _ O
networks -X- _ O
. -X- _ O
It -X- _ O
surpassed -X- _ O
the -X- _ O
earlier -X- _ O
approaches -X- _ O
by -X- _ O
such -X- _ O
a -X- _ O
wide -X- _ O
margin -X- _ O
that -X- _ O
all -X- _ O
the -X- _ O
recent -X- _ O
cutting -X- _ O
edge -X- _ O
models -X- _ O
seem -X- _ O
to -X- _ O
rely -X- _ O
on -X- _ O
these -X- _ O
Transformer -X- _ O
- -X- _ O
based -X- _ O
architectures -X- _ O
. -X- _ O
In -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
provide -X- _ O
an -X- _ O
overview -X- _ O
and -X- _ O
explanations -X- _ O
of -X- _ O
the -X- _ O
latest -X- _ O
models -X- _ O
. -X- _ O
We -X- _ O
cover -X- _ O
the -X- _ O
auto -X- _ O
- -X- _ O
regressive -X- _ O
models -X- _ O
such -X- _ O
as -X- _ O
GPT -X- _ B-MethodName
, -X- _ O
GPT-2 -X- _ B-MethodName
and -X- _ O
XLNET -X- _ B-MethodName
, -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
the -X- _ O
auto -X- _ O
- -X- _ O
encoder -X- _ O
architecture -X- _ O
such -X- _ O
as -X- _ O
BERT -X- _ B-MethodName
and -X- _ O
a -X- _ O
lot -X- _ O
of -X- _ O
post -X- _ O
- -X- _ O
BERT -X- _ B-MethodName
models -X- _ O
like -X- _ O
RoBERTa -X- _ B-MethodName
, -X- _ O
ALBERT -X- _ B-MethodName
, -X- _ O
ERNIE -X- _ B-MethodName
1.0/2.0 -X- _ O
. -X- _ O

Jacky -X- _ O
Casas -X- _ O
, -X- _ O
Elena -X- _ O
Mugellini -X- _ O
, -X- _ O
Omar -X- _ O
Abou -X- _ O
Khaled -X- _ O
University -X- _ O
of -X- _ O
Applied -X- _ O
Sciences -X- _ O
and -X- _ O
Arts -X- _ O
Western -X- _ O
Switzerland -X- _ O
Fribourg -X- _ O
, -X- _ O
Switzerland -X- _ O
Email -X- _ O
: -X- _ O
{ -X- _ O
ﬁrstname.lastname}@hes-so.ch -X- _ O

Anthony -X- _ O
Gillioz -X- _ O
University -X- _ O
of -X- _ O
Neuchâtel -X- _ O
Neuchâtel -X- _ O
, -X- _ O
Switzerland -X- _ O
Email -X- _ O
: -X- _ O
anthony.gillioz@unine.ch -X- _ O

